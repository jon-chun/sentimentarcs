{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_arcs_crux.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mGoFJmeFkTxk",
        "xBpIUgstnE62",
        "FVrqUdhtGFN-",
        "O9LPTX88eHeh",
        "qJB8wF8yjj5j"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jon-chun/sentimentarcs/blob/main/sentiment_arcs_crux.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i0Fg4SYB7g0"
      },
      "source": [
        "# **Sentiment Arcs: Self-Supervising Time Series Sentiment Analysis**\n",
        "\n",
        "* Select Novel (Either precleaned in corpus or clean/upload new novel with defined format)\n",
        "\n",
        "* Run Ensemble Models against the Novel to Generate SentimentArcs (Time Series)\n",
        "\n",
        "* Smooth/Standardize SentimentArcs\n",
        "\n",
        "* (less work: visual) Visual EDA of Variety Plots: SentimentArc (Start/End, Min/Max, Model Noise, MultiModel Consensus)\n",
        "\n",
        "* Analysis Guidelines: MultiModel Agreements (Arc/Crux), Ensemble Baseline for Synthetic Ground Truth, Prioritize Lexical Models given extra weight as Explainable Baseline, Prioritize SOTA Models like RoBERTa, Anomalous Coherent Subgroups (esp if SOTA), look for spans of unusuall model dis/agreement\n",
        "\n",
        "* (more work: close reading) Visual EDA of Variety Plots for Crux Points (Crux and Context both Single and MultiModel)\n",
        "\n",
        "* Look for Motifs: Table Tops, Shoulders, Flairs, Narrowing\n",
        "\n",
        "\n",
        "  (Crux and Arc Verfication)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmPyKSI_ixIw"
      },
      "source": [
        "## [MANUAL INPUT] Connect to Google gDrive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bfkqjgMiw7T"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmDI6aEFivRe"
      },
      "source": [
        "# **Setup and Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpxXvi4Xi9VP"
      },
      "source": [
        "%cd /gdrive/MyDrive/research/2021/sentiment_arcs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9CQ111Gi9Sz"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUSKBpbhSJL1"
      },
      "source": [
        "# CUSTOMIZE: define subdirectory paths\n",
        "\n",
        "data_raw_subdir  = './data/sentiments_raw/'\n",
        "data_clean_subdir  = './data/sentiments_clean/'\n",
        "plots_subdir = './plots/'\n",
        "crux_subdir = './crux/'\n",
        "code_subdir  = './sentiment_arcs/sentiment_arcs/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5GqEXyRkPjj"
      },
      "source": [
        "## Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz5jGrDYi9Qe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajD8hCbzkStO"
      },
      "source": [
        "## Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCRgJK2ri9Nx"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSFkxCFBuvQB"
      },
      "source": [
        "import re\n",
        "from datetime import datetime\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCfXwIBHryr2"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGloK1rcwoGB"
      },
      "source": [
        "import statsmodels.api as sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kosGvnS7sHOA"
      },
      "source": [
        "# https://www.statsmodels.org/devel/examples/notebooks/generated/lowess.html\n",
        "\"\"\"\n",
        "import pylab\n",
        "import statsmodels.api as sm\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "pylab.rc(\"figure\", figsize=(16, 8))\n",
        "pylab.rc(\"font\", size=14)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvofSZ6Zhgqk"
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"]=(20,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGoFJmeFkTxk"
      },
      "source": [
        "## Configure Jupyter Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_GQCdSykXW-"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aRU9tkSXERI"
      },
      "source": [
        "  from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBpIUgstnE62"
      },
      "source": [
        "# **Utility Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEobOwxlnEpy"
      },
      "source": [
        "def get_fullpath( ftype='data_clean', first_note = '',last_note='', plot_ext='png', no_date=False):\n",
        "  '''\n",
        "  Given a required file_type(ftype:['data_clean','data_raw','plot']) and\n",
        "    optional first_note: str inserted after Title and before (optional) SMA/Standardization info\n",
        "             last_note: str insterted after (optional) SMA/Standardization info and before (optional) timedate stamp\n",
        "             plot_ext: change default *.png extension of plot file\n",
        "             no_date: don't add trailing datetime stamp to filename\n",
        "  Generate and return a fullpath (/subdir/filename.ext) to save file to\n",
        "  '''\n",
        "\n",
        "  # String with full path/filename.ext to return\n",
        "  fname = ''\n",
        "\n",
        "  # Get current datetime stamp as a string\n",
        "  if no_date:\n",
        "    date_dt = ''\n",
        "  else:\n",
        "    date_dt = f'_{datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")}'\n",
        "\n",
        "  # Clean optional file notation if passed in\n",
        "  if first_note:\n",
        "    fnote_str = first_note.replace(' ', '_')\n",
        "    fnote_str = '_'.join(fnote_str.split())\n",
        "    fnote_str = '_'.join(fnote_str.split('.'))\n",
        "    fnote_str = '_'.join(fnote_str.split('__'))\n",
        "    fnote_str = fnote_str.lower()\n",
        "\n",
        "  # Get Current Novel Name and Clean\n",
        "  novel_title_str = Novel_Title[0].replace(' ', '_').lower()\n",
        "  novel_title_str = '_'.join(novel_title_str.split())\n",
        "  novel_title_str = '_'.join(novel_title_str.split('.'))\n",
        "  novel_title_str = '_'.join(novel_title_str.split('__'))\n",
        "  if first_note:\n",
        "    novel_title_str = f'{novel_title_str}_{first_note}'\n",
        "\n",
        "  # Option (a): Cleaned Model Data (Smoothed then Standardized)\n",
        "  if ftype == 'data_clean':\n",
        "    subdir_path = data_clean_subdir\n",
        "    fprefix = 'sa_clean_'\n",
        "    fname_str = f'{subdir_path}{fprefix}{novel_title_str}_{Model_Standardization_Method.lower()}_sma{Window_Percent}'\n",
        "    if last_note:\n",
        "      fname = f'{fname_str}_{last_note}{date_dt}.csv'\n",
        "    else:\n",
        "      fname = f'{fname_str}{date_dt}.csv'\n",
        "\n",
        "  # Option (b): Raw Model Data\n",
        "  elif ftype == 'data_raw':\n",
        "    subdir_path = data_raw_subdir\n",
        "    fprefix = 'sa_raw_'\n",
        "    fname_str = f'{subdir_path}{fprefix}{novel_title_str}'\n",
        "    if last_note:\n",
        "      fname = f'{fname_str}_{last_note}{date_dt}.csv'\n",
        "    else:\n",
        "      fname = f'{fname_str}{date_dt}.csv'\n",
        "\n",
        "  # Option (c): Plot Figure\n",
        "  elif ftype == 'plot':\n",
        "    subdir_path = plots_subdir\n",
        "    fprefix = 'plot_'\n",
        "    fname_str = f'{subdir_path}{fprefix}{novel_title_str}'\n",
        "    if last_note:\n",
        "      fname = f'{fname_str}_{last_note}{date_dt}.{plot_ext}'\n",
        "    else:\n",
        "      fname = f'{fname_str}{date_dt}.{plot_ext}'\n",
        "\n",
        "  # Option (d): Crux Text\n",
        "  elif ftype == 'crux_text':\n",
        "    subdir_path = plots_subdir\n",
        "    fprefix = 'crux_'\n",
        "    fname_str = f'{subdir_path}{fprefix}{novel_title_str}'\n",
        "    if last_note:\n",
        "      fname = f'{fname_str}_{last_note}{date_dt}.txt'\n",
        "    else:\n",
        "      fname = f'{fname_str}{date_dt}.txt'\n",
        "\n",
        "  else:\n",
        "    print(f'ERROR: In get_fullpath() with illegal arg ftype:[{ftype}]')\n",
        "    return f'ERROR: ftype:[{ftype}]'\n",
        "\n",
        "  return fname"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3wDhCxynZlD"
      },
      "source": [
        "# Test\n",
        "\n",
        "# NameError: name 'Novel_Title' is not defined\n",
        "\n",
        "# get_fullpath(ftype='data_clean', first_note='vader', last_note='anote bnote', plot_ext='png', no_date=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAyar_tmBx8P"
      },
      "source": [
        "# **Select Novels**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVrqUdhtGFN-"
      },
      "source": [
        "## Choose from Preexisting Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1vX7jvdBwnX"
      },
      "source": [
        "novels_dt = {\n",
        "  'cdickens_christmascarol':['A Christmas Carol by Charles Dickens ',1843,1399],\n",
        "  'cdickens_greatexpectations':['Great Expectations by Charles Dickens' ,1861, 7230],\n",
        "  'ddefoe_robinsoncrusoe':['Robinson Crusoe by Daniel Defoe',1719, 2280],\n",
        "  'emforester_howardsend':['Howards End by E.M. Forester', 1910, 8999],\n",
        "  'fbaum_wizardofoz':['The Wonderful Wizard of Oz by Frank Baum', 1850, 2238],\n",
        "  'fdouglass_narrativeofslave':['Narrative of the life of Frederick Douglass, an American Slave by Frederick Douglass', 1845, 1688],\n",
        "  'fscottfitzerald_greatgatsby':['The Great Gatsby by F. Scott Fitzgerald', 1925, 2950],\n",
        "  'geliot_middlemarch':['Middlemarch by George Eliot', 1871, 10373],\n",
        "  'hjames_portraitofalady':['The Portrait of a Lady by Henry James', 1881, 13258],\n",
        "  'homerwilson_homer':['The Odyssey by Homer (trans Emily Wilson)', 2018, 6814],\n",
        "  'imcewan_machineslikeme':['Machines Like Me by Ian McEwan', 2019, 6448],\n",
        "  'jausten_prideandprejudice':['Pride and Prejudice by Jane Austen', 1813, 5891],\n",
        "  'jconrad_heartofdarkness':['Heart of Darkness by Joseph Conrad', 1902, 1619],\n",
        "  'jjoyce_portraitoftheartist':['A Portrait of the Artist as a Young Man by James Joyce', 2016, 5584],\n",
        "  'jkrowling_pottersorcerersstone':['Harry Potter and the Sorcererâ€™s Stone by J.K. Rowling ', 1997, 5488],\n",
        "  'mproust_searchoflosttime':['In Search of Lost Time, Vol 3: The Guermantes Way by Marcel Proust', 1920, 8388],\n",
        "  'mshelly_frankenstein':['Frankenstein by Mary Shelly', 1818, 3282],\n",
        "  'mtwain_huckleberryfinn':['Huckleberry Finn by Mark Twain', 1884, 5775],\n",
        "  'staugustine_confessions':['Confessions (Books 1-9) by St. Augustine', 400, 3673],\n",
        "  'tmorrison_beloved':['Beloved by Toni Morrison', 1987, 7102],\n",
        "  'vnabokov_palefire':['Pale Fire by Viktor Nabokov', 1962, 2984],\n",
        "  'vwoolf_mrsdalloway':['Mrs. Dalloway by Virginia Woolf', 1925, 3647],\n",
        "  'vwoolf_orlando':['Orlando by Virginia Woolf', 1928, 2,992],\n",
        "  'vwoolf_thewaves':['The Waves by Virginia Woolf', 1931, 3919],\n",
        "  'vwoolf_tothelighthouse':['To The Lighthouse by Virginia Woolf', 1927, 3403],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqPGhyMnKE2Q"
      },
      "source": [
        "# Derive List of Novel a)keys and b)full author and titles\n",
        "\n",
        "novels_key_ls = list(novels_dt.keys())\n",
        "novels_key_ls\n",
        "\n",
        "novels_full_ls = [x[0] for x in list(novels_dt.values())]\n",
        "\n",
        "for akey in novels_full_ls:\n",
        "  print(akey)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVi2NPBgNOW6"
      },
      "source": [
        "novels_corpus_ls = []\n",
        "\n",
        "for akey in novels_key_ls:\n",
        "  print(akey)\n",
        "  novels_corpus_ls.append(akey)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmIOr4qgLbq1"
      },
      "source": [
        "#@title CORPORA: Select Which Novel(s) you want to Analyze:\n",
        "\n",
        "cdickens_christmascarol = True #@param {type:\"boolean\"}\n",
        "cdickens_greatexpectations = False #@param {type:\"boolean\"}\n",
        "ddefoe_robinsoncrusoe = True #@param {type:\"boolean\"}\n",
        "emforester_howardsend = False #@param {type:\"boolean\"}\n",
        "fbaum_wizardofoz = False #@param {type:\"boolean\"}\n",
        "fdouglass_narrativeofslave = False #@param {type:\"boolean\"}\n",
        "fscottfitzerald_greatgatsby = False #@param {type:\"boolean\"}\n",
        "geliot_middlemarch = True #@param {type:\"boolean\"}\n",
        "hjames_portraitofalady = True #@param {type:\"boolean\"}\n",
        "homerwilson_homer = True #@param {type:\"boolean\"}\n",
        "imcewan_machineslikeme = True #@param {type:\"boolean\"}\n",
        "jausten_prideandprejudice = True #@param {type:\"boolean\"}\n",
        "jconrad_heartofdarkness = True #@param {type:\"boolean\"}\n",
        "jjoyce_portraitoftheartist = True #@param {type:\"boolean\"}\n",
        "jkrowling_pottersorcerersstone = True #@param {type:\"boolean\"}\n",
        "mproust_searchoflosttime = True #@param {type:\"boolean\"}\n",
        "mshelly_frankenstein = True #@param {type:\"boolean\"}\n",
        "mtwain_huckleberryfinn = True #@param {type:\"boolean\"}\n",
        "staugustine_confessions = True #@param {type:\"boolean\"}\n",
        "tmorrison_beloved = True #@param {type:\"boolean\"}\n",
        "vnabokov_palefire = True #@param {type:\"boolean\"}\n",
        "vwoolf_mrsdalloway = True #@param {type:\"boolean\"}\n",
        "vwoolf_orlando = True #@param {type:\"boolean\"}\n",
        "vwoolf_thewaves = True #@param {type:\"boolean\"}\n",
        "vwoolf_tothelighthouse = True #@param {type:\"boolean\"}\n",
        "\n",
        "novels_subcorpus_ls = []\n",
        "\n",
        "if cdickens_christmascarol:\n",
        "  novels_subcorpus_ls.append('cdickens_christmascarol')\n",
        "if cdickens_greatexpectations: \n",
        "  novels_subcorpus_ls.append('cdickens_greatexpectations')\n",
        "if ddefoe_robinsoncrusoe:\n",
        "  novels_subcorpus_ls.append('ddefoe_robinsoncrusoe')\n",
        "if emforester_howardsend:\n",
        "  novels_subcorpus_ls.append('emforester_howardsend')\n",
        "if fbaum_wizardofoz:\n",
        "  novels_subcorpus_ls.append('fbaum_wizardofoz')\n",
        "if fdouglass_narrativeofslave:\n",
        "  novels_subcorpus_ls.append('fdouglass_narrativeofslave')\n",
        "if fscottfitzerald_greatgatsby:\n",
        "  novels_subcorpus_ls.append('fscottfitzerald_greatgatsby')\n",
        "if geliot_middlemarch:\n",
        "  novels_subcorpus_ls.append('geliot_middlemarch')\n",
        "if hjames_portraitofalady:\n",
        "  novels_subcorpus_ls.append('hjames_portraitofalady')\n",
        "if homerwilson_homer:\n",
        "  novels_subcorpus_ls.append('homerwilson_homer')\n",
        "if imcewan_machineslikeme:\n",
        "  novels_subcorpus_ls.append('imcewan_machineslikeme')\n",
        "if jausten_prideandprejudice:\n",
        "  novels_subcorpus_ls.append('jausten_prideandprejudice')\n",
        "if jconrad_heartofdarkness:\n",
        "  novels_subcorpus_ls.append('jconrad_heartofdarkness')\n",
        "if jjoyce_portraitoftheartist:\n",
        "  novels_subcorpus_ls.append('jjoyce_portraitoftheartist')\n",
        "if jkrowling_pottersorcerersstone:\n",
        "  novels_subcorpus_ls.append('jkrowling_pottersorcerersstone')\n",
        "if mproust_searchoflosttime:\n",
        "  novels_subcorpus_ls.append('mproust_searchoflosttime')\n",
        "if mshelly_frankenstein:\n",
        "  novels_subcorpus_ls.append('mshelly_frankenstein')\n",
        "if mtwain_huckleberryfinn:\n",
        "  novels_subcorpus_ls.append('mtwain_huckleberryfinn')\n",
        "if staugustine_confessions:\n",
        "  novels_subcorpus_ls.append('staugustine_confessions')\n",
        "if tmorrison_beloved:\n",
        "  novels_subcorpus_ls.append('tmorrison_beloved')\n",
        "if vnabokov_palefire:\n",
        "  novels_subcorpus_ls.append('vnabokov_palefire')\n",
        "if vwoolf_mrsdalloway:\n",
        "  novels_subcorpus_ls.append('vwoolf_mrsdalloway')\n",
        "if vwoolf_orlando:\n",
        "  novels_subcorpus_ls.append('vwoolf_orlando')\n",
        "if vwoolf_thewaves:\n",
        "  novels_subcorpus_ls.append('vwoolf_thewaves')\n",
        "if vwoolf_tothelighthouse:\n",
        "  novels_subcorpus_ls.append('vwoolf_tothelighthouse')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1rSJo31HQdT"
      },
      "source": [
        "# Confirm Novel Selection(s)\n",
        "\n",
        "print(f'Confirm these are the Novels you want to analyze:\\n')\n",
        "\n",
        "for i, anovel in enumerate(novels_subcorpus_ls):\n",
        "  print(f'  Novel #{i}: {novels_dt[anovel][0]}')\n",
        "\n",
        "print('\\nIf there is an error, go back and rerun the previous code cell.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBFK0JK7tBVm"
      },
      "source": [
        "#@title Enter ONE Novel to Focus On:\n",
        "\n",
        "Novel_Key = \"vwoolf_tothelighthouse\" #@param {type:\"string\"}\n",
        "\n",
        "# novels_ls = [Novel_Key]\n",
        "\n",
        "# Novel_Title = novels_dt[novels_ls[0]]\n",
        "# Novel_Title\n",
        "\n",
        "Novel_Title = novels_dt[Novel_Key]\n",
        "\n",
        "novel_focus_str = Novel_Key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9LPTX88eHeh"
      },
      "source": [
        "## Review Summary of Selected Novel(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDSDjfyqa19w"
      },
      "source": [
        "# Review Summary of Novel Selections (at Corpus, SubCorpus and Individual Levels):\n",
        "\n",
        "print(f'\\nCorpus: All Novels (novels_corpus_ls):')\n",
        "for i, anovel in enumerate(novels_corpus_ls):\n",
        "  print(f'  Novel #{i}: {anovel}')\n",
        "\n",
        "print(f'\\nSubCorpus: Only Selected Novels (novels_subcorpus_ls):')\n",
        "for i,anovel in enumerate(novels_subcorpus_ls):\n",
        "  print(f'  Novel #{i}: {anovel}')\n",
        "\n",
        "print(f'\\nFocus: One Novel to Focus On (novels_focus_str):')\n",
        "print(f'  {novel_focus_str}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aYL-J_wB3yH"
      },
      "source": [
        "## Upload New Novel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMxRYJz3Bwkp"
      },
      "source": [
        "# TODO: Coming feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOMp7i_3CGh1"
      },
      "source": [
        "# **Select Sentiment Analysis Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpjhsADm5Wha"
      },
      "source": [
        "cols_map_dt = {'syuzhet':'syuzhetr',\n",
        "               'huliu':'bing_sentimentr',\n",
        "               'sentiword':'sentiword_sentimentr',\n",
        "               'senticnet':'senticnet_sentimentr',\n",
        "               'lmcd':'lmcd_sentimentr',\n",
        "               'jockers':'jockers_sentimentr',\n",
        "               'jockers_rinker':'jockersrinker_sentimentr'\n",
        "               }\n",
        "\n",
        "cols_missing_ls = ['nrc_sentimentr']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUCKZ0s_Ql1t"
      },
      "source": [
        "models_dt = {\n",
        "    # Lexicon\n",
        "    'SyuzhetR_AFINN':['afinn', 'lexicon', 2477],\n",
        "    'SyuzhetR_Bing':['bing', 'lexicon', 5469],\n",
        "    'SyuzhetR_NRC':['nrc', 'lexicon', 5468],\n",
        "    'SyuzhetR_SyuzhetR':['syuzhetr', 'lexicon', 10748],\n",
        "    'SentimentR_SentimentR':['sentimentr', 'lexicon', 11710],\n",
        "    'Pattern':['pattern', 'lexicon', 2918],\n",
        "\n",
        "    # Heuristic\n",
        "    'VADER':['vader', 'heuristic', 7520],\n",
        "    'SentimentR_Bing':['bing_sentimentr', 'heuristic', 5469],\n",
        "    # 'SentimentR_NRC':['nrc_sentimentr', 'heuristic', 5469],\n",
        "    'SentimentR_SentiWord':['sentiword_sentimentr', 'heuristic', 20093],\n",
        "    'SentimentR_SenticNet':['senticnet_sentimentr', 'heuristic', 23626],\n",
        "    'SentimentR_LMcD':['lmcd_sentimentr', 'heuristic', 4150],\n",
        "    'SentimentR_Jockers':['jockers_sentimentr', 'heuristic', 10748],\n",
        "    'SentimentR_JockersRinker':['jockersrinker_sentimentr', 'heuristic', 11710],\n",
        "\n",
        "    # Traditional ML \n",
        "    'Logistic_Regression':['logreg', 'tradml', 'scikit'],\n",
        "    'Logistic_Regression_CV':['logreg_cv', 'tradml', 'scikit'],\n",
        "    'Multinomial_Naive_Bayes':['multinb', 'tradml', 'scikit'],\n",
        "    'TextBlob':['textblob', 'tradml', 'textblob'],\n",
        "    'Random_Forest':['rf', 'tradml', 'scikit'],\n",
        "    'XGBoost':['xgb', 'tradml', 'xgboost'],\n",
        "    'FLAML_AutoML':['flaml', 'tradml', 'flaml'],\n",
        "    'AutoGluon_Text':['autogluon', 'tradml', 'autogluon_text'],\n",
        "\n",
        "    # DNN\n",
        "    'Fully_Connected_Network':['fcn', 'dnn', 6287671],\n",
        "    'LSTM_DNN':['lstm', 'dnn', 7109089],\n",
        "    'CNN_DNN':['cnn', 'dnn', 1315937],\n",
        "    'Multilingual_CNN_Stanza_AutoML':['stanza', 'dnn', 0],\n",
        "    'HyperOpt_CNN_Flair_AutoML':['flair', 'dnn', 0],\n",
        "\n",
        "    # Transformer\n",
        "    'Distilled_BERT':['huggingface', 'transformer', 'bert'],\n",
        "    'T5_IMDB':['t5imdb50k', 'transformer', 't5'],\n",
        "    'BERT_Dual_Coding':['hinglish', 'transformer', 'bert'],\n",
        "    'BERT_Yelp':['yelp', 'transformer', 'bert'],\n",
        "    'BERT_2IMDB':['imdb2way', 'transformer', 'bert'],\n",
        "    'BERT_Multilingual':['nlptown', 'transformer', 'bert'],\n",
        "    'RoBERTa_XML_8Language':['robertaxml8lang', 'transformer', 'roberta'],\n",
        "    'RoBERTa_Large_15DB':['roberta15lg', 'transformer', 'roberta'],\n",
        "\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU_PkB5dRf4M"
      },
      "source": [
        "# Convenience lists for each type of model\n",
        "\n",
        "# Lexicon Models\n",
        "models_lexicon_ls = [x[0] for x in models_dt.values() if x[1] == 'lexicon']\n",
        "print(f'\\nThere are {len(models_lexicon_ls)} Lexicon Models')\n",
        "for i,amodel in enumerate(models_lexicon_ls):\n",
        "  print(f'  Lexicon Model #{i}: {amodel}')\n",
        "\n",
        "# Heuristic Models\n",
        "models_heuristic_ls = [x[0] for x in models_dt.values() if x[1] == 'heuristic']\n",
        "print(f'\\nThere are {len(models_heuristic_ls)} Heuristic Models')\n",
        "for i,amodel in enumerate(models_heuristic_ls):\n",
        "  print(f'  Heuristic Model #{i}: {amodel}')\n",
        "\n",
        "# Traditional ML Models\n",
        "models_tradml_ls = [x[0] for x in models_dt.values() if x[1] == 'tradml']\n",
        "print(f'\\nThere are {len(models_tradml_ls)} Traditional ML Models')\n",
        "for i,amodel in enumerate(models_tradml_ls):\n",
        "  print(f'  Traditional ML Model #{i}: {amodel}')\n",
        "\n",
        "# DNN Models\n",
        "models_dnn_ls = [x[0] for x in models_dt.values() if x[1] == 'dnn']\n",
        "print(f'\\nThere are {len(models_dnn_ls)} DNN Models')\n",
        "for i,amodel in enumerate(models_dnn_ls):\n",
        "  print(f'  DNN Model #{i}: {amodel}')\n",
        "\n",
        "# Transformer Models\n",
        "models_transformer_ls = [x[0] for x in models_dt.values() if x[1] == 'transformer']\n",
        "print(f'\\nThere are {len(models_transformer_ls)} Transformer Models')\n",
        "for i,amodel in enumerate(models_transformer_ls):\n",
        "  print(f'  Transformer Model #{i}: {amodel}')\n",
        "\n",
        "# All Models\n",
        "\n",
        "\"\"\"\n",
        "models_all_ls = models_lexicon_ls + models_heuristic_ls + models_tradml_ls + models_dnn_ls + models_transformer_ls\n",
        "\n",
        "print(f'\\nThere are {len(cols_models_ls)} Total Models:')\n",
        "for i,amodel in enumerate(cols_models_ls):\n",
        "  print(f'  Model #{i:>2}: {amodel}')\n",
        "\"\"\";\n",
        "\n",
        "models_ensemble_ls = models_lexicon_ls + models_heuristic_ls + models_tradml_ls + models_dnn_ls + models_transformer_ls\n",
        "\n",
        "print(f'\\nThere are {len(models_ensemble_ls)} Total Models:')\n",
        "for i,amodel in enumerate(models_ensemble_ls):\n",
        "  print(f'  Model #{i:>2}: {amodel}')\n",
        "\n",
        "print(f'\\nThere are {len(models_ensemble_ls)} Total Models (+ 1 for Ensemble Median)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUTzkqGbFCi5"
      },
      "source": [
        "## Lexical Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFp7F1alQVRN"
      },
      "source": [
        "#@title Select Lexical Model(s) to Include in Ensemble:\n",
        "\n",
        "SyzuhetR_AFINN = False #@param {type:\"boolean\"}\n",
        "SyuzhetR_Bing = False #@param {type:\"boolean\"}\n",
        "SyuzhetR_NRC = False #@param {type:\"boolean\"}\n",
        "SyuzhetR_SyuzhetR = True #@param {type:\"boolean\"}\n",
        "SentimentR_SentimentR = False #@param {type:\"boolean\"}\n",
        "Pattern = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "ensemble_lexicon_ls = []\n",
        "\n",
        "if SyzuhetR_AFINN:\n",
        "  ensemble_lexicon_ls.append('SyuzhetR_AFINN')\n",
        "if SyuzhetR_Bing:\n",
        "  ensemble_lexicon_ls.append('SyuzhetR_Bing')\n",
        "if SyuzhetR_NRC:\n",
        "  ensemble_lexicon_ls.append('SyuzhetR_NRC')\n",
        "if SyuzhetR_SyuzhetR:\n",
        "  ensemble_lexicon_ls.append('SyuzhetR_SyuzhetR')\n",
        "if SentimentR_SentimentR:\n",
        "  ensemble_lexicon_ls.append('SentimentR_SentimentR')\n",
        "if Pattern:\n",
        "  ensemble_lexicon_ls.append('Pattern')\n",
        "\n",
        "\n",
        "ensemble_lexicon_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdSnwQCLFUys"
      },
      "source": [
        "# Confirm Lexicon Model Selection(s)\n",
        "\n",
        "print(f'Confirm these are the Lexicon Model(s) to include in the Ensemble:\\n')\n",
        "\n",
        "for i, amodel in enumerate(ensemble_lexicon_ls):\n",
        "  print(f'  Model #{i}: {models_dt[amodel][0]}')\n",
        "\n",
        "print('\\nIf there is an error, go back and rerun the previous code cell.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGhLjPRDFU-E"
      },
      "source": [
        "## Heuristic Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDNiMoPhXkk2"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in [x for x in models_dt.keys() if models_dt[x][1] == 'heuristic']:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H07bQC3fXez0"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in models_heuristic_ls:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3bpI2LqXTTR",
        "cellView": "form"
      },
      "source": [
        "#@title Select Heuristic Model(s) to Include in Ensemble:\n",
        "\n",
        "VADER = True #@param {type:\"boolean\"}\n",
        "SentimentR_Bing = False #@param {type:\"boolean\"}\n",
        "# SentimentR_NRC = True #@param {type:\"boolean\"}\n",
        "SentimentR_SentiWord = False #@param {type:\"boolean\"}\n",
        "SentimentR_SenticNet = False #@param {type:\"boolean\"}\n",
        "SentimentR_LMcD = False #@param {type:\"boolean\"}\n",
        "SentimentR_Jockers = False #@param {type:\"boolean\"}\n",
        "SentimentR_JockersRinker = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "ensemble_heuristic_ls = []\n",
        "\n",
        "if VADER:\n",
        "  ensemble_heuristic_ls.append('VADER')\n",
        "if SentimentR_Bing:\n",
        "  ensemble_heuristic_ls.append('SentimentR_Bing')\n",
        "\n",
        "# if SentimentR_NRC:\n",
        "#   ensemble_heuristic_ls.append('SentimentR_NRC')\n",
        "\n",
        "if SentimentR_SentiWord:\n",
        "  ensemble_heuristic_ls.append('SentimentR_SentiWord')\n",
        "if SentimentR_SenticNet:\n",
        "  ensemble_heuristic_ls.append('SentimentR_SenticNet')\n",
        "if SentimentR_LMcD:\n",
        "  ensemble_heuristic_ls.append('SentimentR_LMcD')\n",
        "if SentimentR_Jockers:\n",
        "  ensemble_heuristic_ls.append('SentimentR_Jockers')\n",
        "if SentimentR_JockersRinker:\n",
        "  ensemble_heuristic_ls.append('SentimentR_JockersRinker')\n",
        "\n",
        "ensemble_heuristic_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ2BKUtdYZrJ"
      },
      "source": [
        "# Confirm Heuristic Model Selection(s)\n",
        "\n",
        "print(f'Confirm these are the Heuristic Model(s) to include in the Ensemble:\\n')\n",
        "\n",
        "for i, amodel in enumerate(ensemble_heuristic_ls):\n",
        "  print(f'  Model #{i}: {models_dt[amodel][0]}')\n",
        "\n",
        "print('\\nIf there is an error, go back and rerun the previous code cell.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nflvr7w2FaI5"
      },
      "source": [
        "## Traditional ML Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpXIN0LiaKK0"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in [x for x in models_dt.keys() if models_dt[x][1] == 'tradml']:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39nk1DO_aKK4"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in models_tradml_ls:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6UsRYyCaKK7",
        "cellView": "form"
      },
      "source": [
        "#@title Select Traditionl ML Model(s) to Include in Ensemble:\n",
        "\n",
        "Logistic_Regression = False #@param {type:\"boolean\"}\n",
        "Logistic_Regression_CV6 = False #@param {type:\"boolean\"}\n",
        "Multinomial_Naive_Bayes = False #@param {type:\"boolean\"}\n",
        "TextBlob = True #@param {type:\"boolean\"}\n",
        "Random_Forest = False #@param {type:\"boolean\"}\n",
        "XGBoost = False #@param {type:\"boolean\"}\n",
        "FLAML_AutoML = False #@param {type:\"boolean\"}\n",
        "AutoGluon_Text = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "ensemble_tradml_ls = []\n",
        "\n",
        "if Logistic_Regression:\n",
        "  ensemble_tradml_ls.append('Logistic_Regression')\n",
        "if Logistic_Regression_CV6:\n",
        "  ensemble_tradml_ls.append('Logistic_Regression_CV')\n",
        "if Multinomial_Naive_Bayes:\n",
        "  ensemble_tradml_ls.append('Multinomial_Naive_Bayes')\n",
        "if TextBlob:\n",
        "  ensemble_tradml_ls.append('TextBlob')\n",
        "if Random_Forest:\n",
        "  ensemble_tradml_ls.append('Random_Forest')\n",
        "if XGBoost:\n",
        "  ensemble_tradml_ls.append('XGBoost')\n",
        "if FLAML_AutoML:\n",
        "  ensemble_tradml_ls.append('FLAML_AutoML')\n",
        "if AutoGluon_Text:\n",
        "  ensemble_tradml_ls.append('AutoGluon_Text')\n",
        "\n",
        "ensemble_tradml_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdNtHBSSaKK9"
      },
      "source": [
        "# Confirm Traditional ML Model Selection(s)\n",
        "\n",
        "print(f'Confirm these are the Traditional ML Model(s) to include in the Ensemble:\\n')\n",
        "\n",
        "for i, amodel in enumerate(ensemble_tradml_ls):\n",
        "  print(f'  Model #{i}: {models_dt[amodel][0]}')\n",
        "\n",
        "print('\\nIf there is an error, go back and rerun the previous code cell.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9aGvck1FETX"
      },
      "source": [
        "## DNN Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_B3Eco-cOlT"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in [x for x in models_dt.keys() if models_dt[x][1] == 'dnn']:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeiIlaj7cOlY"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in models_dnn_ls:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBpvuGKCcOlb"
      },
      "source": [
        "#@title Select Heuristic Model(s) to Include in Ensemble:\n",
        "\n",
        "Fully_Connected_Network = False #@param {type:\"boolean\"}\n",
        "LSTM_DNN = False #@param {type:\"boolean\"}\n",
        "CNN_DNN = False #@param {type:\"boolean\"}\n",
        "Multilingual_CNN_Stanza_AutoML = True #@param {type:\"boolean\"}\n",
        "HyperOpt_CNN_Flair_AutoML = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "ensemble_dnn_ls = []\n",
        "\n",
        "if Fully_Connected_Network:\n",
        "  ensemble_dnn_ls.append('Fully_Connected_Network')\n",
        "if LSTM_DNN:\n",
        "  ensemble_dnn_ls.append('LSTM_DNN')\n",
        "if CNN_DNN:\n",
        "  ensemble_dnn_ls.append('CNN_DNN')\n",
        "if Multilingual_CNN_Stanza_AutoML:\n",
        "  ensemble_dnn_ls.append('Multilingual_CNN_Stanza_AutoML')\n",
        "if HyperOpt_CNN_Flair_AutoML:\n",
        "  ensemble_dnn_ls.append('HyperOpt_CNN_Flair_AutoML')\n",
        "\n",
        "ensemble_dnn_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etm9ANBPcOlg"
      },
      "source": [
        "# Confirm DNN Model Selection(s)\n",
        "\n",
        "print(f'Confirm these are the DNN Model(s) to include in the Ensemble:\\n')\n",
        "\n",
        "for i, amodel in enumerate(ensemble_dnn_ls):\n",
        "  print(f'  Model #{i}: {models_dt[amodel][0]}')\n",
        "\n",
        "print('\\nIf there is an error, go back and rerun the previous code cell.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rzHudGiFND3"
      },
      "source": [
        "## Transformer Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZajsxoheY7E"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in [x for x in models_dt.keys() if models_dt[x][1] == 'transformer']:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7XOAB0deY7L"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in models_transformer_ls:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNC2j80YeY7R"
      },
      "source": [
        "#@title Select Heuristic Model(s) to Include in Ensemble:\n",
        "\n",
        "Distilled_BERT = False #@param {type:\"boolean\"}\n",
        "T5_IMDB = False #@param {type:\"boolean\"}\n",
        "BERT_Dual_Coding = False #@param {type:\"boolean\"}\n",
        "BERT_Yelp = False #@param {type:\"boolean\"}\n",
        "BERT_2IMDB = False #@param {type:\"boolean\"}\n",
        "BERT_Multilingual = False #@param {type:\"boolean\"}\n",
        "RoBERTa_XML_8Language = False #@param {type:\"boolean\"}\n",
        "RoBERTa_Large_15DB = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "ensemble_transformer_ls = []\n",
        "\n",
        "if Distilled_BERT:\n",
        "  ensemble_transformer_ls.append('Distilled_BERT')\n",
        "if T5_IMDB:\n",
        "  ensemble_transformer_ls.append('T5_IMDB')\n",
        "if BERT_Dual_Coding:\n",
        "  ensemble_transformer_ls.append('BERT_Dual_Coding')\n",
        "if BERT_Yelp:\n",
        "  ensemble_transformer_ls.append('BERT_Yelp')\n",
        "if BERT_2IMDB:\n",
        "  ensemble_transformer_ls.append('BERT_2IMDB')\n",
        "if BERT_Multilingual:\n",
        "  ensemble_transformer_ls.append('BERT_Multilingual')\n",
        "if RoBERTa_XML_8Language:\n",
        "  ensemble_transformer_ls.append('RoBERTa_XML_8Language')\n",
        "if RoBERTa_Large_15DB:\n",
        "  ensemble_transformer_ls.append('RoBERTa_Large_15DB')\n",
        "\n",
        "ensemble_transformer_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK8-9WwpeY7W"
      },
      "source": [
        "# Confirm Transformer Model Selection(s)\n",
        "\n",
        "print(f'Confirm these are the Transformer Model(s) to include in the Ensemble:\\n')\n",
        "\n",
        "for i, amodel in enumerate(ensemble_transformer_ls):\n",
        "  print(f'  Model #{i}: {models_dt[amodel][0]}')\n",
        "\n",
        "print('\\nIf there is an error, go back and rerun the previous code cell.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C4wvECerTbq"
      },
      "source": [
        "# Get list of subensemble models\n",
        "\n",
        "# List of model names (key for indexing into models_dt and col names of ensemble_df)\n",
        "models_subensemble_ls = []\n",
        "\n",
        "# List of model titles (for print)\n",
        "models_ensemble_title_ls = ensemble_lexicon_ls + ensemble_heuristic_ls + ensemble_tradml_ls + \\\n",
        "              ensemble_dnn_ls + ensemble_transformer_ls\n",
        "\n",
        "print(f'Confirm these are the all the Model(s) to include in the SubEnsemble:\\n')\n",
        "\n",
        "for i, amodel in enumerate(models_ensemble_title_ls):\n",
        "  print(f'  Model #{i}: {amodel} ({models_dt[amodel][1]}: {models_dt[amodel][0]})')\n",
        "  models_subensemble_ls.append(models_dt[amodel][0])\n",
        "\n",
        "print(f'\\nIf there is an error in these {len(models_subensemble_ls)} models (+1 for SubEnsemble Median), \\n go back and rerun the previous code cells.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P0a7epkfcQx"
      },
      "source": [
        "## Enter ONE Model to Focus On"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzSglbhffynC"
      },
      "source": [
        "# Generate a string of all Model Titles/keys to cut/paste into dropdown in next code cell\n",
        "\n",
        "models_key_str = ','.join(f\"'{x}'\" for x in models_dt.keys())\n",
        "models_key_str\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x9stHeYfbqB"
      },
      "source": [
        "#@title Select ONE Model to Focus On:\n",
        "\n",
        "Model_Focus = \"VADER\" #@param ['SyuzhetR_AFINN','SyuzhetR_Bing','SyuzhetR_NRC','SyuzhetR_SyuzhetR','SentimentR_SentimentR','Pattern','VADER','SentimentR_Bing','SentimentR_SentiWord','SentimentR_SenticNet','SentimentR_LMcD','SentimentR_Jockers','SentimentR_JockersRinker','Logistic_Regression','Logistic_Regression_CV','Multinomial_Naive_Bayes','TextBlob','Random_Forest','XGBoost','FLAML_AutoML','AutoGluon_Text','Fully_Connected_Network','LSTM_DNN','CNN_DNN','Multilingual_CNN_Stanza_AutoML','HyperOpt_CNN_Flair_AutoML','Distilled_BERT','T5_IMDB','BERT_Dual_Coding','BERT_Yelp','BERT_2IMDB','BERT_Multilingual','RoBERTa_XML_8Language','RoBERTa_Large_15DB']\n",
        "\n",
        "# novels_ls = [Novel_Key]\n",
        "\n",
        "# Novel_Title = novels_dt[novels_ls[0]]\n",
        "# Novel_Title \n",
        "\n",
        "Model_Title = models_dt[Model_Focus]\n",
        "\n",
        "model_focus_str = Model_Focus.lower()\n",
        "print(f'You Selected Model: {model_focus_str}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-Px7FqRfhte"
      },
      "source": [
        "## Review Model Selection(s):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTjCekRmq7JV"
      },
      "source": [
        "# Review Summary of Model Selections (at Ensemble, SubEnsemble and Individual Levels):\n",
        "\n",
        "print(f'\\nEnsemble: All {len(models_ensemble_ls)} Models (models_ensemble_ls):')\n",
        "for i, amodel in enumerate(models_ensemble_ls):\n",
        "  print(f'  Novel #{i}: {amodel}')\n",
        "\n",
        "print(f'\\nSubEnsemble: Only {len(models_subensemble_ls)} Selected Models (models_subensemble_ls):')\n",
        "for i,amodel in enumerate(models_subensemble_ls):\n",
        "  print(f'  Model #{i}: {amodel}')\n",
        "\n",
        "print(f'\\nFocus: One Model to Focus On (model_focus_str):')\n",
        "print(f'  Model: {model_focus_str}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgEiSUDifmg6"
      },
      "source": [
        "# Get list of model names\n",
        "\n",
        "# List of model names (key for indexing into models_dt and col names of ensemble_df)\n",
        "cols_ensemble_ls = []\n",
        "\n",
        "# List of model titles (for print)\n",
        "cols_ensemble_title_ls = ensemble_lexicon_ls + ensemble_heuristic_ls + ensemble_tradml_ls + \\\n",
        "              ensemble_dnn_ls + ensemble_transformer_ls\n",
        "\n",
        "print(f'Confirm these are the all the Model(s) to include in the Ensemble:\\n')\n",
        "\n",
        "for i, amodel in enumerate(cols_ensemble_title_ls):\n",
        "  print(f'  Model #{i}: {amodel} ({models_dt[amodel][1]}: {models_dt[amodel][0]})')\n",
        "  cols_ensemble_ls.append(models_dt[amodel][0])\n",
        "\n",
        "print(f'\\nIf there is an error in these {len(cols_ensemble_ls)} models, \\n go back and rerun the previous code cells.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OLE6ZIuGLxh"
      },
      "source": [
        "# **Get Sentiment Values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6V7wjkgjaxj"
      },
      "source": [
        "## Option (a): Read ALL Clean Values from ONE File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UuH6zEuiqv8"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muq34TU9SrSM"
      },
      "source": [
        "!ls data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxgAlUycTbYY"
      },
      "source": [
        "# Check how many files are in data_clean_subdir\n",
        "\n",
        "!ls $data_clean_subdir | wc -l\n",
        "\n",
        "# !ls ./data/sentiments_clean | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJKL7iTlTxdN"
      },
      "source": [
        "data_clean_subdir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZNvUGJMOfoY"
      },
      "source": [
        "# Get Filenames of Combined data for ALL Sentiment Models for EVERY Novel\n",
        "\n",
        "allz_filenames_ls = []\n",
        "allz_filenames_quoted_ls = []\n",
        "\n",
        "# novels_allz_ls = glob.glob('./data/sentiments_clean/models_allz_*.csv')\n",
        "data_clean_path = f'{data_clean_subdir}models_allz_*.csv'\n",
        "novels_allz_ls = glob.glob(data_clean_path)\n",
        "for i, anovel in enumerate(novels_allz_ls):\n",
        "  allz_filename = anovel.split('/')[-1]\n",
        "  # print(f'Novel #{i}: {allz_filename}')\n",
        "  allz_filename_quoted = f\"'{allz_filename}'\"\n",
        "  allz_filenames_ls.append(allz_filename)\n",
        "  allz_filenames_ls.sort()\n",
        "\n",
        "allz_filenames_quoted_ls = [f\"'{x}'\" for x in allz_filenames_ls]\n",
        "\n",
        "allz_filenames_str = ','.join(allz_filenames_quoted_ls)\n",
        "\n",
        "print('Paste the following string to define the Combined_Sentiment_Datafile dropdown values in the next code cell if necessary:\\n')\n",
        "print(f'\\n{allz_filenames_str}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvutLt32VFCo"
      },
      "source": [
        "allz_filenames_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0NTT36MPmhY"
      },
      "source": [
        "#@title Select a Novel Datafile with the combined Sentiment Values for ALL Models:\n",
        "Combined_Sentiment_Datafile = \"models_allz_vwoolf_tothelighthouse.csv\" #@param ['models_allz_cdickens_achristmascarol.csv','models_allz_cdickens_greatexpectations.csv','models_allz_ddefoe_robinsoncrusoe.csv','models_allz_emforster_howardsend.csv','models_allz_fbaum_thewonderfulwizardofoz.csv','models_allz_fdouglass_narrativelifeofaslave.csv','models_allz_fscottfitzgerald_thegreatgatsby.csv','models_allz_geliot_middlemarch.csv','models_allz_hjames_portraitofalady.csv','models_allz_homer-ewilson_odyssey.csv','models_allz_imcewan_machineslikeme.csv','models_allz_jausten_prideandprejudice.csv','models_allz_jconrad_heartofdarkness.csv','models_allz_jjoyce_portraitoftheartist.csv','models_allz_jkrowling_1sorcerersstone.csv','models_allz_mproust-mtreharne_3guermantesway.csv','models_allz_mshelley_frankenstein.csv','models_allz_mtwain_huckleberryfinn.csv','models_allz_staugustine_confessions9end.csv','models_allz_tmorrison_beloved.csv','models_allz_vnabokov_palefire.csv','models_allz_vwoolf_mrsdalloway.csv','models_allz_vwoolf_orlando.csv','models_allz_vwoolf_thewaves.csv','models_allz_vwoolf_tothelighthouse.csv']\n",
        "\n",
        "ensemble_df = pd.read_csv(f'{data_clean_subdir}{Combined_Sentiment_Datafile}', index_col=[0])\n",
        "ensemble_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvu7JBXjrMbP"
      },
      "source": [
        "# Rename columns to be more consistent using Dictionary cols_map_dt\n",
        "#   defined in Configuration and Setup Section at top of this notebook\n",
        "\n",
        "ensemble_df.rename(columns=cols_map_dt, inplace=True)\n",
        "ensemble_df.info(verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKtEwexynADx"
      },
      "source": [
        "# First, Remove columns derived from base Models\n",
        "\n",
        "cols_model_deriv_drop_ls = []\n",
        "\n",
        "cols_suffix_ls = ['_z', '_sma', '_stdsma', '_lowess', '_stdlowess']\n",
        "for asuffix in cols_suffix_ls:\n",
        "  for acol in ensemble_df.columns: # cols_model_ls:\n",
        "    if acol.endswith(asuffix):\n",
        "      cols_model_deriv_drop_ls.append(acol)\n",
        "\n",
        "ensemble_df.drop(columns=cols_model_deriv_drop_ls, axis=1, inplace=True)\n",
        "ensemble_df.info()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghWeUL7ct2IL"
      },
      "source": [
        "ensemble_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGAuu0IzuZon"
      },
      "source": [
        "%whos list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEFhzw7deoYO"
      },
      "source": [
        "cols_ensemble_ls\n",
        "# cols_models_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ--nePYulx0"
      },
      "source": [
        "cols_models_ls = models_lexicon_ls + models_heuristic_ls + models_tradml_ls + models_dnn_ls + models_transformer_ls\n",
        "cols_models_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "659CvKm3eiNt"
      },
      "source": [
        "temp_set = list(set(cols_models_ls) - set(cols_ensemble_ls))\n",
        "# temp_set = [x for x in cols_models_ls if not(x in cols_ensemble_ls)]\n",
        "temp_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jdPsq-gocUi"
      },
      "source": [
        "# Second, Remove unselected Models from Ensemble\n",
        "\n",
        "# Drop Model columns that are not selected for the Ensemble\n",
        "# cols_model_drop_ls = list(set(cols_models_ls) - set(cols_ensemble_ls))\n",
        "cols_model_drop_ls = [x for x in cols_models_ls if not(x in cols_ensemble_ls)]\n",
        "ensemble_df.drop(columns=cols_model_drop_ls, axis=1, inplace=True)\n",
        "\n",
        "# Skip columns that are not Models\n",
        "cols_notmodel_ls = ['sent_no', 'parag_no', 'sect_no', 'sent_raw', 'sent_clean', 'median']\n",
        "cols_ensemble_ls = list(set(ensemble_df.columns) - set(cols_notmodel_ls))\n",
        "print(f'Ensemble has {len(cols_ensemble_ls)} Models:')\n",
        "for i, amodel in enumerate(cols_ensemble_ls):\n",
        "  print(f'  Model #{i}: {amodel}')\n",
        "\n",
        "print(f'\\nA Total of {len(cols_ensemble_ls)} Raw Models/Columns')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_GO_niNrrbh"
      },
      "source": [
        "# Third, Create one additional Model column that is the Median of all Model Sentiment Values\n",
        "# CAUTION: ERROR if rerun more than once\n",
        "\n",
        "ensemble_df['median'] = ensemble_df[cols_ensemble_ls].median(axis=1)\n",
        "cols_models_ls.append('median')\n",
        "cols_ensemble_ls.append('median')\n",
        "\n",
        "print(f'Added Median Model/Column for ')\n",
        "print(f'\\nA Total of {len(cols_ensemble_ls)} Raw Models/Columns')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrBMPdmdgoCX"
      },
      "source": [
        "# Show just the Ensemble Cols/Models\n",
        "\n",
        "ensemble_df[cols_ensemble_ls].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJB8wF8yjj5j"
      },
      "source": [
        "## Option (b): Read Each Model Raw Values from MANY Files\n",
        "\n",
        "* TODO: May drop feature as too confusing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoZdNC8TGo5P"
      },
      "source": [
        "### Read Lexical Model(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXKryjygkfaO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whv1ZRvAGkvH"
      },
      "source": [
        "### Read Heuristic Model(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFYL1LhABwfg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt-dH7QlfZMM"
      },
      "source": [
        "### Read Traditional ML Model(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uVSz34ufYjX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HkdejuUfcEK"
      },
      "source": [
        "### Read DNN Model(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve-aFJzIfYf6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-woCoeFfd9c"
      },
      "source": [
        "### Read Transformer Model(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XN0hLzafYdM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzLrILtLkJUU"
      },
      "source": [
        "### Join All Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qvO9-R_kkuv"
      },
      "source": [
        "# ensemble_df = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFT6QFgHz2x4"
      },
      "source": [
        "# SMA Smooth Raw Sentiment Values for All Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecUMs0u_z2Be"
      },
      "source": [
        "#@title Enter SMA window size as a Percent of total Corpus Length\n",
        "\n",
        "Window_Percent = 10 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "\n",
        "win_size = int(Window_Percent/100 * ensemble_df.shape[0])\n",
        "\n",
        "plt.figure(figsize=(30,20))\n",
        "\n",
        "# If does not exist, calculate smaing mean (SMA)\n",
        "if ~ensemble_df.columns.str.contains('_sma').any():\n",
        "  for acol in cols_ensemble_ls:\n",
        "    acol_sma = f'{acol}_sma'\n",
        "    ensemble_df[acol_sma] = ensemble_df[acol].rolling(win_size, center=True, min_periods=0).mean()\n",
        "\n",
        "# Plot the SMA (*_sma columns) SentimentArcs for Ensemble Models\n",
        "for acol in cols_ensemble_ls:\n",
        "  if acol in models_lexicon_ls:\n",
        "    alinestyle = 'solid'\n",
        "  elif acol in models_heuristic_ls:\n",
        "    alinestyle = 'solid'\n",
        "  elif acol in models_tradml_ls:\n",
        "    alinestyle = 'dashdot'\n",
        "  elif acol in models_dnn_ls:\n",
        "    alinestyle = 'dotted'\n",
        "  elif acol in models_transformer_ls:\n",
        "    alinestyle='dashed'\n",
        "\n",
        "  acol_sma = f'{acol}_sma'\n",
        "  ensemble_df[acol_sma].plot(alpha=0.5, label=acol, linewidth=2, linestyle=alinestyle)\n",
        "\n",
        "plt.grid(True)\n",
        "plt.title(f'{Novel_Title[0]} \\n SentimentArcs for all {len(cols_ensemble_ls)} Ensemble Models \\n SMA {Window_Percent}% Smoothing but no Standardization', fontsize=20)\n",
        "plt.xlabel('Sentence Number')\n",
        "plt.ylabel('Sentiment')\n",
        "# plt.legend(title='Model Name', loc='best')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', title='Model Name')\n",
        "plt.tight_layout()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y-QB5TFvnS3"
      },
      "source": [
        "ensemble_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RV-1W4gGwfQ"
      },
      "source": [
        "# Standardize the SMA Smoothed SentimentArcs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbhHj_YHZ3zp"
      },
      "source": [
        "# Select Standardized Method and Standardize all Model Values\n",
        "\n",
        "#@title Select a Standardization Method to Use:\n",
        "\n",
        "Model_Standardization_Method = \"zScore\" #@param [\"zScore\", \"MinMax\", \"Robust\"]\n",
        "\n",
        "# Create Standardized versions of each Model/Column values\n",
        "# CAUTION: Only run once\n",
        "\n",
        "# Select a Standardization method = ['zscore','minmax','robust']\n",
        "# Model_Standardization_Method = 'robust'\n",
        "\n",
        "if Model_Standardization_Method == 'zScore':\n",
        "  scaler = StandardScaler()\n",
        "elif Model_Standardization_Method == 'MinMax':\n",
        "  scaler = MinMaxScaler()\n",
        "elif Model_Standardization_Method == 'Robust':\n",
        "  scaler = RobustScaler()\n",
        "\n",
        "# If does not exist, standardize the smaing mean (SMA)\n",
        "if ~ensemble_df.columns.str.contains('_stdsma').any():\n",
        "  for acol in cols_ensemble_ls:\n",
        "    acol_sma = f'{acol}_sma'\n",
        "    acol_stdsma = f'{acol}_stdsma'\n",
        "    ensemble_df[acol_stdsma] = scaler.fit_transform(ensemble_df[acol_sma].values.reshape(-1,1))\n",
        "\n",
        "ensemble_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwHd-IDnvTUS"
      },
      "source": [
        "## Compare Raw, SMA, SMA+Standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvL6hrpBmPlk"
      },
      "source": [
        "# Compare all three plots for a given Model\n",
        "\n",
        "model_str = 'vader'\n",
        "\n",
        "ensemble_df[model_str].plot(label='None (raw)', color='blue', alpha=0.3)\n",
        "ensemble_df[f'{model_str}_sma'].plot(label=f'SMA {Window_Percent}%', color='orange', alpha=0.5, linewidth=5)\n",
        "ensemble_df[f'{model_str}_stdsma'].plot(label=f'SMA then {Model_Standardization_Method}', color='black', alpha=0.7, linewidth=3)\n",
        "\n",
        "plt.grid(True)\n",
        "plt.title(f'{Novel_Title[0]} \\n SentimentArcs for {model_str.upper()} Model \\n Three different Processing Methods', fontsize=20)\n",
        "plt.xlabel('Sentence Number')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.legend(title='Processing Method', loc='best')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', title='Model Name')\n",
        "plt.tight_layout()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDKzBZgAvl5Z"
      },
      "source": [
        "## Compare Three Standardization Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul2rhXwcyR__"
      },
      "source": [
        "# Compare 3 Standardization Methods with (a)Standardization then (b)SMA Smoothing\n",
        "\n",
        "model_str = 'vader'\n",
        "\n",
        "win_per = 10\n",
        "win_size = int(win_per/100 * ensemble_df.shape[0])\n",
        "\n",
        "plot_stdsma_fl = False\n",
        "plot_smastd_fl = True\n",
        "\n",
        "temp_df = pd.DataFrame()\n",
        "temp_df['model'] = ensemble_df[model_str].copy(deep=True)\n",
        "temp_df['model_sma'] = temp_df['model'].rolling(win_size, center=True, min_periods=0).mean()\n",
        "\n",
        "rscaler = RobustScaler()\n",
        "temp_df['robust'] = rscaler.fit_transform(temp_df['model'].values.reshape(-1,1)).flatten()\n",
        "temp_df['sma_robust'] = rscaler.fit_transform(temp_df['model_sma'].values.reshape(-1,1)).flatten()\n",
        "\n",
        "\n",
        "mmscaler = MinMaxScaler()\n",
        "temp_df['minmax'] = mmscaler.fit_transform(temp_df['model'].values.reshape(-1,1)).flatten()\n",
        "temp_df['sma_minmax'] = mmscaler.fit_transform(temp_df['model_sma'].values.reshape(-1,1)).flatten()\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "temp_df['zscore'] = scaler.fit_transform(temp_df['model'].values.reshape(-1,1)).flatten()\n",
        "temp_df['sma_zscore'] = scaler.fit_transform(temp_df['model_sma'].values.reshape(-1,1)).flatten()\n",
        "\n",
        "\n",
        "if plot_smastd_fl:\n",
        "  temp_df['sma_robust'].plot(label='SMA+Robust')\n",
        "  temp_df['sma_minmax'].plot(label='SMA+MinMax')\n",
        "  temp_df['sma_zscore'].plot(label='SMA+zScore')\n",
        "\n",
        "if plot_stdsma_fl:\n",
        "  temp_df['robust'].rolling(win_size, center=True, min_periods=0).mean().plot(label='Robust+SMA', color='orange', linewidth=5, alpha=0.3)\n",
        "  temp_df['minmax'].rolling(win_size, center=True, min_periods=0).mean().plot(label='MinMax+SMA', color='blue', linewidth=5, alpha=0.3)\n",
        "  temp_df['zscore'].rolling(win_size, center=True, min_periods=0).mean().plot(label='zScore+SMA', color='black', linewidth=5, alpha=0.3)\n",
        "\n",
        "plt.grid(True)\n",
        "plt.title(f'{Novel_Title[0]} \\n SentimentArcs for {model_str.upper()} Model \\n Compare Three different Standardization Methods AFTER SMA Smoothing', fontsize=20)\n",
        "plt.xlabel('Sentence Number')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.legend(title='Processing Method', loc='best')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', title='Model Name')\n",
        "plt.tight_layout()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYfiT1Jn4h-K"
      },
      "source": [
        "# Compare 3 Standardization Methods with (a)SMA Smoothing then (b)Standardization\n",
        "\n",
        "model_str = 'vader'\n",
        "\n",
        "win_per = 10\n",
        "win_size = int(win_per/100 * ensemble_df.shape[0])\n",
        "\n",
        "plot_stdsma_fl = True\n",
        "plot_smastd_fl = False\n",
        "\n",
        "temp_df = pd.DataFrame()\n",
        "temp_df['model'] = ensemble_df[model_str].copy(deep=True)\n",
        "temp_df['model_sma'] = temp_df['model'].rolling(win_size, center=True, min_periods=0).mean()\n",
        "\n",
        "rscaler = RobustScaler()\n",
        "temp_df['robust'] = rscaler.fit_transform(temp_df['model'].values.reshape(-1,1)).flatten()\n",
        "temp_df['sma_robust'] = rscaler.fit_transform(temp_df['model_sma'].values.reshape(-1,1)).flatten()\n",
        "\n",
        "\n",
        "mmscaler = MinMaxScaler()\n",
        "temp_df['minmax'] = mmscaler.fit_transform(temp_df['model'].values.reshape(-1,1)).flatten()\n",
        "temp_df['sma_minmax'] = mmscaler.fit_transform(temp_df['model_sma'].values.reshape(-1,1)).flatten()\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "temp_df['zscore'] = scaler.fit_transform(temp_df['model'].values.reshape(-1,1)).flatten()\n",
        "temp_df['sma_zscore'] = scaler.fit_transform(temp_df['model_sma'].values.reshape(-1,1)).flatten()\n",
        "\n",
        "\n",
        "if plot_smastd_fl:\n",
        "  temp_df['sma_robust'].plot(label='SMA+Robust')\n",
        "  temp_df['sma_minmax'].plot(label='SMA+MinMax')\n",
        "  temp_df['sma_zscore'].plot(label='SMA+zScore')\n",
        "\n",
        "if plot_stdsma_fl:\n",
        "  temp_df['robust'].rolling(win_size, center=True, min_periods=0).mean().plot(label='Robust+SMA', color='orange', linewidth=5, alpha=0.3)\n",
        "  temp_df['minmax'].rolling(win_size, center=True, min_periods=0).mean().plot(label='MinMax+SMA', color='blue', linewidth=5, alpha=0.3)\n",
        "  temp_df['zscore'].rolling(win_size, center=True, min_periods=0).mean().plot(label='zScore+SMA', color='black', linewidth=5, alpha=0.3)\n",
        "\n",
        "plt.grid(True)\n",
        "plt.title(f'{Novel_Title[0]} \\n SentimentArcs for {model_str.upper()} Model \\n Compare Three different Standardization Methods AFTER SMA Smoothing', fontsize=20)\n",
        "plt.xlabel('Sentence Number')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.legend(title='Processing Method', loc='best')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', title='Model Name')\n",
        "plt.tight_layout()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxGwnUDXh-K8"
      },
      "source": [
        "# Save Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dPOhYc4uSBF"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeBn0lwRuaCW"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyYp1mF8iNi5"
      },
      "source": [
        "# Save newly cleaned (Smoothed then Standardized) Ensemble Model data\n",
        "\n",
        "file_fullpath = get_fullpath(ftype='data_clean')\n",
        "ensemble_df.to_csv(file_fullpath)\n",
        "\n",
        "print(f'Saved to file: {file_fullpath}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg5VrD9Smhm6"
      },
      "source": [
        "# Verify saved file contents\n",
        "\n",
        "!head -n 10 $file_fullpath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxaf0vtbG1ff"
      },
      "source": [
        "# **Plot Sentiment Arcs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbkwemQF3SB5"
      },
      "source": [
        "# Confirm Ensemble Models and Optionally Save Plot to File\n",
        "\n",
        "#@title Confirm Ensemble SentimentArcs and Option to Saved:\n",
        "Save_Plot = False #@param {type:\"boolean\"}\n",
        "HiRes_300dpi = False #@param {type:\"boolean\"}\n",
        "\n",
        "# ensemble_ls = ensemble_lexicon_ls + ensemble_heuristic_ls + ensemble_tradml_ls + \\\n",
        "#               ensemble_dnn_ls + ensemble_transformer_ls\n",
        "\n",
        "print(f'Confirm these are the all the Model(s) to include in the Ensemble:\\n')\n",
        "\n",
        "for i, amodel in enumerate(cols_ensemble_ls):\n",
        "  print(f'  Model #{i:>2}: {amodel}') # ({models_dt[amodel][1]})')\n",
        "\n",
        "print(f'\\nIf there is an error in these {len(cols_ensemble_ls)} models, \\n go back and rerun the previous code cells.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ2TrYv6NWDk"
      },
      "source": [
        "## Detailed SMA Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u14TnhlRKjva"
      },
      "source": [
        "# Plot all the Model values that have been Smoothed then Standardized\n",
        "\n",
        "plt.figure(figsize=(40,20))\n",
        "\n",
        "# cols_stdsma_ls = [x for x in ensemble_df.columns if x.endswith('_stdsma')]\n",
        "\n",
        "# cols_sma_ls = [x for x in ensemble_df.columns if x.endswith('_sma')]\n",
        "for acol in cols_ensemble_ls:\n",
        "  if acol in models_lexicon_ls:\n",
        "    alinestyle = 'solid'\n",
        "  elif acol in models_heuristic_ls:\n",
        "    alinestyle = 'solid'\n",
        "  elif acol in models_tradml_ls:\n",
        "    alinestyle = 'dashdot'\n",
        "  elif acol in models_dnn_ls:\n",
        "    alinestyle = 'dotted'\n",
        "  elif acol in models_transformer_ls:\n",
        "    alinestyle='dashed'\n",
        "\n",
        "  acol_stdsma = f'{acol}_stdsma'\n",
        "  ensemble_df[acol_stdsma].plot(alpha=0.5, label=acol, linewidth=2, linestyle=alinestyle)\n",
        "\n",
        "plt.grid(True)\n",
        "# plt.title(f'{Novel_Title[0]} \\n Sentiment Analysis \\n SMA {Window_Percent}% Smoothing then {Model_Standardization_Method} Standardization', fontsize=20)\n",
        "plt.title(f'{Novel_Title[0]} \\n SentimentArcs for all {len(cols_ensemble_ls)} Ensemble Models \\n SMA {Window_Percent}% Smoothing then {Model_Standardization_Method} Standardization', fontsize=20)\n",
        "plt.xlabel('Sentence Number')\n",
        "plt.ylabel('Sentiment')\n",
        "\n",
        "plt.legend(title='Model Name', loc='best')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', title='Model Name', prop={\"size\":16})\n",
        "# plt.tight_layout()\n",
        "\n",
        "if Save_Plot:\n",
        "  # get_fullpath(ftype='data_clean', first_note='vader', last_note='anote bnote', plot_ext='png', no_date=True)\n",
        "  if HiRes_300dpi:\n",
        "    #fig.set_size_inches(30.,18.)\n",
        "    # figsize_tup = (40,20)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'{len(cols_ensemble_ls)}models', last_note=f'sma{Window_Percent}_300dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=300)\n",
        "    print(f'Saved hi-res 300dpi Plot to: {plot_fullpath}')\n",
        "  else:\n",
        "    # figsize_tup = (20,10)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'{len(cols_ensemble_ls)}models', last_note=f'sma{Window_Percent}_100dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=100)\n",
        "    print(f'Saved low-res 100dpi Plot to: {plot_fullpath}')\n",
        "\n",
        "plt.show();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpk9lxf6NRzF"
      },
      "source": [
        "## General LOWESS Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJH-_YFvumzE"
      },
      "source": [
        "def get_lowess_sma(adf, acol, awin_per=10, afrac=0.08):\n",
        "  '''\n",
        "  Given a DataFrame, Column Name and Frac float\n",
        "  Return an (n x 2)np.array of LOWESS x,y smoothed values and SMA y_sma values\n",
        "  '''\n",
        "\n",
        "  # win_per = 10 # SMA window as % of corpus length\n",
        "  win_size = int(awin_per/100*adf.shape[0])\n",
        "  \n",
        "  # Generate data looking like cosine\n",
        "  x = adf.index # np.random.uniform(0, 4 * np.pi, size=200)\n",
        "  # Experiment: apply LOWESS smoothing after SMA smoothing\n",
        "  y_sma = adf[acol].rolling(win_size, center=True, min_periods=0).mean().values # np.cos(x) + np.random.random(size=len(x))\n",
        "  y = adf[acol]\n",
        "\n",
        "  # Compute a lowess smoothing of the data\n",
        "  smoothed = sm.nonparametric.lowess(exog=x, endog=y, frac=afrac)\n",
        "\n",
        "  return x, y, y_sma, smoothed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPnm8pUNtYx4"
      },
      "source": [
        "# Compare SMA vs LOWESS both working on raw time series\n",
        "# NOTE: Big disagreement \n",
        "\n",
        "afrac_ls = [1./8, 1./10, 1./15, 1./20]\n",
        "awin_per = 10\n",
        "\n",
        "amodel = 'vader'\n",
        "# amodel = 'vader_stdsma'\n",
        "# x, y, y_sma, smoothed = get_lowess_sma(ensemble_df, amodel, awin_per=awin_per, afrac=afrac)\n",
        "\n",
        "# Plot the fit line\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# raw_arc = ax.scatter(x, y, color='blue', alpha=0.1)\n",
        "# sma_arc = ax.scatter(x, y_sma, s=5, color='orange')\n",
        "\n",
        "x, y, y_sma, smoothed5 = get_lowess_sma(ensemble_df, amodel, awin_per=awin_per, afrac=1./5)\n",
        "lowess_arc5 = ax.scatter(smoothed5[:, 0], smoothed5[:, 1], s=5, alpha=0.3) # c=\"grey\", s=5)\n",
        "sma_arc = ax.scatter(x, y_sma, s=5, color='orange')\n",
        "\n",
        "_, _, _, smoothed8 = get_lowess_sma(ensemble_df, amodel, awin_per=awin_per, afrac=1./8)\n",
        "lowess_arc8 = ax.scatter(smoothed8[:, 0], smoothed8[:, 1], s=5, alpha=0.3) # , c=\"navy\", s=5)\n",
        "\n",
        "_, _, _, smoothed10 = get_lowess_sma(ensemble_df, amodel, awin_per=awin_per, afrac=1./10)\n",
        "lowess_arc10 = ax.scatter(smoothed10[:, 0], smoothed10[:, 1], s=5, alpha=0.3) # , c=\"navy\", s=5)\n",
        "\n",
        "_, _, _, smoothed15 = get_lowess_sma(ensemble_df, amodel, awin_per=awin_per, afrac=1./15)\n",
        "lowess_arc15 = ax.scatter(smoothed15[:, 0], smoothed15[:, 1], s=5, alpha=0.3) # , c=\"navy\", s=5)\n",
        "\n",
        "_, _, _, smoothed20 = get_lowess_sma(ensemble_df, amodel, awin_per=awin_per, afrac=1./20)\n",
        "lowess_arc20 = ax.scatter(smoothed20[:, 0], smoothed20[:, 1], s=5, alpha=0.3) # , c=\"black\", s=5)\n",
        "\n",
        "_, _, _, smoothed30 = get_lowess_sma(ensemble_df, amodel, awin_per=awin_per, afrac=1./30)\n",
        "lowess_arc30 = ax.scatter(smoothed30[:, 0], smoothed30[:, 1], s=5, alpha=0.3) # , c=\"black\", s=5)\n",
        "\n",
        "\n",
        "ax.set_title(f\"{Novel_Title[0]} \\n Sentiment {amodel.split('_')[0]} \\n SMA {awin_per}% vs LOWESS frac=1./[5,8,10,15,20]\")\n",
        "ax.set_xlabel('Sentence Number')\n",
        "ax.set_ylabel('Sentiment')\n",
        "plt.legend((sma_arc, lowess_arc5, lowess_arc8, lowess_arc10, lowess_arc15, lowess_arc20, lowess_arc30), # \n",
        "           # (raw_arc, sma_arc, lowess_arc),\n",
        "           (f'SMA window={awin_per}%', f'LOWESS frac={1./5:.2f}', f'LOWESS frac={1./8:.2f}', f'LOWESS frac={1./10:.2f}', f'LOWESS frac={1./15:.2f}', f'LOWESS frac={1./20:.2f}', f'LOWESS frac={1./30:.2f}'),\n",
        "           # ('Raw Values', f'SMA window={awin_per}%', f'LOWESS frac={afrac:.3f}'),\n",
        "           scatterpoints=5,\n",
        "           loc='best',\n",
        "           # ncol=3,\n",
        "           fontsize=12)\n",
        "plt.autoscale(enable=True, axis=\"x\", tight=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6mCgf6VtniL"
      },
      "source": [
        "ensemble_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioufE0gutwdx"
      },
      "source": [
        "# Compare all three plots (raw, SMA, SMA+zScore) for a given Model\n",
        "\"\"\"\n",
        "model_str = 'vader'\n",
        "\n",
        "ensemble_df[model_str].plot(label='raw', alpha=0.3)\n",
        "ensemble_df[f'{model_str}_sma'].plot(label=f'SMA {Window_Percent}%', alpha=0.5, linewidth=5)\n",
        "ensemble_df[f'{model_str}_stdsma'].plot(label=f'SMA then Std {Model_Standardization_Method}', alpha=0.7, linewidth=3)\n",
        "\n",
        "plt.grid(True)\n",
        "plt.title(f'{Novel_Title[0]} \\n SentimentArcs for {model_str.upper()} Model \\n (a)Raw, (b)SMA ({Window_Percent}%) and (c)SMA then Standardize ({Model_Standardization_Method})', fontsize=20)\n",
        "plt.xlabel('Sentence Number')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.legend(title='Processing Method', loc='best')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', title='Model Name')\n",
        "plt.tight_layout()\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6muDCLFVNdvj"
      },
      "source": [
        "# Compare LOWESS smoothed version of SMA Smoothed Standardized Model\n",
        "\n",
        "#@title Compare SMA and LOWESS smoothed values of Model\n",
        "\n",
        "# Standardized Model Values\n",
        "Standardized_Model = \"VADER\" #@param [\"VADER\", \"TextBlob\", \"RoBERTa\"]\n",
        "\n",
        "# Simple Moving Average Smoothing Window Size\n",
        "Window_Percent = 10 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "\n",
        "# LOWESS Smoothing Fraction Size\n",
        "LOWESS_Fraction = 30 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "# Confirm Ensemble Models and Optionally Save Plot to File\n",
        "\n",
        "#@markdown Confirm Ensemble SentimentArcs and Option to Saved:\n",
        "Save_Plot = True #@param {type:\"boolean\"}\n",
        "HiRes_300dpi = True #@param {type:\"boolean\"}\n",
        "\n",
        "# ensemble_ls = ensemble_lexicon_ls + ensemble_heuristic_ls + ensemble_tradml_ls + \\\n",
        "#               ensemble_dnn_ls + ensemble_transformer_ls\n",
        "\n",
        "# print(f'Confirm these are the all the Model(s) to include in the Ensemble:\\n')\n",
        "\n",
        "# for i, amodel in enumerate(ensemble_ls):\n",
        "#   print(f'  Model #{i:>2}: {amodel} ({models_dt[amodel][1]})')\n",
        "\n",
        "# print(f'\\nIf there is an error in these {len(ensemble_ls)} models, \\n go back and rerun the previous code cells.')\n",
        "\n",
        "# KEY DECISION: (LOWESS of SMA+STD) > (LOWESS of RAW+STD)\n",
        "col_model = f'{Standardized_Model.lower()}_stdsma'\n",
        "# col_model = f'{Standardized_Model.lower()}'\n",
        "\n",
        "print(f'Plotting Model: {col_model}')\n",
        "lowess_frac = 1./LOWESS_Fraction\n",
        "\n",
        "win_size = int(Window_Percent/100 * ensemble_df.shape[0])\n",
        "\n",
        "# Get LOWESS values\n",
        "x, y, y_sma, smoothed = get_lowess_sma(ensemble_df, col_model, awin_per=Window_Percent, afrac=lowess_frac)\n",
        "\n",
        "# Plot the fit line\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "raw_arc = ax.scatter(x, y, c='blue', alpha=0.1)\n",
        "sma_arc = ax.scatter(x, y_sma, c='red', s=5)\n",
        "lowess_arc = ax.scatter(smoothed[:, 0], smoothed[:, 1], c=\"black\", s=5)\n",
        "ax.set_title(f\"{Novel_Title[0]} \\n Sentiment {amodel.split('_')[0]} \\n 1st Pass: SMA, 2nd Pass: SMA {Window_Percent}% vs LOWESS frac={LOWESS_Fraction}\")\n",
        "ax.set_xlabel('Sentence Number')\n",
        "ax.set_ylabel('Sentiment')\n",
        "plt.legend((raw_arc, sma_arc, lowess_arc),\n",
        "           ('SMA 10%', f'2nd Pass: SMA window={Window_Percent}%', f'2nd Pass: LOWESS frac={lowess_frac}'),\n",
        "           scatterpoints=1,\n",
        "           loc='best',\n",
        "           # ncol=3,\n",
        "           fontsize=8)\n",
        "plt.autoscale(enable=True, axis=\"x\", tight=True)\n",
        "\n",
        "\"\"\"\n",
        "plt.grid(True)\n",
        "# plt.title(f'{Novel_Title[0]} \\n Sentiment Analysis \\n SMA {Window_Percent}% Smoothing then {Model_Standardization_Method} Standardization', fontsize=20)\n",
        "plt.title(f'{Novel_Title[0]} \\n SentimentArcs for all {len(cols_model_ls)} Ensemble Models \\n SMA {Window_Percent}% Smoothing then {Model_Standardization_Method} Standardization', fontsize=20)\n",
        "plt.xlabel('Sentence Number')\n",
        "plt.ylabel('Sentiment')\n",
        "# plt.legend(title='Model Name', loc='best')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', title='Model Name', prop={\"size\":16})\n",
        "plt.tight_layout()\n",
        "\"\"\";\n",
        "\n",
        "if Save_Plot:\n",
        "  # get_fullpath(ftype='data_clean', first_note='vader', last_note='anote bnote', plot_ext='png', no_date=True)\n",
        "  if HiRes_300dpi:\n",
        "    # fig.set_size_inches(30.,18.)\n",
        "    # figsize_tup = (40,20)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'sma{Window_Percent}_vs_lowess{LOWESS_Fraction}_300dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=300)\n",
        "    print(f'Saved hi-res 300dpi Plot to: {plot_fullpath}')\n",
        "  else:\n",
        "    # figsize_tup = (20,10)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'sma{Window_Percent}_vs_lowess{LOWESS_Fraction}_100dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=100)\n",
        "    print(f'Saved low-res 100dpi Plot to: {plot_fullpath}')\n",
        "\n",
        "plt.show();\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Plot the fit line\n",
        "fig, ax = pylab.subplots()\n",
        "\n",
        "ax.scatter(x, y)\n",
        "ax.plot(smoothed[:, 0], smoothed[:, 1], c=\"k\")\n",
        "ax.set_ylabel('Standardized Sentiment/Emotion Value')\n",
        "ax.set_xlabel('Sentence Number')\n",
        "ax.set_title(f'{Novel_Title[0]}\\n Sentiment/Emotion Model: {Standardized_Model} \\n Smoothing: LOWESS (frac=1/{LOWESS_Fraction}) vs SMA ({Window_Percent}%)')\n",
        "pylab.autoscale(enable=True, axis=\"x\", tight=True)\n",
        "\"\"\";\n",
        "\n",
        "\"\"\"\n",
        "# Plot the fit line\n",
        "fig, ax = pylab.subplots()\n",
        "\n",
        "smoothed_np = get_lowess(novel_df, col_model, win_size, lowess_frac)\n",
        "\n",
        "y_smaing = novel_df[col_model].smaing(win_size, center=True, min_periods=0).mean().values\n",
        "\n",
        "ax.scatter(novel_df.index, y_smaing) \n",
        "ax.plot(smoothed_np[:, 0], smoothed_np[:, 1], c=\"k\")\n",
        "pylab.autoscale(enable=True, axis=\"x\", tight=True)\n",
        "plt.title(f'{Novel_Title}\\n Sentiment/Emotion Model: {Standardized_Model} \\n Smoothing: LOWESS (frac=1/{LOWESS_Fraction}) vs SMA ({Window_Percent}%)')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsHgTFkGSDDN"
      },
      "source": [
        "# Get LOWESS smoothed values for all Standardized Models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axGb9Tgw91ah"
      },
      "source": [
        "ensemble_df.info(verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S8ukRtNL1v9"
      },
      "source": [
        "ensemble_df.drop(columns=[x for x in ensemble_df.columns if x.endswith('lowess')], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P-AvepzEx_C"
      },
      "source": [
        "# Compute LOWESS smoothed and LOWESS smoothed+Standardized time series in ensemble DataFrame\n",
        "\n",
        "# novel_lowess_dt = {}\n",
        "model_labels_dt = {}\n",
        "\n",
        "# If does not exist, calculate LOWESS(*_lowess) and LOWESS+Standardized(*_stdlowess) values/columns\n",
        "if ~ensemble_df.columns.str.contains('_stdlowess').any():\n",
        "  for i, amodel in enumerate(cols_ensemble_ls):\n",
        "    print(f'Calculating (a)LOWESS and (b)LOWESS+Standardized values for Model #{i:>2}: {amodel}')\n",
        "    lowess_frac = 1./int(LOWESS_Fraction)\n",
        "    amodel_stdsma = f'{amodel}_stdsma'\n",
        "    amodel_lowess = f'{amodel}_lowess'\n",
        "    amodel_stdlowess = f'{amodel}_stdlowess'\n",
        "\n",
        "    # KEY DECISION: (LOWESS of SMA+STD:amodel_stdsma) > (LOWESS of RAW+STD:amodel)\n",
        "    # _, _, _, smoothed = get_lowess(ensemble_df, amodel, awin_per=Window_Percent, afrac=lowess_frac)\n",
        "    _, _, _, smoothed = get_lowess_sma(ensemble_df, amodel_stdsma, awin_per=Window_Percent, afrac=lowess_frac)\n",
        "    x_vals = smoothed[:, 0]\n",
        "    y_vals = smoothed[:, 1]\n",
        "    # novel_lowess_dt[amodel] = (x_vals, y_vals)\n",
        "    ensemble_df[amodel_lowess] = pd.Series(y_vals)\n",
        "    ensemble_df[amodel_stdlowess] = scaler.fit_transform(ensemble_df[amodel_lowess].values.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnTq9FRyFLbg"
      },
      "source": [
        "ensemble_df.info(verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPrCapqGFwi4"
      },
      "source": [
        "ensemble_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUbgju8IGHKF"
      },
      "source": [
        "# EDA comparison for Bing LOWESS vs LOWESS+Standardization\n",
        "#   Standardization NOT needed since LOWESS is smoothing an already Standardized SMA+STD Arc\n",
        "\n",
        "ensemble_df['vader_lowess'].plot(label='Bing LOWESS')\n",
        "ensemble_df['vader_stdlowess'].plot(label=f'Bing LOWESS+{Model_Standardization_Method}')\n",
        "plt.title(f'{Novel_Title} \\n Sentiment Analysis \\n LOWESS vs LOWESS with Standardization')\n",
        "plt.legend(loc='best')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA3vj-IFNCLJ"
      },
      "source": [
        "## EDA on Problem Calculating LOWESS for lmcd_sentimentr (too many zero values?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2wj8xDOMmRJ"
      },
      "source": [
        "ensemble_df['lmcd_sentimentr_stdlowess'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RelfDkOoLW6W"
      },
      "source": [
        "# EDA comparison for Bing LOWESS vs LOWESS+Standardization\n",
        "#   Not needed since LOWESS is smoothing an already Standardized SMA+STD Arc\n",
        "\n",
        "ensemble_df['lmcd_sentimentr_lowess'].plot(label='LMcD LOWESS')\n",
        "ensemble_df['lmcd_sentimentr_stdlowess'].plot(label=f'LMcD LOWESS+{Model_Standardization_Method}')\n",
        "plt.title(f'{Novel_Title[0]} \\n Sentiment Analysis \\n LOWESS vs LOWESS with Standardization')\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCnbaHjIMLrN"
      },
      "source": [
        "# Add 3rd Pass of Smoothing from Original SMA->LOWESS->SMA\n",
        "\n",
        "ensemble_df['lmcd_sentimentr_stdlowess'].rolling(300, center=True, min_periods=0).mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naEvlLoeMxbr"
      },
      "source": [
        "# Distribution of Sentiment values \n",
        "\n",
        "ensemble_df['lmcd_sentimentr'].plot(kind='hist', bins=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Ve41dKK6R3"
      },
      "source": [
        "ensemble_df['lmcd_sentimentr_stdsma'].plot()\n",
        "ensemble_df['vader_stdsma'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqfwNqA_WAHU"
      },
      "source": [
        "## Plot Ensemble of Selected Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4veOgVHqFP7I"
      },
      "source": [
        "# EDA plot of All LOWESS Smoothed Standardized+SMA SentimentArcs by Model Family\n",
        "\n",
        "#@markdown Confirm Ensemble SentimentArcs and Option to Saved:\n",
        "Save_Plot = True #@param {type:\"boolean\"}\n",
        "HiRes_300dpi = True #@param {type:\"boolean\"}\n",
        "\n",
        "plt.figure(figsize=(30,20))\n",
        "\n",
        "stdlowess_ls = [x for x in ensemble_df.columns if x.endswith('_stdlowess')]\n",
        "\n",
        "# ax, fig = plt.subplot()\n",
        "\n",
        "for i, an_arc in enumerate(stdlowess_ls):\n",
        "  ensemble_df[an_arc].plot(label=an_arc, alpha=0.3)\n",
        "\n",
        "# plt.title(f'{Novel_Title[0]} \\n Sentiment Analysis \\n LOWESS with {Model_Standardization_Method} Standardization')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.title(f'{Novel_Title[0]} \\n SentimentArcs for All {len(cols_ensemble_ls)} Models in Ensemble Models \\n LOWESS frac={LOWESS_Fraction} Smoothing of {Model_Standardization_Method} Standardized SMA {Window_Percent}%', fontsize=20)\n",
        "plt.xlabel('Sentence Number')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.legend(title='Model Name', loc='upper right')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', title='Model Name')\n",
        "plt.tight_layout()\n",
        "\n",
        "if Save_Plot:\n",
        "  # get_fullpath(ftype='data_clean', first_note='vader', last_note='anote bnote', plot_ext='png', no_date=True)\n",
        "  if HiRes_300dpi:\n",
        "    # fig.set_size_inches(30.,18.)\n",
        "    # figsize_tup = (40,20)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'lowess{LOWESS_Fraction}_of_{Model_Standardization_Method}_SMA{Window_Percent}_300dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=300)\n",
        "    print(f'Saved hi-res 300dpi Plot to: {plot_fullpath}')\n",
        "  else:\n",
        "    # figsize_tup = (20,10)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'lowess{LOWESS_Fraction}_of_{Model_Standardization_Method}_SMA{Window_Percent}__100dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=100)\n",
        "    print(f'Saved low-res 100dpi Plot to: {plot_fullpath}')\n",
        "\n",
        "plt.show();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFNl-GdZSNaw"
      },
      "source": [
        "# Compute, save and plot LOWESS smoothed time series\n",
        "\n",
        "\"\"\"\n",
        "# novel_lowess_dt = {}\n",
        "model_labels_dt = {}\n",
        "\n",
        "for i, amodel in enumerate(cols_model_ls):\n",
        "  print(f'Calculating LOWESS for Model #{i:>2}: {amodel}')\n",
        "  lowess_frac = 1./int(LOWESS_Fraction)\n",
        "  amodel_stdsma = f'{amodel}_stdsma'\n",
        "  amodel_lowess = f'{amodel}_lowess'\n",
        "  amodel_stdlowess = f'{amode}_stdlowess}'\n",
        "  # _, _, _, smoothed = get_lowess(ensemble_df, amodel_stdsma, awin_per=Window_Percent, afrac=lowess_frac)\n",
        "  _, _, _, smoothed = get_lowess(ensemble_df, amodel, awin_per=Window_Percent, afrac=lowess_frac)\n",
        "  x_vals = smoothed[:, 0]\n",
        "  y_vals = smoothed[:, 1]\n",
        "  # novel_lowess_dt[amodel] = (x_vals, y_vals)\n",
        "  ensemble_df[amodel_lowess] = pd.Series(y_vals)\n",
        "  ensemble_df[amodel_stdlowess] = scaler.fit_transform(ensemble_df[acol_sma].values.reshape(-1,1))\n",
        "  \n",
        "\n",
        "  model_labels_dt[amodel] = plt.scatter(x_vals, y_vals, s=5, alpha=0.5)\n",
        "\n",
        "model_labels_tup = tuple(model_labels_dt.keys())\n",
        "# plt.title(f'{Novel_Title} \\n Model: {amodel}')\n",
        "\n",
        "# Plot the fit line\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# raw_arc = ax.scatter(x, y, c='blue', alpha=0.1)\n",
        "# sma_arc = ax.scatter(x, y_sma, c='red', s=5)\n",
        "# lowess_arc = ax.scatter(smoothed[:, 0], smoothed[:, 1], c=\"black\", s=5)\n",
        "\n",
        "ax.set_title(f\"{Novel_Title[0]} \\n Sentiment {amodel.split('_')[0]} \\n SMA {Window_Percent}% vs LOWESS frac={afrac}\")\n",
        "ax.set_xlabel('Sentence Number')\n",
        "ax.set_ylabel('Sentiment')\n",
        "plt.legend(model_labels_tup, # (raw_arc, sma_arc, lowess_arc),\n",
        "           ('Raw Values', f'SMA window={Window_Percent}%', f'LOWESS frac={afrac}'),\n",
        "           scatterpoints=1,\n",
        "           loc='best',\n",
        "           # ncol=3,\n",
        "           fontsize=8)\n",
        "plt.autoscale(enable=True, axis=\"x\", tight=True)\n",
        "\n",
        "if Save_Plot:\n",
        "  # get_fullpath(ftype='data_clean', first_note='vader', last_note='anote bnote', plot_ext='png', no_date=True)\n",
        "  if HiRes_300dpi:\n",
        "    # fig.set_size_inches(30.,18.)\n",
        "    # figsize_tup = (40,20)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'sma{Window_Percent}_vs_lowess{LOWESS_Fraction}_300dpi', last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=300)\n",
        "    print(f'Saved hi-res 300dpi Plot to: {plot_fullpath}')\n",
        "  else:\n",
        "    # figsize_tup = (20,10)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'sma{Window_Percent}_vs_lowess{LOWESS_Fraction}_100dpi', last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=100)\n",
        "    print(f'Saved low-res 100dpi Plot to: {plot_fullpath}')\n",
        "\n",
        "plt.show();\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ormrcJA7WRDk"
      },
      "source": [
        "## Plot Individual Model within Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bB_A51EWfso"
      },
      "source": [
        "# Get list and string for all LOWESS smoothed SMA+STD Arcs\n",
        "\n",
        "stdlowess_ls = [x for x in ensemble_df.columns if x.endswith('_stdlowess')]\n",
        "\n",
        "stdlowess_str = ','.join([f\"'{x}'\" for x in stdlowess_ls if x.endswith('_stdlowess')])\n",
        "stdlowess_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH8pF3PiWQ2G"
      },
      "source": [
        "# EDA plot of LOWESS Smoothed Standardized+SMA SentimentArcs by Model Family\n",
        "\n",
        "#@title Select which Model to Plot LOWESS Smoothed of SMA+STD:\n",
        "\n",
        "Model_Name = \"roberta15lg_stdlowess\" #@param ['sentimentr_stdlowess','syuzhetr_stdlowess','bing_stdlowess','sentiword_sentimentr_stdlowess','senticnet_sentimentr_stdlowess','nrc_stdlowess','afinn_stdlowess','vader_stdlowess','textblob_stdlowess','pattern_stdlowess','stanza_stdlowess','flair_stdlowess','jockersrinker_sentimentr_stdlowess','jockers_sentimentr_stdlowess','bing_sentimentr_stdlowess','lmcd_sentimentr_stdlowess','roberta15lg_stdlowess','yelp_stdlowess','nlptown_stdlowess','huggingface_stdlowess','hinglish_stdlowess','imdb2way_stdlowess','t5imdb50k_stdlowess','robertaxml8lang_stdlowess','fcn_stdlowess','lstm_stdlowess','cnn_stdlowess','multinb_stdlowess','logreg_stdlowess','logreg_cv_stdlowess','rf_stdlowess','xgb_stdlowess','flaml_stdlowess','autogluon_stdlowess','median_stdlowess']\n",
        "\n",
        "# Novel_Name = {Novel_Title[0]} #@param [Novel_Title[0]]\n",
        "\n",
        "#@markdown Confirm Ensemble SentimentArcs and Option to Saved:\n",
        "Save_Plot = True #@param {type:\"boolean\"}\n",
        "HiRes_300dpi = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "plt.figure(figsize=(30,20))\n",
        "\n",
        "ensemble_df[Model_Name].plot()\n",
        "\n",
        "plt.grid(True)\n",
        "plt.title(f'{Novel_Title[0]} \\n SentimentArcs for {\" \".join(Model_Name.split(\"_\")[:-1]).capitalize()} Model in Ensemble \\n LOWESS frac={LOWESS_Fraction} Smoothing of {Model_Standardization_Method} Standardized SMA {Window_Percent}%', fontsize=20)\n",
        "plt.xlabel('Sentence Number')\n",
        "plt.ylabel('Sentiment')\n",
        "# plt.legend(title='Model Name', loc='best')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', title='Model Name')\n",
        "plt.tight_layout()\n",
        "\n",
        "if Save_Plot:\n",
        "  # get_fullpath(ftype='data_clean', first_note='vader', last_note='anote bnote', plot_ext='png', no_date=True)\n",
        "  if HiRes_300dpi:\n",
        "    # fig.set_size_inches(30.,18.)\n",
        "    # figsize_tup = (40,20)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'{Model_Name.lower()}', last_note=f'lowess{LOWESS_Fraction}_of_{Model_Standardization_Method}_SMA{Window_Percent}_300dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=300)\n",
        "    print(f'Saved hi-res 300dpi Plot to: {plot_fullpath}')\n",
        "  else:\n",
        "    # figsize_tup = (20,10)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'{Model_Name.lower()}', last_note=f'lowess{LOWESS_Fraction}_of_{Model_Standardization_Method}_SMA{Window_Percent}__100dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=100)\n",
        "    print(f'Saved low-res 100dpi Plot to: {plot_fullpath}')\n",
        "\n",
        "plt.show();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkEi2nDuWFbS"
      },
      "source": [
        "## Plot Model Families within Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qm3Nxt9H96R"
      },
      "source": [
        "# EDA plot of LOWESS Smoothed Standardized+SMA SentimentArcs by Model Family\n",
        "\n",
        "#@title Select which Model Family to Plot LOWESS Smoothed of SMA+STD:\n",
        "\n",
        "Model_Family = \"lexicon\" #@param [\"lexicon\", \"heuristic\", \"tradml\", \"dnn\", \"transformer\"]\n",
        "\n",
        "#@markdown Confirm Ensemble SentimentArcs and Option to Saved:\n",
        "Save_Plot = True #@param {type:\"boolean\"}\n",
        "HiRes_300dpi = True #@param {type:\"boolean\"}\n",
        "\n",
        "stdlowess_ls = [x for x in ensemble_df.columns if x.endswith('_stdlowess')]\n",
        "\n",
        "# Select which family to Plot\n",
        "plot_family = ['lexicon','heuristic','tradml','dnn','transformer']\n",
        "\n",
        "plt.figure(figsize=(30,20))\n",
        "\n",
        "# Plot the LOWESS+Standardized (*_stdlowess columns) SentimentArcs for Ensemble Models\n",
        "for acol in cols_ensemble_ls:\n",
        "  # print(f'Processing Model: {acol}')\n",
        "  if (acol in models_lexicon_ls) & (Model_Family == 'lexicon'):\n",
        "    cols_family_ls = models_lexicon_ls\n",
        "    alinestyle = 'solid'\n",
        "    acol_stdlowess = f'{acol}_stdlowess'\n",
        "    ensemble_df[acol_stdlowess].plot(alpha=0.5, label=acol_stdlowess, linewidth=2, linestyle=alinestyle)\n",
        "  elif (acol in models_heuristic_ls) & (Model_Family == 'heuristic'):\n",
        "    cols_family_ls = models_heuristic_ls\n",
        "    alinestyle = 'solid'\n",
        "    # Catch exception with LOWESS error on lmcd_sentimentr (too many zero values?)\n",
        "    # BUGFIX for lmcd_sentimentr, works with LOWESS smoothing of STD+SMA\n",
        "    # if acol.startswith('lmcd_sentimentr'):\n",
        "    if acol.startswith('execute_else_stmt'):\n",
        "      # Keep this error check inplace in case future novels/models/params replicate this problem\n",
        "      continue\n",
        "    else:\n",
        "      acol_stdlowess = f'{acol}_stdlowess'\n",
        "      ensemble_df[acol_stdlowess].plot(alpha=0.5, label=acol_stdlowess, linewidth=2, linestyle=alinestyle)\n",
        "  elif (acol in models_tradml_ls) & (Model_Family == 'tradml'):\n",
        "    cols_family_ls = models_tradml_ls\n",
        "    alinestyle = 'dashdot'\n",
        "    acol_stdlowess = f'{acol}_stdlowess'\n",
        "    ensemble_df[acol_stdlowess].plot(alpha=0.5, label=acol_stdlowess, linewidth=2, linestyle=alinestyle)\n",
        "  elif (acol in models_dnn_ls) & (Model_Family == 'dnn'):\n",
        "    cols_family_ls = models_dnn_ls\n",
        "    alinestyle = 'dotted'\n",
        "    acol_stdlowess = f'{acol}_stdlowess'\n",
        "    ensemble_df[acol_stdlowess].plot(alpha=0.5, label=acol_stdlowess, linewidth=2, linestyle=alinestyle)  \n",
        "  elif (acol in models_transformer_ls) & (Model_Family == 'transformer'):\n",
        "    cols_family_ls = models_transformer_ls\n",
        "    alinestyle='dashed'\n",
        "    acol_stdlowess = f'{acol}_stdlowess'\n",
        "    ensemble_df[acol_stdlowess].plot(alpha=0.5, label=acol_stdlowess, linewidth=2, linestyle=alinestyle)\n",
        "\n",
        "plt.grid(True)\n",
        "plt.title(f'{Novel_Title[0]} \\n SentimentArcs for {len(cols_family_ls)} {Model_Family.capitalize()} Models in Ensemble Models \\n LOWESS frac={LOWESS_Fraction} Smoothing of {Model_Standardization_Method} Standardized SMA {Window_Percent}%', fontsize=20)\n",
        "plt.xlabel('Sentence Number')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.legend(title='Model Name', loc='best')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', title='Model Name')\n",
        "plt.tight_layout()\n",
        "\n",
        "if Save_Plot:\n",
        "  # get_fullpath(ftype='data_clean', first_note='vader', last_note='anote bnote', plot_ext='png', no_date=True)\n",
        "  if HiRes_300dpi:\n",
        "    # fig.set_size_inches(30.,18.)\n",
        "    # figsize_tup = (40,20)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'lowess{LOWESS_Fraction}_of_{Model_Standardization_Method}_SMA{Window_Percent}_300dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=300)\n",
        "    print(f'Saved hi-res 300dpi Plot to: {plot_fullpath}')\n",
        "  else:\n",
        "    # figsize_tup = (20,10)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'lowess{LOWESS_Fraction}_of_{Model_Standardization_Method}_SMA{Window_Percent}__100dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=100)\n",
        "    print(f'Saved low-res 100dpi Plot to: {plot_fullpath}')\n",
        "\n",
        "plt.show();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojwqGVxTYJ0U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wbfjnorYKKq"
      },
      "source": [
        "# Save Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SzT_IFxYKKr"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA9pNyrWYKKs"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n4ughBqYKKs"
      },
      "source": [
        "# Save newly cleaned (Smoothed then Standardized) Ensemble Model data\n",
        "\n",
        "file_fullpath = get_fullpath(ftype='data_clean')\n",
        "ensemble_df.to_csv(file_fullpath)\n",
        "\n",
        "print(f'Saved to file: {file_fullpath}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KDYiVRVYKKu"
      },
      "source": [
        "# Verify saved file contents\n",
        "\n",
        "!head -n 10 $file_fullpath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl9mUv_RNg1x"
      },
      "source": [
        "# Explore Cruxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vySHv5-JEywP"
      },
      "source": [
        "list(range(10,15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q98_0WL_jSr"
      },
      "source": [
        "def get_crux_context(adf=ensemble_df, acol_sentraw='sent_raw', asent_no=5, ahalf_win=2, do_upper=True):\n",
        "  '''\n",
        "  Given a DataFrame with acol_sentraw of Sentence raw text along with\n",
        "    a crux sent_no and half_win number of sentences on before and after crux point\n",
        "  Return a list of Sentences that define the (2*ahalf_win + 1) context window around the Crux asent_no\n",
        "  '''\n",
        "\n",
        "  context_ls = []\n",
        "\n",
        "  corpus_len = adf.shape[0]\n",
        "\n",
        "  # Get lower bound of Crux Context Window\n",
        "  context_min = asent_no - ahalf_win\n",
        "  if context_min < 0:\n",
        "    context_min = 0\n",
        "\n",
        "  # Get upper bound of Crux Context Window\n",
        "  context_max = asent_no + ahalf_win + 1 # correct for zero-based indexing \n",
        "  if context_max > corpus_len:\n",
        "    context_max = corpus_len - 1\n",
        "\n",
        "  lineno_ls = list(range(context_min,context_max))\n",
        "  context_ls = adf.iloc[context_min:context_max][acol_sentraw].to_list()\n",
        "\n",
        "  if do_upper:\n",
        "    # [f(x) if condition else g(x) for x in sequence]\n",
        "    crux_str = adf.iloc[asent_no][acol_sentraw]\n",
        "    context_ls = [x.upper() if x == crux_str else x for x in context_ls]\n",
        "\n",
        "  context_tup_ls = list(zip(lineno_ls, context_ls))\n",
        "\n",
        "  return context_tup_ls\n",
        "\n",
        "# Test\n",
        "\n",
        "temp_tup_ls = get_crux_context(ensemble_df, 'sent_raw', 20, 5)\n",
        "temp_tup_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DxcfeGzJ68A"
      },
      "source": [
        "from scipy import interpolate\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy import signal\n",
        "from scipy.signal import argrelextrema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8H13HrsJGnt"
      },
      "source": [
        "def get_crux_points(adf, acol_name, text_type='sentence', win_per=5, sec_y_labels=True, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame and a Time Series Column within it and a LOWESS window\n",
        "  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n",
        "  '''\n",
        "\n",
        "  # print('entered get_crux_points') \n",
        "  crux_ls = []\n",
        "\n",
        "  series_len = adf.shape[0]\n",
        "  # print(f'series_len = {series_len}')\n",
        "\n",
        "  # sent_no_min = adf[].min()\n",
        "  sent_no_min = 0\n",
        "  # sent_no_max = adf.sent_no.max()\n",
        "  sent_no_max = series_len - 1\n",
        "  # print(f'sent_no_min {sent_no_min}')\n",
        "\n",
        "  sm_x = adf.index.values\n",
        "  sm_y = adf[acol_name].values.flatten()\n",
        "\n",
        "  half_win = int((win_per/100)*series_len)\n",
        "  # print(f'half_win = {half_win}')\n",
        "  # print(f'sm_y type = {type(sm_y)}')\n",
        "\n",
        "  # Find peaks(max).\n",
        "  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n",
        "  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n",
        "  # peak_indexes = peak_indexes + sent_no_min\n",
        "  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n",
        "  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n",
        "  # peak_indexes_np = peak_indexes_np + sent_no_min\n",
        "  # print(f'peak_indexes type = {type(peak_indexes)}') # sent_no_start sent\n",
        "  peak_indexes = peak_indexes[0]\n",
        "\n",
        "  peak_x_ls = list(peak_indexes)\n",
        "  peak_x_adj_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "\n",
        "  peak_y_ls = list(sm_y[peak_indexes])\n",
        "\n",
        "  peak_label_ls = ['peak'] * len(peak_y_ls)\n",
        "  peak_coord_ls = tuple(zip(peak_label_ls, peak_x_adj_ls, peak_y_ls))\n",
        "\n",
        "  # peak_y_all_ls = peak_y_ls + valley_y_ls\n",
        "  # crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # Find valleys(min).\n",
        "  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n",
        "  valley_indexes = valley_indexes[0]\n",
        "  \n",
        "  valley_x_ls = list(valley_indexes)\n",
        "  valley_x_adj_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "\n",
        "  valley_y_ls = list(sm_y[valley_indexes])\n",
        "\n",
        "  valley_label_ls = ['valley'] * len(valley_y_ls)\n",
        "  valley_coord_ls = tuple(zip(valley_label_ls, valley_x_adj_ls, valley_y_ls))\n",
        "\n",
        "  # Combine Peaks and Valley Coordinates into List of Tuples(label, x_coord, y_coord)\n",
        "  crux_coord_ls = peak_coord_ls + valley_coord_ls\n",
        "\n",
        "  # Save all peaks/valleys as list of (x,y) coordinate tuples\n",
        "  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n",
        "  #  x_all_ls = peak_x_ls + valley_x_ls\n",
        "  # readjust starting Sentence No to start with first sentence in segement window\n",
        "  #  x_all_ls = [x+sent_no_min for x in x_all_ls]\n",
        "  #  y_all_ls = peak_y_ls + valley_y_ls\n",
        "  # crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n",
        "\n",
        "\n",
        "  if do_plot == True:\n",
        "    # Plot main graph.\n",
        "    (fig, ax) = plt.subplots()\n",
        "    ax.plot(sm_x, sm_y)\n",
        "\n",
        "    # corpus_sects_df\n",
        "    if sec_y_labels == True:\n",
        "      section_sent_no_boundries_ls = list(corpus_sects_df['sent_no_start'])\n",
        "      section_no_ls = list(corpus_sects_df['sect_no'])\n",
        "      for i, asect_no in enumerate(section_sent_no_boundries_ls):\n",
        "        # Plot vertical lines for section boundries\n",
        "        plt.text(asect_no, sec_y_height, f'Section #{section_no_ls[i]}', alpha=0.2, rotation=90)\n",
        "        plt.axvline(asect_no, color='blue', alpha=0.1)    \n",
        "\n",
        "\n",
        "    win_half = 0 # 2500\n",
        "\n",
        "    # Plot peaks.\n",
        "    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n",
        "\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "    ax.scatter(peak_x_ls, peak_y_ls)\n",
        "    for i, txt in enumerate(list(peak_x_ls)):\n",
        "        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, ha='center', va='bottom', annotation_clip=True)\n",
        "\n",
        "    # Plot valleys.\n",
        "    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "    ax.scatter(valley_x_ls, valley_y_ls)\n",
        "    for i, txt in enumerate(list(valley_x_ls)):\n",
        "        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, ha='center', va='top', annotation_clip=True) # xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n",
        "\n",
        "    # for i, txt in enumerate(list(valley_x_ls)):\n",
        "    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n",
        "    # plt.plot(x, y, 'bo')\n",
        "    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n",
        "    # adjust_text(texts)\n",
        "\n",
        "    # Confidence Interval (Min/Max Range)\n",
        "    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n",
        "    plt.suptitle(f'{Novel_Title[0]} \\n SentimentArc Crux Detection for Model: {acol_name} \\n SMA ({Window_Percent}%) Smoothed then Standardized ({Model_Standardization_Method}) \\n {subtitle_str}', size=16, y=1.05);    \n",
        "    # plt.title(f'{Novel_Title[0]} \\n SentimentArc Crux Detection for Model: {acol_name} \\n SMA ({Window_Percent}%) Smoothed then Standardized ({Model_Standardization_Method}) \\n {subtitle_str}')\n",
        "    plt.xlabel(f'Sentence No') # within selected Section #{Select_Section_No}')\n",
        "\n",
        "    # locs, labels = xticks()  # Get the current locations and labels.\n",
        "    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n",
        "\n",
        "    plt.ylabel(f'Sentiment Value')\n",
        "    # plt.legend(loc='best');\n",
        "  \n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plt.title(f'{BOOK_TITLE_FULL} \\n SMA Smoothed Sentence Sentiment Arcs Crux Points')\n",
        "    # plt.legend(loc='best')\n",
        "    plt.savefig(f\"{CORPUS_FILENAME.split('.')[0]}_find_peaks.png\")\n",
        "\n",
        "  return crux_coord_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QVPFLrSKqvm"
      },
      "source": [
        "# Test\n",
        "\n",
        "crux_tup_ls = get_crux_points(ensemble_df, 'vader_stdsma', text_type='sentence', win_per=5, sec_y_labels=False, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdEn08QG8tvX"
      },
      "source": [
        "def crux_sortsents_report(crux_ls, ts_df=ensemble_df, library_type='baseline', top_n=3, get_peaks=True, sort_by='sent_no', n_sideparags=1, sentence_highlight=True):\n",
        "  '''\n",
        "  Wrapper function to produce report based upon 'crux_sortsents() described as:\n",
        "    Given a list of tuples (sent_no, sentiment value), top_n cruxes to retrieve and bool flag get_peaks\n",
        "    Return a sorted list of peaks/valleys (sentiment_value, sent_no, sent_raw) from greatest down for top_n items\n",
        "\n",
        "    # get_sentnocontext_report\n",
        "  '''\n",
        "\n",
        "  if get_peaks == True:\n",
        "    crux_label = 'Peak'\n",
        "  else:\n",
        "    crux_label = 'Valley'\n",
        "\n",
        "  # Filter and keep only the desired crux type in List crux_subset_ls\n",
        "  crux_subset_ls = []\n",
        "  for acrux_tup in crux_ls:\n",
        "    crux_type, crux_x_coord, crux_y_coord = acrux_tup\n",
        "    if crux_type.lower() == crux_label.lower():\n",
        "      crux_subset_ls.append((crux_x_coord,crux_y_coord)) # Append a Tuple to List\n",
        "\n",
        "  flag_2few_cruxes = False\n",
        "\n",
        "  # Check to see if asked for more Cruxes than were found \n",
        "  top_n_found = len(crux_subset_ls)\n",
        "  if top_n_found < top_n:\n",
        "    flag_2few_cruxes = True\n",
        "    print(f'\\n\\nWARNING: You asked for {top_n} {crux_label}s\\n         but there only {top_n_found} were found above.\\n')\n",
        "    print(f'             Displaying as many {crux_label}s as possible,')\n",
        "    print(f'             to retrieve more, go back to the previous code cells and re-run with wider Crux Window.\\n\\n')\n",
        "\n",
        "\n",
        "  # Get Sentence no and raw text for appropriate Crux subset\n",
        "  # print(f'Calling crux_n_top_ls with crux_subset_ls={crux_subset_ls}\\ntop_n={top_n}\\nget_peaks={get_peaks}')\n",
        "  crux_n_top_ls = crux_sortsents(corpus_df = ts_df, crux_ls=crux_subset_ls, atop_n=top_n, get_peaks=get_peaks, sort_key=sort_by)\n",
        "  # print(f'Returning crux_n_top_ls = {crux_n_top_ls}')\n",
        "\n",
        "  # Print appropriate header Select_Section_No sent_no\n",
        "  print('------------------------------')\n",
        "  # print(f'library_type: {library_type}')\n",
        "  if library_type in ['baseline','sentimentr','syuzhetr','transformer','unified']:\n",
        "    if (sort_by != 'sent_no') & (flag_2few_cruxes == False):\n",
        "      print(f'Library: {library_type.capitalize()} ALL Top {top_n} {crux_label}s Found\\n')\n",
        "    else:\n",
        "      print(f'Library #{library_type.capitalize()} ONLY Top {top_n_found} {crux_label}s Found\\n')\n",
        "  else:\n",
        "    if (sort_by != 'sent_no') & (flag_2few_cruxes == False):\n",
        "      print(f'Section #{Select_Section_No} ALL Top {top_n} {crux_label}s Found\\n')\n",
        "    else:\n",
        "      print(f'Section #{Select_Section_No} ONLY Top {top_n_found} {crux_label}s Found\\n')\n",
        "\n",
        "  # Print summary of subset Cruxes\n",
        "  for i,crux_sent_tup in enumerate(crux_n_top_ls):\n",
        "    # crux_type, crux_x_coord, crux_y_coord = crux_sent_tup\n",
        "    crux_x_coord, crux_y_coord, crux_sent_raw = crux_sent_tup\n",
        "    print(f'   {crux_label} #{i} at Sentence #{crux_x_coord} with Sentiment Value {crux_y_coord}')\n",
        "  # print('------------------------------\\n')\n",
        "  # print('Sent_No  Sentiment   Sentence (Raw Text)\\n')\n",
        "  \n",
        "  # Print details of each Crux in subset\n",
        "  for sent_no, sent_pol, sent_raw in crux_n_top_ls: \n",
        "    sent_no = int(sent_no)\n",
        "    print('\\n\\n-------------------------------------------------------------')\n",
        "    print(f'Sentence #{sent_no}   Sentiment: {sent_pol:.3f}\\n') #     {sent_raw}\\n')\n",
        "    # print('------------------------------')\n",
        "    get_sentnocontext_report(ts_df=ts_df, the_sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)\n",
        "    # get_sentnocontext(sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRPog2AY_g-M"
      },
      "source": [
        "def get_lowess_cruxes(ts_df, col_series, text_type='sentence', win_lowess=5, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame and a Time Series Column within it and a LOWESS window\n",
        "  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n",
        "  '''\n",
        "\n",
        "  crux_ls = []\n",
        "\n",
        "  series_len = ts_df.shape[0]\n",
        "\n",
        "  sent_no_min = ts_df.sent_no.min()\n",
        "  sent_no_max = ts_df.sent_no.max()\n",
        "  # print(f'sent_no_min {sent_no_min}')\n",
        "\n",
        "  sm_x = ts_df.index.values\n",
        "  sm_y = ts_df[col_series].values\n",
        "\n",
        "  half_win = int((win_lowess/100)*series_len)\n",
        "\n",
        "  # Find peaks(max).\n",
        "  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n",
        "  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n",
        "  # peak_indexes = peak_indexes + sent_no_min\n",
        "  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n",
        "  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n",
        "  # peak_indexes_np = peak_indexes_np + sent_no_min\n",
        "  peak_indexes = peak_indexes[0]\n",
        "\n",
        "  peak_x_ls = list(peak_indexes)\n",
        "  peak_y_ls = list(sm_y[peak_indexes])\n",
        "\n",
        "  # Find valleys(min).\n",
        "  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n",
        "  valley_indexes = valley_indexes[0]\n",
        "  \n",
        "  valley_x_ls = list(valley_indexes)\n",
        "  valley_y_ls = list(sm_y[valley_indexes])\n",
        "\n",
        "  # Save all peaks/valleys as list of (x,y) coordinate tuples\n",
        "  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n",
        "  x_all_ls = peak_x_ls + valley_x_ls\n",
        "  # readjust starting Sentence No to start with first sentence in segement window\n",
        "  x_all_ls = [x+sent_no_min for x in x_all_ls]\n",
        "  y_all_ls = peak_y_ls + valley_y_ls\n",
        "  crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n",
        "\n",
        "\n",
        "  if do_plot == True:\n",
        "    # Plot main graph.\n",
        "    (fig, ax) = plt.subplots()\n",
        "    ax.plot(sm_x, sm_y)\n",
        "\n",
        "    if text_type == 'sentence':\n",
        "      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "      for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "        if i%5 == 0:\n",
        "          # Plot every 5th paragraph\n",
        "          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "          plt.text(sent_no, sec_y_height, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "          plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "    elif text_type == 'paragraph':\n",
        "      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "      for i, aparag_no in enumerate(paragraph_boundries_ls):\n",
        "        if i%5 == 0:\n",
        "          # Plot every 5th paragraph\n",
        "          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "          plt.text(aparag_no, sec_y_height, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n",
        "          plt.axvline(aparag_no, color='blue', alpha=0.1)    \n",
        "    else:\n",
        "      print(f\"ERROR: text_type is {text_type} but must be either 'sentence' or 'paragarph'\")\n",
        "\n",
        "    win_half = 0 # 2500\n",
        "\n",
        "    # Plot peaks.\n",
        "    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n",
        "\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "    ax.scatter(peak_x_ls, peak_y_ls)\n",
        "    for i, txt in enumerate(list(peak_x_ls)):\n",
        "        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n",
        "\n",
        "    # Plot valleys.\n",
        "    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "    ax.scatter(valley_x_ls, valley_y_ls)\n",
        "    for i, txt in enumerate(list(valley_x_ls)):\n",
        "        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n",
        "\n",
        "    # for i, txt in enumerate(list(valley_x_ls)):\n",
        "    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n",
        "    # plt.plot(x, y, 'bo')\n",
        "    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n",
        "    # adjust_text(texts)\n",
        "\n",
        "    # Confidence Interval (Min/Max Range)\n",
        "    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} Raw Sentence Crux Detection in Section #{Select_Section_No}\\nLOWESS Smoothed {subtitle_str} and SciPy find_peaks')\n",
        "    plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "\n",
        "    # locs, labels = xticks()  # Get the current locations and labels.\n",
        "    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n",
        "\n",
        "    plt.ylabel(f'Sentiment Value')\n",
        "    plt.legend(loc='best');\n",
        "  \n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Crux Points via SciPy.argrelextrema')\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig('argrelextrema.png')\n",
        "\n",
        "  return crux_coord_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8QX8tSKM376"
      },
      "source": [
        "## Search Corpus for Substring\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* In [Search_for_Substring] enter a Substring to search for in the Corpus\n",
        "\n",
        "* Enter a Substring long enough/unique enough so only a reasonable number of Sentences will be returned\n",
        "\n",
        "* Substring can contain spaces/punctuation, for example: 'in the garden'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJYjOu9Ks_pL"
      },
      "source": [
        "# Search Corpus Sentences for Substring\n",
        "\n",
        "Search_for_Substring = \"abuse\" #@param {type:\"string\"}\n",
        "\n",
        "match_sentno_ls = ensemble_df[ensemble_df['sent_raw'].str.contains(Search_for_Substring, regex=False)]['sent_no']\n",
        "\n",
        "for i, asentno in enumerate(match_sentno_ls):\n",
        "  # sentno, sentraw = asent\n",
        "  print(f\"\\n\\nMatch #{i}: Sentence #{asentno}\\n\\n\")\n",
        "  sent_highlight = re.sub(Search_for_Substring, Search_for_Substring.upper(), ensemble_df.iloc[asentno]['sent_raw'])\n",
        "  print(f'    {sent_highlight}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajEpjFVPa6jD"
      },
      "source": [
        "# Get Context around Matching Sentences\n",
        "\n",
        "#@title Get Context Around Each Matching Sentence:\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "\n",
        "crux_context_dt = {}\n",
        "for i, asent_no in enumerate(match_sentno_ls):\n",
        "  crux_tup_ls = get_crux_context(adf=ensemble_df, acol_sentraw='sent_raw', asent_no=asent_no, ahalf_win=No_Paragraphs_on_Each_Side, do_upper=Highlight_Sentence)\n",
        "  crux_context_dt[asent_no] = crux_tup_ls\n",
        "\n",
        "\n",
        "for key, val in crux_context_dt.items():\n",
        "  print(f'\\nCrux No: {key}')\n",
        "  for aval in val:\n",
        "    alineno, asent = aval\n",
        "    print(f'  Line #{alineno}: {asent}')\n",
        "  print('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap_K_gpH0FTm"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufnb6YB6HFtL"
      },
      "source": [
        "%whos list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br_uMcIoHJQn"
      },
      "source": [
        "# Generate the list of Model names currently selected to be in the Ensemble\n",
        "#   need to paste into drop down below\n",
        "\n",
        "# TODO: Automatically generate values for all models, not just those selected to be in Ensemble\n",
        "\n",
        "temp_ls = []\n",
        "\n",
        "for acol in cols_models_ls:\n",
        "  temp_ls.append(f\"'{acol}'\")\n",
        "\n",
        "temp_str = ','.join(temp_ls)\n",
        "temp_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHPx5a0xvJYb"
      },
      "source": [
        "Crux_Window_Percent = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "Model_Name = \"vader\" #@param ['afinn','bing','nrc','syuzhetr','sentimentr','pattern','vader','bing_sentimentr','sentiword_sentimentr','senticnet_sentimentr','lmcd_sentimentr','jockers_sentimentr','jockersrinker_sentimentr','logreg','logreg_cv','multinb','textblob','rf','xgb','flaml','autogluon','fcn','lstm','cnn','stanza','flair','huggingface','t5imdb50k','hinglish','yelp','imdb2way','nlptown','robertaxml8lang','roberta15lg','median']\n",
        "# Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "# Vertical_Labels = True #@param {type:\"boolean\"}\n",
        "# Vertical_Labels_Height = -0.1 #@param {type:\"slider\", min:-50, max:50, step:0.1}\n",
        "# Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "amodel_stdsma = f'{Model_Name}_stdsma'\n",
        "crux_tup_ls = get_crux_points(ensemble_df, amodel_stdsma, text_type='sentence', win_per=5, sec_y_labels=False, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6PU1zR8vJYf"
      },
      "source": [
        "## Context around Top-n Crux Peaks/Valleys\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6U2kzrfOeQ9"
      },
      "source": [
        "crux_tup_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUoJz_nyvJYh"
      },
      "source": [
        "#@title Get Context around Crux Points:\n",
        "\n",
        "#@markdown Crux Point Details\n",
        "# Get_Peak_Cruxes = False #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 20 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Context Details\n",
        "No_Paragraphs_on_Each_Side = 5 #@param {type:\"slider\", min:0, max:15, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "# Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "if Sort_by_SentenceNo == True:\n",
        "  sort_on = 'sent_no'\n",
        "else:\n",
        "  sort_on = Model_Name  # Selected in previous code cell\n",
        "\n",
        "\n",
        "crux_context_dt = {}\n",
        "crux_peaks_dt = {}\n",
        "crux_valleys_dt = {}\n",
        "for i, acrux_tup in enumerate(crux_tup_ls):\n",
        "  crux_type, crux_sentno, crux_sentiment = acrux_tup\n",
        "  acrux_ls = get_crux_context(adf=ensemble_df, acol_sentraw='sent_raw', asent_no=crux_sentno, ahalf_win=No_Paragraphs_on_Each_Side, do_upper=Highlight_Sentence)\n",
        "  if crux_type == 'peak':\n",
        "    if Sort_by_SentenceNo:\n",
        "      crux_peaks_dt[crux_sentno] = ['peak', crux_sentno, crux_sentiment, acrux_ls]\n",
        "    else:\n",
        "      crux_peaks_dt[crux_sentiment] = ['peak', crux_sentno, crux_sentiment, acrux_ls]\n",
        "  elif crux_type == 'valley':\n",
        "    if Sort_by_SentenceNo:\n",
        "      crux_valleys_dt[crux_sentno] = ['valley', crux_sentno, crux_sentiment, acrux_ls]\n",
        "    else:\n",
        "      crux_valleys_dt[crux_sentiment] = ['valley', crux_sentno, crux_sentiment, acrux_ls]\n",
        "\n",
        "if Sort_by_SentenceNo:\n",
        "  print(f'\\n\\n{Novel_Title[0]} \\nCrux Peaks sorted by Sentence No\\n====================\\n')\n",
        "  for key, val in crux_peaks_dt.items():\n",
        "    # print(f'\\nCrux at Sentence No: {key}')\n",
        "    acrux_type, acrux_sentno, acrux_sentiment, acrux_ls = val\n",
        "    print(f'\\nCrux at Sentence No: {key}  ({acrux_type}: {acrux_sentiment:.3f})')\n",
        "    for acrux_sent in acrux_ls:\n",
        "      print(f'    {acrux_sent}')\n",
        "  print(f'\\n\\n{Novel_Title[0]} \\nCrux Valleys sorted by Sentence No\\n====================\\n')\n",
        "  for key, val in crux_valleys_dt.items():\n",
        "    # print(f'\\nCrux at Sentence No: {key}')\n",
        "    acrux_type, acrux_sentno, acrux_sentiment, acrux_ls = val\n",
        "    print(f'\\nCrux at Sentence No: {key}  ({acrux_type}: {acrux_sentiment:.3f})')\n",
        "    for acrux_sent in acrux_ls:\n",
        "      print(f'    {acrux_sent}')\n",
        "\n",
        "\"\"\"\n",
        "else:\n",
        "  for key, val in crux_peaks_dt.items():\n",
        "    print(f'\\nCrux at Polarity: {key}')\n",
        "    acrux_sentno, acrux_sentiment, acrux_ls = val\n",
        "    print(f'  Sentiment: {acrux_sentiment}')\n",
        "    for acrux_sent in acrux_ls:\n",
        "      print(f'    {acrux_sent}')\n",
        "\n",
        "\n",
        "\n",
        "    for aval in val:\n",
        "      acrux_sentno, acrux_sentiment, acrux_ls = aval\n",
        "      print(f'  Sentiment: {acrux_sentiment}')\n",
        "      for aline \n",
        "      print(f'  Line #{alineno}: {asent}')\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "print(f'Crux Report --------------------\\n')\n",
        "print(f'            Corpus: {CORPUS_FULL}')\n",
        "print(f'            Model: {Baseline_SMA_Model}')\n",
        "print(f'            Crux Win%: {Crux_Window_Percent}')\n",
        "print(f'            SMA Win%: {sma_str}')\n",
        "\n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        ts_df = corpus_sents_df,\n",
        "                        library_type='baseline', \n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes,\n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "else:\n",
        "  # import sys\n",
        "  # with open('filename.txt', 'w') as f:\n",
        "  #   print('This message will be written to a file.', file=f)\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky-Ku-1DU4yq"
      },
      "source": [
        "### Save Crux to File\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad86Yzl-E_yu"
      },
      "source": [
        "%%capture cap --no-stderr\n",
        "\n",
        "# Print Context around each Sentiment Peak\n",
        "\n",
        "\"\"\"\n",
        "novel_sent_len = novel_df.shape[0]\n",
        "halfwin = int(Crux_Sentence_Context_Count/2)\n",
        "crux_sents_ls = []\n",
        "nl = '\\n'\n",
        "\"\"\";\n",
        "\n",
        "print('==================================================')\n",
        "print('============     Peak Crux Points   ==============')\n",
        "print('==================================================\\n\\n')\n",
        "\n",
        "if Sort_by_SentenceNo:\n",
        "  # print(f'\\n\\n{Novel_Title[0]} \\nCrux Peaks sorted by Sentence No\\n====================\\n')\n",
        "  for key, val in crux_peaks_dt.items():\n",
        "    # print(f'\\nCrux at Sentence No: {key}')\n",
        "    acrux_type, acrux_sentno, acrux_sentiment, acrux_ls = val\n",
        "    print(f'\\nCrux at Sentence No: {key}  ({acrux_type}: {acrux_sentiment:.3f})')\n",
        "    for acrux_sent in acrux_ls:\n",
        "      print(f'    {acrux_sent}')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# for i, apeak in enumerate(peaks2):\n",
        "for i, apeak in enumerate(peaks):\n",
        "  crux_sents_ls = []\n",
        "  win_start = max(0, apeak-halfwin)\n",
        "  win_end = min(apeak+halfwin+1, novel_sent_len)\n",
        "  # for sent_idx in range(apeak-halfwin,apeak+halfwin+1):\n",
        "  for sent_idx in range(win_start,win_end):\n",
        "\n",
        "    sent_cur = novel_df.iloc[sent_idx].text_raw\n",
        "    if sent_idx == apeak:\n",
        "      sent_str = sent_cur.upper()\n",
        "    else:\n",
        "      sent_str = sent_cur\n",
        "    crux_sents_ls.append(sent_str)\n",
        "  \n",
        "  # context_ls = novel_df.iloc[apeak-halfwin:apeak+halfwin].text_raw\n",
        "  print(f\"Peak #{i} at Sentence #{apeak}:\\n\\n{nl.join(crux_sents_ls)}\\n\\n\\n\")\n",
        "\"\"\";\n",
        "\n",
        "\n",
        "print('==================================================')\n",
        "print('===========     Crux Valley Points    ============')\n",
        "print('==================================================\\n\\n')\n",
        "\n",
        "if Sort_by_SentenceNo:\n",
        "  # print(f'\\n\\n{Novel_Title[0]} \\nCrux Valleys sorted by Sentence No\\n====================\\n')\n",
        "  for key, val in crux_valleys_dt.items():\n",
        "    # print(f'\\nCrux at Sentence No: {key}')\n",
        "    acrux_type, acrux_sentno, acrux_sentiment, acrux_ls = val\n",
        "    print(f'\\nCrux at Sentence No: {key}  ({acrux_type}: {acrux_sentiment:.3f})')\n",
        "    for acrux_sent in acrux_ls:\n",
        "      print(f'    {acrux_sent}')\n",
        "\n",
        "\"\"\"\n",
        "# for i, avalley in enumerate(valleys2):\n",
        "for i, avalley in enumerate(valleys):\n",
        "  crux_sents_ls = []\n",
        "  win_start = max(0, avalley-halfwin)\n",
        "  win_end = min(avalley+halfwin+1, novel_sent_len)\n",
        "  # for sent_idx in range(avalley-halfwin,avalley+halfwin+1):\n",
        "  for sent_idx in range(win_start,win_end):\n",
        "    sent_cur = novel_df.iloc[sent_idx].text_raw\n",
        "    if sent_idx == avalley:\n",
        "      sent_str = sent_cur.upper()\n",
        "    else:\n",
        "      sent_str = sent_cur\n",
        "    crux_sents_ls.append(sent_str)\n",
        "\n",
        "  # context_ls = novel_df.iloc[avalley-halfwin:avalley+halfwin].text_raw\n",
        "  print(f\"Valley #{i} at Sentence #{avalley}:\\n\\n{nl.join(crux_sents_ls)}\\n\\n\\n\")\n",
        "\"\"\";\n",
        "\n",
        "# Save newly cleaned (Smoothed then Standardized) Ensemble Model data\n",
        "\n",
        "file_fullpath = get_fullpath(ftype='crux_text')\n",
        "# ensemble_df.to_csv(file_fullpath)\n",
        "\n",
        "# print(f'Saved to file: {file_fullpath}')\n",
        "\n",
        "# filename_cruxes = f\"cruxes_context_{Novel_Title[0].replace(' ', '_')}.txt\" \n",
        "\n",
        "with open(file_fullpath, 'w') as f:\n",
        "    f.write(str(cap))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2OSRfUF3el1"
      },
      "source": [
        "# Download Crux Point Report file 'cruxes.txt' to your laptop\n",
        "\n",
        "print(f'Downloading crux text file: {file_fullpath}')\n",
        "files.download(file_fullpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPNOY6adXkiV"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My51TiJcMiBF"
      },
      "source": [
        "# **CONTENT OF NEXT NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcsHB9yUX2q3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK1e6dLkN6Dv"
      },
      "source": [
        "# Smooth, Dimensionally Reduce and Cluster SentimentArcs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXfU_le6KQd2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsTwPtIdOUl2"
      },
      "source": [
        "# Get Correlation Heatmaps of SentimentArcs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7j7XWMiG5yQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kDW1twRG856"
      },
      "source": [
        "# Get SentimentArcs Metrics and Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoAZtW8jG5ui"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxFqZhDQOcW4"
      },
      "source": [
        "# Summarize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QKTy2ZTMjMI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}