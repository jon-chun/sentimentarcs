{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_arcs_cluster.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qJB8wF8yjj5j"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jon-chun/sentimentarcs/blob/main/sentiment_arcs_cluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i0Fg4SYB7g0"
      },
      "source": [
        "# **Sentiment Arcs: Self-Supervising Time Series Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmPyKSI_ixIw"
      },
      "source": [
        "## [MANUAL INPUT] Connect to Google gDrive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bfkqjgMiw7T"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmDI6aEFivRe"
      },
      "source": [
        "# **Setup and Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84b-vTPlVWS4"
      },
      "source": [
        "%cd /gdrive/MyDrive/research/2021/sentiment_arcs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bpu4KdMVWS6"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ508_9rVWTE"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdNlqRE7VWS7"
      },
      "source": [
        "# CUSTOMIZE: define subdirectory paths\n",
        "\n",
        "data_raw_subdir  = './data/sentiments_raw/'\n",
        "data_clean_subdir  = './data/sentiments_clean/'\n",
        "plots_subdir = './plots/'\n",
        "crux_subdir = './crux/'\n",
        "code_subdir  = './sentiment_arcs/sentiment_arcs/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HUYO6o5VWS8"
      },
      "source": [
        "## Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IOscpVpVWS9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6UX66TmVWS9"
      },
      "source": [
        "## Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP7vPUnSVWS-"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr7CDA7BVWS_"
      },
      "source": [
        "import re\n",
        "from datetime import datetime\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMsmrqvwVWTC"
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"]=(20,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz33k9sAVWTA"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4UU_JjWIx46"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rshIrryVwnh"
      },
      "source": [
        "# [SKIP] to next section"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i0b1_4GRzTO"
      },
      "source": [
        "import statsmodels.api as sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_vZlMMXVWTB"
      },
      "source": [
        "# https://www.statsmodels.org/devel/examples/notebooks/generated/lowess.html\n",
        "\"\"\"\n",
        "import pylab\n",
        "import statsmodels.api as sm\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "pylab.rc(\"figure\", figsize=(16, 8))\n",
        "pylab.rc(\"font\", size=14)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DKD2FJKVWTD"
      },
      "source": [
        "## Configure Jupyter Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4m2nYAHI5a7"
      },
      "source": [
        "# Automatically reload changed libraries\n",
        "\n",
        "# https://switowski.com/blog/ipython-autoreload\n",
        "# import importlib\n",
        "# importlib.reload(sentiment_arcs_utils.get_fullpath)\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CkX9gONAsmp"
      },
      "source": [
        "# Ignore warnings\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK8zKENjsyig"
      },
      "source": [
        "# Configure Jupyter\n",
        "\n",
        "# Enable multiple outputs from one code cell\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "from IPython.display import display\n",
        "from ipywidgets import widgets, interactive\n",
        "\n",
        "# Configure Google Colab\n",
        "\n",
        "# %load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beGB-p4T7wCC"
      },
      "source": [
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DINP3VWXpi3C"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPhnMpJwVWTF"
      },
      "source": [
        "# **Utility Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQEiFngTTgaU"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTEFbG09TixT"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRzOAuS-Uq2q"
      },
      "source": [
        "# Add sentiment_arcs.py to the sys path in order to import library\n",
        "\n",
        "import sys\n",
        "\n",
        "# sys.path.append('/content/gdrive/mypythondirectory')\n",
        "\n",
        "sys.path.append('/gdrive/My Drive/research/2021/sentiment_arcs/sentiment_arcs')\n",
        "\n",
        "sys.path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdaf6H2mVTiZ"
      },
      "source": [
        "!ls sentiment_arcs/*.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVBYhJRumLIo"
      },
      "source": [
        "# import importlib\n",
        "\n",
        "# importlib.reload(get_fullpath)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q_EXZ13mcjh"
      },
      "source": [
        "# %aimport sentiment_arcs_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3908VHlvTita"
      },
      "source": [
        "# from sentiment_arcs_utils import get_fullpath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMRljNEPl-T4"
      },
      "source": [
        "# ??get_fullpath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7xne0GWVWTG"
      },
      "source": [
        "def get_fullpath( ftype='data_clean', first_note = '',last_note='', plot_ext='png', no_date=False):\n",
        "  '''\n",
        "  Given a required file_type(ftype:['data_clean','data_raw','plot']) and\n",
        "    optional first_note: str inserted after Title and before (optional) SMA/Standardization info\n",
        "             last_note: str insterted after (optional) SMA/Standardization info and before (optional) timedate stamp\n",
        "             plot_ext: change default *.png extension of plot file\n",
        "             no_date: don't add trailing datetime stamp to filename\n",
        "  Generate and return a fullpath (/subdir/filename.ext) to save file to\n",
        "  '''\n",
        "\n",
        "  # String with full path/filename.ext to return\n",
        "  fname = ''\n",
        "\n",
        "  # Get current datetime stamp as a string\n",
        "  if no_date:\n",
        "    date_dt = ''\n",
        "  else:\n",
        "    date_dt = f'_{datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")}'\n",
        "\n",
        "  # Clean optional file notation if passed in\n",
        "  if first_note:\n",
        "    fnote_str = first_note.replace(' ', '_')\n",
        "    fnote_str = '_'.join(fnote_str.split())\n",
        "    fnote_str = '_'.join(fnote_str.split('.'))\n",
        "    fnote_str = '_'.join(fnote_str.split('__'))\n",
        "    fnote_str = fnote_str.lower()\n",
        "\n",
        "  # Get Current Novel Name and Clean\n",
        "  novel_title_str = Novel_Title[0].replace(' ', '_').lower()\n",
        "  novel_title_str = '_'.join(novel_title_str.split())\n",
        "  novel_title_str = '_'.join(novel_title_str.split('.'))\n",
        "  novel_title_str = '_'.join(novel_title_str.split('__'))\n",
        "  if first_note:\n",
        "    novel_title_str = f'{novel_title_str}_{first_note}'\n",
        "\n",
        "  # Option (a): Cleaned Model Data (Smoothed then Standardized)\n",
        "  if ftype == 'data_clean':\n",
        "    subdir_path = data_clean_subdir\n",
        "    fprefix = 'sa_clean_'\n",
        "    fname_str = f'{subdir_path}{fprefix}{novel_title_str}_{Model_Standardization_Method.lower()}_sma{Window_Percent}'\n",
        "    if last_note:\n",
        "      fname = f'{fname_str}_{last_note}{date_dt}.csv'\n",
        "    else:\n",
        "      fname = f'{fname_str}{date_dt}.csv'\n",
        "\n",
        "  # Option (b): Raw Model Data\n",
        "  elif ftype == 'data_raw':\n",
        "    subdir_path = data_raw_subdir\n",
        "    fprefix = 'sa_raw_'\n",
        "    fname_str = f'{subdir_path}{fprefix}{novel_title_str}'\n",
        "    if last_note:\n",
        "      fname = f'{fname_str}_{last_note}{date_dt}.csv'\n",
        "    else:\n",
        "      fname = f'{fname_str}{date_dt}.csv'\n",
        "\n",
        "  # Option (c): Plot Figure\n",
        "  elif ftype == 'plot':\n",
        "    subdir_path = plots_subdir\n",
        "    fprefix = 'plot_'\n",
        "    fname_str = f'{subdir_path}{fprefix}{novel_title_str}'\n",
        "    if last_note:\n",
        "      fname = f'{fname_str}_{last_note}{date_dt}.{plot_ext}'\n",
        "    else:\n",
        "      fname = f'{fname_str}{date_dt}.{plot_ext}'\n",
        "\n",
        "  # Option (d): Crux Text\n",
        "  elif ftype == 'crux_text':\n",
        "    subdir_path = plots_subdir\n",
        "    fprefix = 'crux_'\n",
        "    fname_str = f'{subdir_path}{fprefix}{novel_title_str}'\n",
        "    if last_note:\n",
        "      fname = f'{fname_str}_{last_note}{date_dt}.txt'\n",
        "    else:\n",
        "      fname = f'{fname_str}{date_dt}.txt'\n",
        "\n",
        "  else:\n",
        "    print(f'ERROR: In get_fullpath() with illegal arg ftype:[{ftype}]')\n",
        "    return f'ERROR: ftype:[{ftype}]'\n",
        "\n",
        "  return fname"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASbKXX1VVWTI"
      },
      "source": [
        "# Test\n",
        "\n",
        "# NameError: name 'Novel_Title' is not defined\n",
        "\n",
        "# get_fullpath(ftype='data_clean', first_note='vader', last_note='anote bnote', plot_ext='png', no_date=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llBjZQdOVWTJ"
      },
      "source": [
        "# **Select Novels**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzBboEReVWTK"
      },
      "source": [
        "## Choose from Preexisting Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSFSn4DEVWTK"
      },
      "source": [
        "novels_dt = {\n",
        "  'cdickens_christmascarol':['A Christmas Carol by Charles Dickens ',1843,1399],\n",
        "  'cdickens_greatexpectations':['Great Expectations by Charles Dickens' ,1861, 7230],\n",
        "  'ddefoe_robinsoncrusoe':['Robinson Crusoe by Daniel Defoe',1719, 2280],\n",
        "  'emforester_howardsend':['Howards End by E.M. Forester', 1910, 8999],\n",
        "  'fbaum_wizardofoz':['The Wonderful Wizard of Oz by Frank Baum', 1850, 2238],\n",
        "  'fdouglass_narrativeofslave':['Narrative of the life of Frederick Douglass, an American Slave by Frederick Douglass', 1845, 1688],\n",
        "  'fscottfitzerald_greatgatsby':['The Great Gatsby by F. Scott Fitzgerald', 1925, 2950],\n",
        "  'geliot_middlemarch':['Middlemarch by George Eliot', 1871, 10373],\n",
        "  'hjames_portraitofalady':['The Portrait of a Lady by Henry James', 1881, 13258],\n",
        "  'homerwilson_homer':['The Odyssey by Homer (trans Emily Wilson)', 2018, 6814],\n",
        "  'imcewan_machineslikeme':['Machines Like Me by Ian McEwan', 2019, 6448],\n",
        "  'jausten_prideandprejudice':['Pride and Prejudice by Jane Austen', 1813, 5891],\n",
        "  'jconrad_heartofdarkness':['Heart of Darkness by Joseph Conrad', 1902, 1619],\n",
        "  'jjoyce_portraitoftheartist':['A Portrait of the Artist as a Young Man by James Joyce', 2016, 5584],\n",
        "  'jkrowling_pottersorcerersstone':['Harry Potter and the Sorcererâ€™s Stone by J.K. Rowling ', 1997, 5488],\n",
        "  'mproust_searchoflosttime':['In Search of Lost Time, Vol 3: The Guermantes Way by Marcel Proust', 1920, 8388],\n",
        "  'mshelly_frankenstein':['Frankenstein by Mary Shelly', 1818, 3282],\n",
        "  'mtwain_huckleberryfinn':['Huckleberry Finn by Mark Twain', 1884, 5775],\n",
        "  'staugustine_confessions':['Confessions (Books 1-9) by St. Augustine', 400, 3673],\n",
        "  'tmorrison_beloved':['Beloved by Toni Morrison', 1987, 7102],\n",
        "  'vnabokov_palefire':['Pale Fire by Viktor Nabokov', 1962, 2984],\n",
        "  'vwoolf_mrsdalloway':['Mrs. Dalloway by Virginia Woolf', 1925, 3647],\n",
        "  'vwoolf_orlando':['Orlando by Virginia Woolf', 1928, 2,992],\n",
        "  'vwoolf_thewaves':['The Waves by Virginia Woolf', 1931, 3919],\n",
        "  'vwoolf_tothelighthouse':['To The Lighthouse by Virginia Woolf', 1927, 3403],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBuPfh12VWTL"
      },
      "source": [
        "# Derive List of Novel a)keys and b)full author and titles\n",
        "\n",
        "novels_corpus_ls = list(novels_dt.keys())\n",
        "print(f'\\nNovel Columns:')\n",
        "for akey in novels_corpus_ls:\n",
        "  print(f'  {akey}')\n",
        "print('\\n')\n",
        "\n",
        "print(f'\\nNovel Titles:')\n",
        "novels_full_ls = [x[0] for x in list(novels_dt.values())]\n",
        "for akey in novels_full_ls:\n",
        "  print(f'  {akey}')\n",
        "  \n",
        "\"\"\"\n",
        "# Derive List of Novel a)keys and b)full author and titles\n",
        "\n",
        "novels_key_ls = list(novels_dt.keys())\n",
        "novels_key_ls\n",
        "\n",
        "novels_full_ls = [x[0] for x in list(novels_dt.values())]\n",
        "\n",
        "for akey in novels_full_ls:\n",
        "  print(akey)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf_ap7TZVWTM"
      },
      "source": [
        "# need?\n",
        "\n",
        "novels_corpus_ls = []\n",
        "\n",
        "for akey in novels_key_ls:\n",
        "  print(akey)\n",
        "  novels_corpus_ls.append(akey)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O9bJ3L2VWTM"
      },
      "source": [
        "#@title CORPORA: Select Which Novel(s) you want to Analyze:\n",
        "\n",
        "cdickens_christmascarol = False #@param {type:\"boolean\"}\n",
        "cdickens_greatexpectations = False #@param {type:\"boolean\"}\n",
        "ddefoe_robinsoncrusoe = False #@param {type:\"boolean\"}\n",
        "emforester_howardsend = False #@param {type:\"boolean\"}\n",
        "fbaum_wizardofoz = False #@param {type:\"boolean\"}\n",
        "fdouglass_narrativeofslave = False #@param {type:\"boolean\"}\n",
        "fscottfitzerald_greatgatsby = False #@param {type:\"boolean\"}\n",
        "geliot_middlemarch = False #@param {type:\"boolean\"}\n",
        "hjames_portraitofalady = False #@param {type:\"boolean\"}\n",
        "homerwilson_homer = False #@param {type:\"boolean\"}\n",
        "imcewan_machineslikeme = False #@param {type:\"boolean\"}\n",
        "jausten_prideandprejudice = False #@param {type:\"boolean\"}\n",
        "jconrad_heartofdarkness = False #@param {type:\"boolean\"}\n",
        "jjoyce_portraitoftheartist = False #@param {type:\"boolean\"}\n",
        "jkrowling_pottersorcerersstone = False #@param {type:\"boolean\"}\n",
        "mproust_searchoflosttime = False #@param {type:\"boolean\"}\n",
        "mshelly_frankenstein = False #@param {type:\"boolean\"}\n",
        "mtwain_huckleberryfinn = False #@param {type:\"boolean\"}\n",
        "staugustine_confessions = False #@param {type:\"boolean\"}\n",
        "tmorrison_beloved = False #@param {type:\"boolean\"}\n",
        "vnabokov_palefire = False #@param {type:\"boolean\"}\n",
        "vwoolf_mrsdalloway = True #@param {type:\"boolean\"}\n",
        "vwoolf_orlando = True #@param {type:\"boolean\"}\n",
        "vwoolf_thewaves = True #@param {type:\"boolean\"}\n",
        "vwoolf_tothelighthouse = True #@param {type:\"boolean\"}\n",
        "\n",
        "novels_subcorpus_ls = []\n",
        "\n",
        "if cdickens_christmascarol:\n",
        "  novels_subcorpus_ls.append('cdickens_christmascarol')\n",
        "if cdickens_greatexpectations: \n",
        "  novels_subcorpus_ls.append('cdickens_greatexpectations')\n",
        "if ddefoe_robinsoncrusoe:\n",
        "  novels_subcorpus_ls.append('ddefoe_robinsoncrusoe')\n",
        "if emforester_howardsend:\n",
        "  novels_subcorpus_ls.append('emforester_howardsend')\n",
        "if fbaum_wizardofoz:\n",
        "  novels_subcorpus_ls.append('fbaum_wizardofoz')\n",
        "if fdouglass_narrativeofslave:\n",
        "  novels_subcorpus_ls.append('fdouglass_narrativeofslave')\n",
        "if fscottfitzerald_greatgatsby:\n",
        "  novels_subcorpus_ls.append('fscottfitzerald_greatgatsby')\n",
        "if geliot_middlemarch:\n",
        "  novels_subcorpus_ls.append('geliot_middlemarch')\n",
        "if hjames_portraitofalady:\n",
        "  novels_subcorpus_ls.append('hjames_portraitofalady')\n",
        "if homerwilson_homer:\n",
        "  novels_subcorpus_ls.append('homerwilson_homer')\n",
        "if imcewan_machineslikeme:\n",
        "  novels_subcorpus_ls.append('imcewan_machineslikeme')\n",
        "if jausten_prideandprejudice:\n",
        "  novels_subcorpus_ls.append('jausten_prideandprejudice')\n",
        "if jconrad_heartofdarkness:\n",
        "  novels_subcorpus_ls.append('jconrad_heartofdarkness')\n",
        "if jjoyce_portraitoftheartist:\n",
        "  novels_subcorpus_ls.append('jjoyce_portraitoftheartist')\n",
        "if jkrowling_pottersorcerersstone:\n",
        "  novels_subcorpus_ls.append('jkrowling_pottersorcerersstone')\n",
        "if mproust_searchoflosttime:\n",
        "  novels_subcorpus_ls.append('mproust_searchoflosttime')\n",
        "if mshelly_frankenstein:\n",
        "  novels_subcorpus_ls.append('mshelly_frankenstein')\n",
        "if mtwain_huckleberryfinn:\n",
        "  novels_subcorpus_ls.append('mtwain_huckleberryfinn')\n",
        "if staugustine_confessions:\n",
        "  novels_subcorpus_ls.append('staugustine_confessions')\n",
        "if tmorrison_beloved:\n",
        "  novels_subcorpus_ls.append('tmorrison_beloved')\n",
        "if vnabokov_palefire:\n",
        "  novels_subcorpus_ls.append('vnabokov_palefire')\n",
        "if vwoolf_mrsdalloway:\n",
        "  novels_subcorpus_ls.append('vwoolf_mrsdalloway')\n",
        "if vwoolf_orlando:\n",
        "  novels_subcorpus_ls.append('vwoolf_orlando')\n",
        "if vwoolf_thewaves:\n",
        "  novels_subcorpus_ls.append('vwoolf_thewaves')\n",
        "if vwoolf_tothelighthouse:\n",
        "  novels_subcorpus_ls.append('vwoolf_tothelighthouse')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQAgxbCaVWTN"
      },
      "source": [
        "# Confirm Novel Selection(s)\n",
        "\n",
        "# novels_subcorpus_ls = []\n",
        "print(f'Confirm these are the Novels you want to analyze:\\n')\n",
        "\n",
        "for i, anovel in enumerate(novels_subcorpus_ls):\n",
        "  print(f'  Novel #{i}: {novels_dt[anovel][0]} ({novels_dt[anovel][2]})')\n",
        "  # novels_corpus_ls.append(anovel)\n",
        "\n",
        "print('\\nIf there is an error, go back and rerun the previous code cell.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSkKP0OfVWTN"
      },
      "source": [
        "#@title Enter ONE Novel to Focus On:\n",
        "\n",
        "Novel_Key = \"vwoolf_tothelighthouse\" #@param {type:\"string\"}\n",
        "\n",
        "# novels_ls = [Novel_Key]\n",
        "\n",
        "# Novel_Title = novels_dt[novels_ls[0]]\n",
        "# Novel_Title \n",
        "\n",
        "Novel_Title = novels_dt[Novel_Key][0]\n",
        "\n",
        "novel_focus_str = Novel_Key\n",
        "\n",
        "Novel_Title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9LPTX88eHeh"
      },
      "source": [
        "## Review Summary of Selected Novel(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDSDjfyqa19w"
      },
      "source": [
        "# Review Summary of Novel Selections (at Corpus, SubCorpus and Individual Levels):\n",
        "\n",
        "print(f'\\nCorpus: All {len(novels_corpus_ls)} Novels (novels_corpus_ls):')\n",
        "for i, anovel in enumerate(novels_corpus_ls):\n",
        "  print(f'  Novel #{i}: {anovel}')\n",
        "\n",
        "print(f'\\nSubCorpus: Only {len(novels_subcorpus_ls)} Selected Novels (novels_subcorpus_ls):')\n",
        "for i,anovel in enumerate(novels_subcorpus_ls):\n",
        "  print(f'  Novel #{i}: {anovel}')\n",
        "\n",
        "print(f'\\nFocus: One Novel to Focus On (novels_focus_str):')\n",
        "print(f'  {novel_focus_str}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMuusbmnVWTN"
      },
      "source": [
        "## Upload New Novel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdJsyP3jVWTO"
      },
      "source": [
        "# TODO: Coming feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzDqGBQzVWTO"
      },
      "source": [
        "# **Select Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFuR-RqXVWTP"
      },
      "source": [
        "# Standardize model/column names when reading from other sources\n",
        "\n",
        "cols_map_dt = {'syuzhet':'syuzhetr',\n",
        "               'huliu':'bing_sentimentr',\n",
        "               'sentiword':'sentiword_sentimentr',\n",
        "               'senticnet':'senticnet_sentimentr',\n",
        "               'lmcd':'lmcd_sentimentr',\n",
        "               'jockers':'jockers_sentimentr',\n",
        "               'jockers_rinker':'jockersrinker_sentimentr'\n",
        "               }\n",
        "\n",
        "cols_missing_ls = ['nrc_sentimentr']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzTWCZ7LVWTP"
      },
      "source": [
        "models_dt = {\n",
        "    # Lexicon\n",
        "    'SyuzhetR_AFINN':['afinn', 'lexicon', 2477],\n",
        "    'SyuzhetR_Bing':['bing', 'lexicon', 5469],\n",
        "    'SyuzhetR_NRC':['nrc', 'lexicon', 5468],\n",
        "    'SyuzhetR_SyuzhetR':['syuzhetr', 'lexicon', 10748],\n",
        "    'SentimentR_SentimentR':['sentimentr', 'lexicon', 11710],\n",
        "    'Pattern':['pattern', 'lexicon', 2918],\n",
        "\n",
        "    # Heuristic\n",
        "    'VADER':['vader', 'heuristic', 7520],\n",
        "    'SentimentR_Bing':['bing_sentimentr', 'heuristic', 5469],\n",
        "    # 'SentimentR_NRC':['nrc_sentimentr', 'heuristic', 5469],\n",
        "    'SentimentR_SentiWord':['sentiword_sentimentr', 'heuristic', 20093],\n",
        "    'SentimentR_SenticNet':['senticnet_sentimentr', 'heuristic', 23626],\n",
        "    'SentimentR_LMcD':['lmcd_sentimentr', 'heuristic', 4150],\n",
        "    'SentimentR_Jockers':['jockers_sentimentr', 'heuristic', 10748],\n",
        "    'SentimentR_JockersRinker':['jockersrinker_sentimentr', 'heuristic', 11710],\n",
        "\n",
        "    # Traditional ML \n",
        "    'Logistic_Regression':['logreg', 'tradml', 'scikit'],\n",
        "    'Logistic_Regression_CV':['logreg_cv', 'tradml', 'scikit'],\n",
        "    'Multinomial_Naive_Bayes':['multinb', 'tradml', 'scikit'],\n",
        "    'TextBlob':['textblob', 'tradml', 'textblob'],\n",
        "    'Random_Forest':['rf', 'tradml', 'scikit'],\n",
        "    'XGBoost':['xgb', 'tradml', 'xgboost'],\n",
        "    'FLAML_AutoML':['flaml', 'tradml', 'flaml'],\n",
        "    'AutoGluon_Text':['autogluon', 'tradml', 'autogluon_text'],\n",
        "\n",
        "    # DNN\n",
        "    'Fully_Connected_Network':['fcn', 'dnn', 6287671],\n",
        "    'LSTM_DNN':['lstm', 'dnn', 7109089],\n",
        "    'CNN_DNN':['cnn', 'dnn', 1315937],\n",
        "    'Multilingual_CNN_Stanza_AutoML':['stanza', 'dnn', 0],\n",
        "    'HyperOpt_CNN_Flair_AutoML':['flair', 'dnn', 0],\n",
        "\n",
        "    # Transformer\n",
        "    'Distilled_BERT':['huggingface', 'transformer', 'bert'],\n",
        "    'T5_IMDB':['t5imdb50k', 'transformer', 't5'],\n",
        "    'BERT_Dual_Coding':['hinglish', 'transformer', 'bert'],\n",
        "    'BERT_Yelp':['yelp', 'transformer', 'bert'],\n",
        "    'BERT_2IMDB':['imdb2way', 'transformer', 'bert'],\n",
        "    'BERT_Multilingual':['nlptown', 'transformer', 'bert'],\n",
        "    'RoBERTa_XML_8Language':['robertaxml8lang', 'transformer', 'roberta'],\n",
        "    'RoBERTa_Large_15DB':['roberta15lg', 'transformer', 'roberta'],\n",
        "\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bykD77UqVWTQ"
      },
      "source": [
        "# Convenience lists for each type of model\n",
        "\n",
        "# Lexicon Models\n",
        "models_lexicon_ls = [x[0] for x in models_dt.values() if x[1] == 'lexicon']\n",
        "print(f'\\nThere are {len(models_lexicon_ls)} Lexicon Models')\n",
        "for i,amodel in enumerate(models_lexicon_ls):\n",
        "  print(f'  Lexicon Model #{i}: {amodel}')\n",
        "\n",
        "# Heuristic Models\n",
        "models_heuristic_ls = [x[0] for x in models_dt.values() if x[1] == 'heuristic']\n",
        "print(f'\\nThere are {len(models_heuristic_ls)} Heuristic Models')\n",
        "for i,amodel in enumerate(models_heuristic_ls):\n",
        "  print(f'  Heuristic Model #{i}: {amodel}')\n",
        "\n",
        "# Traditional ML Models\n",
        "models_tradml_ls = [x[0] for x in models_dt.values() if x[1] == 'tradml']\n",
        "print(f'\\nThere are {len(models_tradml_ls)} Traditional ML Models')\n",
        "for i,amodel in enumerate(models_tradml_ls):\n",
        "  print(f'  Traditional ML Model #{i}: {amodel}')\n",
        "\n",
        "# DNN Models\n",
        "models_dnn_ls = [x[0] for x in models_dt.values() if x[1] == 'dnn']\n",
        "print(f'\\nThere are {len(models_dnn_ls)} DNN Models')\n",
        "for i,amodel in enumerate(models_dnn_ls):\n",
        "  print(f'  DNN Model #{i}: {amodel}')\n",
        "\n",
        "# Transformer Models\n",
        "models_transformer_ls = [x[0] for x in models_dt.values() if x[1] == 'transformer']\n",
        "print(f'\\nThere are {len(models_transformer_ls)} Transformer Models')\n",
        "for i,amodel in enumerate(models_transformer_ls):\n",
        "  print(f'  Transformer Model #{i}: {amodel}')\n",
        "\n",
        "# All Models\n",
        "\n",
        "\"\"\"\n",
        "models_all_ls = models_lexicon_ls + models_heuristic_ls + models_tradml_ls + models_dnn_ls + models_transformer_ls\n",
        "\n",
        "print(f'\\nThere are {len(cols_models_ls)} Total Models:')\n",
        "for i,amodel in enumerate(cols_models_ls):\n",
        "  print(f'  Model #{i:>2}: {amodel}')\n",
        "\"\"\";\n",
        "\n",
        "models_ensemble_ls = models_lexicon_ls + models_heuristic_ls + models_tradml_ls + models_dnn_ls + models_transformer_ls\n",
        "\n",
        "print(f'\\nThere are {len(models_ensemble_ls)} Total Models:')\n",
        "for i,amodel in enumerate(models_ensemble_ls):\n",
        "  print(f'  Model #{i:>2}: {amodel}')\n",
        "\n",
        "print(f'\\nThere are {len(models_ensemble_ls)} Total Models (+1 for Ensemble Median)')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3IHb2J5VWTQ"
      },
      "source": [
        "## Lexical Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq4B3RMgVWTR"
      },
      "source": [
        "#@title Select Lexical Model(s) to Include in Ensemble:\n",
        "\n",
        "SyzuhetR_AFINN = True #@param {type:\"boolean\"}\n",
        "SyuzhetR_Bing = False #@param {type:\"boolean\"}\n",
        "SyuzhetR_NRC = True #@param {type:\"boolean\"}\n",
        "SyuzhetR_SyuzhetR = True #@param {type:\"boolean\"}\n",
        "SentimentR_SentimentR = False #@param {type:\"boolean\"}\n",
        "Pattern = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "ensemble_lexicon_ls = []\n",
        "\n",
        "if SyzuhetR_AFINN:\n",
        "  ensemble_lexicon_ls.append('SyuzhetR_AFINN')\n",
        "if SyuzhetR_Bing:\n",
        "  ensemble_lexicon_ls.append('SyuzhetR_Bing')\n",
        "if SyuzhetR_NRC:\n",
        "  ensemble_lexicon_ls.append('SyuzhetR_NRC')\n",
        "if SyuzhetR_SyuzhetR:\n",
        "  ensemble_lexicon_ls.append('SyuzhetR_SyuzhetR')\n",
        "if SentimentR_SentimentR:\n",
        "  ensemble_lexicon_ls.append('SentimentR_SentimentR')\n",
        "if Pattern:\n",
        "  ensemble_lexicon_ls.append('Pattern')\n",
        "\n",
        "\n",
        "ensemble_lexicon_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WFEG5JzVWTR"
      },
      "source": [
        "# Confirm Lexicon Model Selection(s)\n",
        "\n",
        "print(f'Confirm these are the Lexicon Model(s) to include in the Ensemble:\\n')\n",
        "\n",
        "for i, amodel in enumerate(ensemble_lexicon_ls):\n",
        "  print(f'  Model #{i}: {models_dt[amodel][0]}')\n",
        "\n",
        "print('\\nIf there is an error, go back and rerun the previous code cell.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l38k3N9xVWTS"
      },
      "source": [
        "## Heuristic Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTrtDyBAVWTS"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in [x for x in models_dt.keys() if models_dt[x][1] == 'heuristic']:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAZWMrjTVWTS"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in models_heuristic_ls:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zv0vN3pVWTT"
      },
      "source": [
        "#@title Select Heuristic Model(s) to Include in Ensemble:\n",
        "\n",
        "VADER = True #@param {type:\"boolean\"}\n",
        "SentimentR_Bing = False #@param {type:\"boolean\"}\n",
        "# SentimentR_NRC = True #@param {type:\"boolean\"}\n",
        "SentimentR_SentiWord = False #@param {type:\"boolean\"}\n",
        "SentimentR_SenticNet = False #@param {type:\"boolean\"}\n",
        "SentimentR_LMcD = False #@param {type:\"boolean\"}\n",
        "SentimentR_Jockers = False #@param {type:\"boolean\"}\n",
        "SentimentR_JockersRinker = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "ensemble_heuristic_ls = []\n",
        "\n",
        "if VADER:\n",
        "  ensemble_heuristic_ls.append('VADER')\n",
        "if SentimentR_Bing:\n",
        "  ensemble_heuristic_ls.append('SentimentR_Bing')\n",
        "\n",
        "# if SentimentR_NRC:\n",
        "#   ensemble_heuristic_ls.append('SentimentR_NRC')\n",
        "\n",
        "if SentimentR_SentiWord:\n",
        "  ensemble_heuristic_ls.append('SentimentR_SentiWord')\n",
        "if SentimentR_SenticNet:\n",
        "  ensemble_heuristic_ls.append('SentimentR_SenticNet')\n",
        "if SentimentR_LMcD:\n",
        "  ensemble_heuristic_ls.append('SentimentR_LMcD')\n",
        "if SentimentR_Jockers:\n",
        "  ensemble_heuristic_ls.append('SentimentR_Jockers')\n",
        "if SentimentR_JockersRinker:\n",
        "  ensemble_heuristic_ls.append('SentimentR_JockersRinker')\n",
        "\n",
        "ensemble_heuristic_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqFGceMHVWTT"
      },
      "source": [
        "# Confirm Heuristic Model Selection(s)\n",
        "\n",
        "print(f'Confirm these are the Heuristic Model(s) to include in the Ensemble:\\n')\n",
        "\n",
        "for i, amodel in enumerate(ensemble_heuristic_ls):\n",
        "  print(f'  Model #{i}: {models_dt[amodel][0]}')\n",
        "\n",
        "print('\\nIf there is an error, go back and rerun the previous code cell.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLmuNCDHVWTT"
      },
      "source": [
        "## Traditional ML Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylujDC1VVWTT"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in [x for x in models_dt.keys() if models_dt[x][1] == 'tradml']:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg6YqGhYVWTT"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in models_tradml_ls:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoU-TesjVWTU"
      },
      "source": [
        "#@title Select Heuristic Model(s) to Include in Ensemble:\n",
        "\n",
        "Logistic_Regression = True #@param {type:\"boolean\"}\n",
        "Logistic_Regression_CV6 = False #@param {type:\"boolean\"}\n",
        "Multinomial_Naive_Bayes = False #@param {type:\"boolean\"}\n",
        "TextBlob = True #@param {type:\"boolean\"}\n",
        "Random_Forest = False #@param {type:\"boolean\"}\n",
        "XGBoost = True #@param {type:\"boolean\"}\n",
        "FLAML_AutoML = True #@param {type:\"boolean\"}\n",
        "AutoGluon_Text = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "ensemble_tradml_ls = []\n",
        "\n",
        "if Logistic_Regression:\n",
        "  ensemble_tradml_ls.append('Logistic_Regression')\n",
        "if Logistic_Regression_CV6:\n",
        "  ensemble_tradml_ls.append('Logistic_Regression_CV')\n",
        "if Multinomial_Naive_Bayes:\n",
        "  ensemble_tradml_ls.append('Multinomial_Naive_Bayes')\n",
        "if TextBlob:\n",
        "  ensemble_tradml_ls.append('TextBlob')\n",
        "if Random_Forest:\n",
        "  ensemble_tradml_ls.append('Random_Forest')\n",
        "if XGBoost:\n",
        "  ensemble_tradml_ls.append('XGBoost')\n",
        "if FLAML_AutoML:\n",
        "  ensemble_tradml_ls.append('FLAML_AutoML')\n",
        "if AutoGluon_Text:\n",
        "  ensemble_tradml_ls.append('AutoGluon_Text')\n",
        "\n",
        "ensemble_tradml_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYssOOXtVWTU"
      },
      "source": [
        "# Confirm Traditional ML Model Selection(s)\n",
        "\n",
        "print(f'Confirm these are the Traditional ML Model(s) to include in the Ensemble:\\n')\n",
        "\n",
        "for i, amodel in enumerate(ensemble_tradml_ls):\n",
        "  print(f'  Model #{i}: {models_dt[amodel][0]}')\n",
        "\n",
        "print('\\nIf there is an error, go back and rerun the previous code cell.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQPorXb2VWTU"
      },
      "source": [
        "## DNN Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd7CSYdiVWTU"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in [x for x in models_dt.keys() if models_dt[x][1] == 'dnn']:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EukqTCzKVWTV"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in models_dnn_ls:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApnZ0d7hVWTW"
      },
      "source": [
        "#@title Select Heuristic Model(s) to Include in Ensemble:\n",
        "\n",
        "Fully_Connected_Network = True #@param {type:\"boolean\"}\n",
        "LSTM_DNN = False #@param {type:\"boolean\"}\n",
        "CNN_DNN = False #@param {type:\"boolean\"}\n",
        "Multilingual_CNN_Stanza_AutoML = True #@param {type:\"boolean\"}\n",
        "HyperOpt_CNN_Flair_AutoML = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "ensemble_dnn_ls = []\n",
        "\n",
        "if Fully_Connected_Network:\n",
        "  ensemble_dnn_ls.append('Fully_Connected_Network')\n",
        "if LSTM_DNN:\n",
        "  ensemble_dnn_ls.append('LSTM_DNN')\n",
        "if CNN_DNN:\n",
        "  ensemble_dnn_ls.append('CNN_DNN')\n",
        "if Multilingual_CNN_Stanza_AutoML:\n",
        "  ensemble_dnn_ls.append('Multilingual_CNN_Stanza_AutoML')\n",
        "if HyperOpt_CNN_Flair_AutoML:\n",
        "  ensemble_dnn_ls.append('HyperOpt_CNN_Flair_AutoML')\n",
        "\n",
        "ensemble_dnn_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-Y2IXbHVWTW"
      },
      "source": [
        "# Confirm DNN Model Selection(s)\n",
        "\n",
        "print(f'Confirm these are the DNN Model(s) to include in the Ensemble:\\n')\n",
        "\n",
        "for i, amodel in enumerate(ensemble_dnn_ls):\n",
        "  print(f'  Model #{i}: {models_dt[amodel][0]}')\n",
        "\n",
        "print('\\nIf there is an error, go back and rerun the previous code cell.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CctsZwSVWTW"
      },
      "source": [
        "## Transformer Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyxfBEaqVWTW"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in [x for x in models_dt.keys() if models_dt[x][1] == 'transformer']:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC8r56MhVWTY"
      },
      "source": [
        "\"\"\"\n",
        "for amodel in models_transformer_ls:\n",
        "  print(amodel)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeCAn-c0VWTY"
      },
      "source": [
        "#@title Select Heuristic Model(s) to Include in Ensemble:\n",
        "\n",
        "Distilled_BERT = True #@param {type:\"boolean\"}\n",
        "T5_IMDB = True #@param {type:\"boolean\"}\n",
        "BERT_Dual_Coding = False #@param {type:\"boolean\"}\n",
        "BERT_Yelp = False #@param {type:\"boolean\"}\n",
        "BERT_2IMDB = False #@param {type:\"boolean\"}\n",
        "BERT_Multilingual = False #@param {type:\"boolean\"}\n",
        "RoBERTa_XML_8Language = True #@param {type:\"boolean\"}\n",
        "RoBERTa_Large_15DB = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "ensemble_transformer_ls = []\n",
        "\n",
        "if Distilled_BERT:\n",
        "  ensemble_transformer_ls.append('Distilled_BERT')\n",
        "if T5_IMDB:\n",
        "  ensemble_transformer_ls.append('T5_IMDB')\n",
        "if BERT_Dual_Coding:\n",
        "  ensemble_transformer_ls.append('BERT_Dual_Coding')\n",
        "if BERT_Yelp:\n",
        "  ensemble_transformer_ls.append('BERT_Yelp')\n",
        "if BERT_2IMDB:\n",
        "  ensemble_transformer_ls.append('BERT_2IMDB')\n",
        "if BERT_Multilingual:\n",
        "  ensemble_transformer_ls.append('BERT_Multilingual')\n",
        "if RoBERTa_XML_8Language:\n",
        "  ensemble_transformer_ls.append('RoBERTa_XML_8Language')\n",
        "if RoBERTa_Large_15DB:\n",
        "  ensemble_transformer_ls.append('RoBERTa_Large_15DB')\n",
        "\n",
        "ensemble_transformer_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnpDNvuQVWTY"
      },
      "source": [
        "# Confirm Transformer Model Selection(s)\n",
        "\n",
        "print(f'Confirm these are the Transformer Model(s) to include in the Ensemble:\\n')\n",
        "\n",
        "for i, amodel in enumerate(ensemble_transformer_ls):\n",
        "  print(f'  Model #{i}: {models_dt[amodel][0]}')\n",
        "\n",
        "print('\\nIf there is an error, go back and rerun the previous code cell.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAXJS-fiVWTY"
      },
      "source": [
        "# Get list of subensemble models\n",
        "\n",
        "# List of model names (key for indexing into models_dt and col names of ensemble_df)\n",
        "models_subensemble_ls = []\n",
        "\n",
        "# List of model titles (for print)\n",
        "models_ensemble_title_ls = ensemble_lexicon_ls + ensemble_heuristic_ls + ensemble_tradml_ls + \\\n",
        "              ensemble_dnn_ls + ensemble_transformer_ls\n",
        "\n",
        "print(f'Confirm these are the all the Model(s) to include in the SubEnsemble:\\n')\n",
        "\n",
        "for i, amodel in enumerate(models_ensemble_title_ls):\n",
        "  print(f'  Model #{i}: {amodel} ({models_dt[amodel][1]}: {models_dt[amodel][0]})')\n",
        "  models_subensemble_ls.append(models_dt[amodel][0])\n",
        "\n",
        "print(f'\\nIf there is an error in these {len(models_subensemble_ls)} models (+1 for SubEnsemble Median), \\n go back and rerun the previous code cells.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P0a7epkfcQx"
      },
      "source": [
        "## Enter ONE Model to Focus On"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzSglbhffynC"
      },
      "source": [
        "# Generate a string of all Model Titles/keys to cut/paste into dropdown in next code cell\n",
        "\n",
        "models_key_str = ','.join(f\"'{x}'\" for x in models_dt.keys())\n",
        "models_key_str\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x9stHeYfbqB"
      },
      "source": [
        "#@title Select ONE Model to Focus On:\n",
        "\n",
        "Model_Focus = \"VADER\" #@param ['SyuzhetR_AFINN','SyuzhetR_Bing','SyuzhetR_NRC','SyuzhetR_SyuzhetR','SentimentR_SentimentR','Pattern','VADER','SentimentR_Bing','SentimentR_SentiWord','SentimentR_SenticNet','SentimentR_LMcD','SentimentR_Jockers','SentimentR_JockersRinker','Logistic_Regression','Logistic_Regression_CV','Multinomial_Naive_Bayes','TextBlob','Random_Forest','XGBoost','FLAML_AutoML','AutoGluon_Text','Fully_Connected_Network','LSTM_DNN','CNN_DNN','Multilingual_CNN_Stanza_AutoML','HyperOpt_CNN_Flair_AutoML','Distilled_BERT','T5_IMDB','BERT_Dual_Coding','BERT_Yelp','BERT_2IMDB','BERT_Multilingual','RoBERTa_XML_8Language','RoBERTa_Large_15DB']\n",
        "\n",
        "# novels_ls = [Novel_Key]\n",
        "\n",
        "# Novel_Title = novels_dt[novels_ls[0]]\n",
        "# Novel_Title \n",
        "\n",
        "Model_Title = models_dt[Model_Focus]\n",
        "\n",
        "model_focus_str = Model_Focus.lower()\n",
        "print(f'You Selected Model: {model_focus_str}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AreXkzcyVWTY"
      },
      "source": [
        "## Review Summary of Selected Model(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKHwuJNYeZfx"
      },
      "source": [
        "# Review Summary of Model Selections (at Ensemble, SubEnsemble and Individual Levels):\n",
        "\n",
        "print(f'\\nEnsemble: All {len(models_ensemble_ls)} (+1 for Median) Models (models_ensemble_ls):')\n",
        "for i, amodel in enumerate(models_ensemble_ls):\n",
        "  print(f'  Novel #{i}: {amodel}')\n",
        "\n",
        "print(f'\\nSubEnsemble: Only {len(models_subensemble_ls)} (+1 for Median) Selected Models (models_subensemble_ls):')\n",
        "for i,amodel in enumerate(models_subensemble_ls):\n",
        "  print(f'  Model #{i}: {amodel}')\n",
        "\n",
        "print(f'\\nFocus: One Model to Focus On (model_focus_str):')\n",
        "print(f'  Model: {model_focus_str}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_VEF5MIVWTY"
      },
      "source": [
        "# **Get Sentiment Values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp9koah0VWTY"
      },
      "source": [
        "## Option (a): Read ALL Clean Values from ONE File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcH1ndJVLvxX"
      },
      "source": [
        "print('Current Working Directory')\n",
        "!pwd\n",
        "\n",
        "print(f'\\nSubdir containing Clean datafiles: \\n  {data_clean_subdir}')\n",
        "\n",
        "print(f'\\nSubdir [{data_clean_subdir}*.csv] file count:')\n",
        "!ls $data_clean_subdir/*.csv | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO4O1ZqNMsIZ"
      },
      "source": [
        "!ls -altr ./data/sentiments_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSO6NP7OMFiU"
      },
      "source": [
        "# Get Filenames of Combined data for ALL Sentiment Models for EVERY Novel\n",
        "#   'models_allz_*.csv' datafiles created by sentiment_arcs_clean.ipynb \n",
        "#     has one file for each Novel in the Corpus, each will all Model Sentiment Time Series\n",
        "\n",
        "saclean_filenames_ls = []\n",
        "saclean_filenames_quoted_ls = []\n",
        "\n",
        "# novels_saclean_ls = glob.glob('./data/sentiments_clean/sa_clean_*.csv')\n",
        "data_clean_path = f'{data_clean_subdir}sa_clean_*.csv'\n",
        "novels_saclean_ls = glob.glob(data_clean_path)\n",
        "for i, anovel in enumerate(novels_saclean_ls):\n",
        "  saclean_filename = anovel.split('/')[-1]\n",
        "  # print(f'Novel #{i}: {saclean_filename}')\n",
        "  saclean_filename_quoted = f\"'{saclean_filename}'\"\n",
        "  saclean_filenames_ls.append(saclean_filename)\n",
        "  saclean_filenames_ls.sort()\n",
        "\n",
        "saclean_filenames_quoted_ls = [f\"'{x}'\" for x in saclean_filenames_ls]\n",
        "\n",
        "saclean_filenames_str = ','.join(saclean_filenames_quoted_ls)\n",
        "\n",
        "print(f'All {len(saclean_filenames_ls)} Novels in default SentimentArcs Corpus:')\n",
        "for i, afile in enumerate(saclean_filenames_ls):\n",
        "  print(f'  Novel #{i}: {afile}')\n",
        "\n",
        "print('\\nPaste the following string to define the Combined_Sentiment_Datafile dropdown values in the next code cell if necessary:')\n",
        "print(f'\\n  {saclean_filenames_str}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3EddknhVWTb"
      },
      "source": [
        "#@title Select a Datafile with ALL Model Sentiment Values:\n",
        "\n",
        "# #@markdown Cut/Paste filename from directory listing above:\n",
        "# Combined_Sentiment_Datafile = \"models_allz_vwoolf_tothelighthouse.csv\" #@param ['models_allz_cdickens_achristmascarol.csv','models_allz_cdickens_greatexpectations.csv','models_allz_ddefoe_robinsoncrusoe.csv','models_allz_emforster_howardsend.csv','models_allz_fbaum_thewonderfulwizardofoz.csv','models_allz_fdouglass_narrativelifeofaslave.csv','models_allz_fscottfitzgerald_thegreatgatsby.csv','models_allz_geliot_middlemarch.csv','models_allz_hjames_portraitofalady.csv','models_allz_homer-ewilson_odyssey.csv','models_allz_imcewan_machineslikeme.csv','models_allz_jausten_prideandprejudice.csv','models_allz_jconrad_heartofdarkness.csv','models_allz_jjoyce_portraitoftheartist.csv','models_allz_jkrowling_1sorcerersstone.csv','models_allz_mproust-mtreharne_3guermantesway.csv','models_allz_mshelley_frankenstein.csv','models_allz_mtwain_huckleberryfinn.csv','models_allz_staugustine_confessions9end.csv','models_allz_tmorrison_beloved.csv','models_allz_vnabokov_palefire.csv','models_allz_vwoolf_mrsdalloway.csv','models_allz_vwoolf_orlando.csv','models_allz_vwoolf_thewaves.csv','models_allz_vwoolf_tothelighthouse.csv']\n",
        "# All_Sentiments_Datafile = \"models_allz_vwoolf_tothelighthouse.csv\" #@param {type:\"string\"}\n",
        "Select_Sentiment_Datafile = \"models_allz_vwoolf_tothelighthouse.csv\" #@param ['models_allz_cdickens_achristmascarol.csv','models_allz_cdickens_greatexpectations.csv','models_allz_ddefoe_robinsoncrusoe.csv','models_allz_emforster_howardsend.csv','models_allz_fbaum_thewonderfulwizardofoz.csv','models_allz_fdouglass_narrativelifeofaslave.csv','models_allz_fscottfitzgerald_thegreatgatsby.csv','models_allz_geliot_middlemarch.csv','models_allz_hjames_portraitofalady.csv','models_allz_homer-ewilson_odyssey.csv','models_allz_imcewan_machineslikeme.csv','models_allz_jausten_prideandprejudice.csv','models_allz_jconrad_heartofdarkness.csv','models_allz_jjoyce_portraitoftheartist.csv','models_allz_jkrowling_1sorcerersstone.csv','models_allz_mproust-mtreharne_3guermantesway.csv','models_allz_mshelley_frankenstein.csv','models_allz_mtwain_huckleberryfinn.csv','models_allz_staugustine_confessions9end.csv','models_allz_tmorrison_beloved.csv','models_allz_vnabokov_palefire.csv','models_allz_vwoolf_mrsdalloway.csv','models_allz_vwoolf_orlando.csv','models_allz_vwoolf_thewaves.csv','models_allz_vwoolf_tothelighthouse.csv']\n",
        "\n",
        "\n",
        "\n",
        "subensemble_df = pd.read_csv(f'{data_clean_subdir}{Select_Sentiment_Datafile}', index_col=[0])\n",
        "subensemble_df.head()\n",
        "print('\\n')\n",
        "subensemble_df.info(verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gLO0nnVVWTb"
      },
      "source": [
        "# Rename columns to be more consistent using Dictionary cols_map_dt\n",
        "#   defined in Configuration and Setup Section at top of this notebook\n",
        "\n",
        "subensemble_df.rename(columns=cols_map_dt, inplace=True)\n",
        "\n",
        "ensemble_df.rename(columns=cols_map_dt, inplace=True)\n",
        "ensemble_df.info(verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC--nYdzVWTb"
      },
      "source": [
        "# First, Remove columns derived from base Models\n",
        "# NOTE: Don't trust any Model Sentiment values except for raw values\n",
        "#       drop and re-derive any other Models (e.g. vader_stdsma, vader_stdlowess)\n",
        "\n",
        "cols_model_deriv_drop_ls = []\n",
        "\n",
        "cols_suffix_ls = ['_z', '_sma', '_stdsma', '_lowess', '_stdlowess']\n",
        "for asuffix in cols_suffix_ls:\n",
        "  for acol in subensemble_df.columns: # cols_model_ls:\n",
        "    if acol.endswith(asuffix):\n",
        "      cols_model_deriv_drop_ls.append(acol)\n",
        "\n",
        "subensemble_df.drop(columns=cols_model_deriv_drop_ls, axis=1, inplace=True)\n",
        "subensemble_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHFyQK9gVWTc"
      },
      "source": [
        "subensemble_df.info(verbose=True)\n",
        "print('\\n')\n",
        "models_subensemble_ls\n",
        "print(len(models_subensemble_ls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVP84JwqO6bM"
      },
      "source": [
        "# cols_ensemble_ls\n",
        "# cols_models_ls\n",
        "models_subensemble_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "659CvKm3eiNt"
      },
      "source": [
        "cols_drop_set = list(set(models_ensemble_ls) - set(models_subensemble_ls))\n",
        "# cols_drop_set = [x for x in models_ensemble_ls if not(x in cols_subensemble_ls)]\n",
        "cols_drop_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUh_xQTAO9Xu"
      },
      "source": [
        "models_subensemble_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbChtogBVWTd"
      },
      "source": [
        "# Second, Remove unselected Models from SubEnsemble\n",
        "\n",
        "# UPDATE: Not Necessary (subensemble_ls models already defined above)\n",
        "\n",
        "print(f'SubEnsemble has {len(models_subensemble_ls)} Models:')\n",
        "for i, amodel in enumerate(models_subensemble_ls):\n",
        "  print(f'  Model #{i}: {amodel}')\n",
        "\n",
        "print(f'\\nA Total of {len(models_subensemble_ls)} (+1 for Median) Raw Models/Columns')\n",
        "\n",
        "\"\"\"\n",
        "# Drop Model columns that are not selected for the Ensemble\n",
        "# cols_model_drop_ls = list(set(cols_models_ls) - set(cols_ensemble_ls))\n",
        "cols_model_drop_ls = [x for x in models_ensemble_ls if not(x in models_subensemble_ls)]\n",
        "subensemble_df.drop(columns=cols_model_drop_ls, axis=1, inplace=True)\n",
        "\n",
        "# Skip columns that are not Models\n",
        "cols_notmodel_ls = ['sent_no', 'parag_no', 'sect_no', 'sent_raw', 'sent_clean', 'median']\n",
        "cols_ensemble_ls = list(set(subensemble_df.columns) - set(cols_notmodel_ls))\n",
        "print(f'Ensemble has {len(cols_ensemble_ls)} Models:')\n",
        "for i, amodel in enumerate(cols_ensemble_ls):\n",
        "  print(f'  Model #{i}: {amodel}')\n",
        "\n",
        "print(f'\\nA Total of {len(cols_ensemble_ls)} Raw Models/Columns')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waFnpS4cVWTd"
      },
      "source": [
        "# Third, Create one additional Model column that is the Median of all Model Sentiment Values\n",
        "\n",
        "# Only add median to SubEnsemble if it doesn't already exist\n",
        "if ('median' in subensemble_df.columns):\n",
        "  print(f'Model [median] already in SubEnsemble')\n",
        "else:\n",
        "  subensemble_df['median'] = subensemble_df[models_subensemble_ls].median(axis=1)\n",
        "  models_subensemble_ls.append('median')\n",
        "\n",
        "# Only add median to Ensemble if it doesn't already exist\n",
        "if ('median' in ensemble_df.columns):\n",
        "  print(f'Model [median] already in Ensemble')\n",
        "else:\n",
        "  ensemble_df['median'] = ensemble_df[models_ensemble_ls].median(axis=1)\n",
        "  models_ensemble_ls.append('median')\n",
        "\n",
        "print(f'\\nSubEnsemble Models Selected:')\n",
        "for i, amodel in enumerate(models_subensemble_ls):\n",
        "  print(f'  Model #{i}: {amodel}')\n",
        "\n",
        "print(f'\\nAdded Median Model/Column for ')\n",
        "print(f'  A Total of {len(models_subensemble_ls)} SubEnsemble Raw Models/Columns')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMmgpImWVWTd"
      },
      "source": [
        "# Show just the SubEnsemble Cols/Models\n",
        "\n",
        "subensemble_df[models_subensemble_ls].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJB8wF8yjj5j"
      },
      "source": [
        "## Option (b): Read Each Model Raw Values from MANY Files\n",
        "\n",
        "* TODO: May drop feature as too confusing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoZdNC8TGo5P"
      },
      "source": [
        "### Read Lexical Model(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXKryjygkfaO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whv1ZRvAGkvH"
      },
      "source": [
        "### Read Heuristic Model(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFYL1LhABwfg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt-dH7QlfZMM"
      },
      "source": [
        "### Read Traditional ML Model(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uVSz34ufYjX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HkdejuUfcEK"
      },
      "source": [
        "### Read DNN Model(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve-aFJzIfYf6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-woCoeFfd9c"
      },
      "source": [
        "### Read Transformer Model(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XN0hLzafYdM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzLrILtLkJUU"
      },
      "source": [
        "### Join All Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qvO9-R_kkuv"
      },
      "source": [
        "# ensemble_df = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFT6QFgHz2x4"
      },
      "source": [
        "# SMA Smooth Raw Sentiment Values for All Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecUMs0u_z2Be"
      },
      "source": [
        "#@title Enter SMA window size as a Percent of total Corpus Length\n",
        "\n",
        "Window_Percent = 10 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "\n",
        "win_size = int(Window_Percent/100 * ensemble_df.shape[0])\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "# If does not exist, calculate smaing mean (SMA)\n",
        "if ~subensemble_df.columns.str.contains('_sma').any():\n",
        "  for acol in models_subensemble_ls:\n",
        "    acol_sma = f'{acol}_sma'\n",
        "    subensemble_df[acol_sma] = subensemble_df[acol].rolling(win_size, center=True, min_periods=0).mean()\n",
        "\n",
        "# Plot the SMA (*_sma columns) SentimentArcs for Ensemble Models\n",
        "for acol in models_subensemble_ls:\n",
        "  if acol in models_lexicon_ls:\n",
        "    alinestyle = 'solid'\n",
        "  elif acol in models_heuristic_ls:\n",
        "    alinestyle = 'solid'\n",
        "  elif acol in models_tradml_ls:\n",
        "    alinestyle = 'dashdot'\n",
        "  elif acol in models_dnn_ls:\n",
        "    alinestyle = 'dotted'\n",
        "  elif acol in models_transformer_ls:\n",
        "    alinestyle='dashed'\n",
        "\n",
        "  acol_sma = f'{acol}_sma'\n",
        "  subensemble_df[acol_sma].plot(alpha=0.5, label=acol, linewidth=2, linestyle=alinestyle)\n",
        "\n",
        "plt.grid(True)\n",
        "plt.title(f'{Novel_Title[0]} \\n SentimentArcs for all {len(models_subensemble_ls)} Ensemble Models \\n SMA {Window_Percent}% Smoothing but no Standardization', fontsize=20)\n",
        "plt.xlabel('Sentence Number', fontsize=14)\n",
        "plt.ylabel('Sentiment', fontsize=14)\n",
        "# plt.legend(title='Model Name', loc='best')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', title='Model Name', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RV-1W4gGwfQ"
      },
      "source": [
        "# Standardize the SMA Smoothed SentimentArcs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbhHj_YHZ3zp"
      },
      "source": [
        "# Select Standardized Method and Standardize all Model Values\n",
        "\n",
        "#@title Select a Standardization Method to Use:\n",
        "\n",
        "Model_Standardization_Method = \"zScore\" #@param [\"zScore\", \"MinMax\", \"Robust\"]\n",
        "\n",
        "# Create Standardized versions of each Model/Column values\n",
        "# CAUTION: Only run once\n",
        "\n",
        "# Select a Standardization method = ['zscore','minmax','robust']\n",
        "# Model_Standardization_Method = 'robust'\n",
        "\n",
        "if Model_Standardization_Method == 'zScore':\n",
        "  scaler = StandardScaler()\n",
        "elif Model_Standardization_Method == 'MinMax':\n",
        "  scaler = MinMaxScaler()\n",
        "elif Model_Standardization_Method == 'Robust':\n",
        "  scaler = RobustScaler()\n",
        "\n",
        "# If does not exist, standardize the smaing mean (SMA)\n",
        "if ~subensemble_df.columns.str.contains('_stdsma').any():\n",
        "  for acol in models_subensemble_ls:\n",
        "    acol_sma = f'{acol}_sma'\n",
        "    acol_stdsma = f'{acol}_stdsma'\n",
        "    subensemble_df[acol_stdsma] = scaler.fit_transform(subensemble_df[acol_sma].values.reshape(-1,1))\n",
        "\n",
        "subensemble_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxGwnUDXh-K8"
      },
      "source": [
        "# Save Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dPOhYc4uSBF"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeBn0lwRuaCW"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyYp1mF8iNi5"
      },
      "source": [
        "# Save newly cleaned (Smoothed then Standardized) Ensemble Model data\n",
        "\n",
        "file_fullpath = get_fullpath(ftype='data_clean')\n",
        "ensemble_df.to_csv(file_fullpath)\n",
        "\n",
        "print(f'Saved to file: {file_fullpath}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg5VrD9Smhm6"
      },
      "source": [
        "# Verify saved file contents\n",
        "\n",
        "!head -n 10 $file_fullpath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxaf0vtbG1ff"
      },
      "source": [
        "# **Plot Sentiment Arcs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbkwemQF3SB5"
      },
      "source": [
        "# Confirm Ensemble Models and Optionally Save Plot to File\n",
        "\n",
        "#@title Confirm Ensemble SentimentArcs and Option to Saved:\n",
        "Save_Plot = True #@param {type:\"boolean\"}\n",
        "HiRes_300dpi = True #@param {type:\"boolean\"}\n",
        "\n",
        "# ensemble_ls = ensemble_lexicon_ls + ensemble_heuristic_ls + ensemble_tradml_ls + \\\n",
        "#               ensemble_dnn_ls + ensemble_transformer_ls\n",
        "\n",
        "print(f'Confirm these are the all the Model(s) to include in the Ensemble:\\n')\n",
        "\n",
        "for i, amodel in enumerate(models_subensemble_ls):\n",
        "  print(f'  Model #{i:>2}: {amodel}') # ({models_dt[amodel][1]})')\n",
        "\n",
        "print(f'\\nIf there is an error in these {len(models_subensemble_ls)} models, \\n go back and rerun the previous code cells.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ2TrYv6NWDk"
      },
      "source": [
        "## Detailed SMA Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JeCPU1XReyw"
      },
      "source": [
        "Novel_Title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u14TnhlRKjva"
      },
      "source": [
        "# Plot all the Model values that have been Smoothed then Standardized\n",
        "\n",
        "plt.figure(figsize=(30,15))\n",
        "\n",
        "# cols_stdsma_ls = [x for x in ensemble_df.columns if x.endswith('_stdsma')]\n",
        "\n",
        "# cols_sma_ls = [x for x in ensemble_df.columns if x.endswith('_sma')]\n",
        "for acol in models_subensemble_ls:\n",
        "  if acol in models_lexicon_ls:\n",
        "    alinestyle = 'solid'\n",
        "  elif acol in models_heuristic_ls:\n",
        "    alinestyle = 'solid'\n",
        "  elif acol in models_tradml_ls:\n",
        "    alinestyle = 'dashdot'\n",
        "  elif acol in models_dnn_ls:\n",
        "    alinestyle = 'dotted'\n",
        "  elif acol in models_transformer_ls:\n",
        "    alinestyle='dashed'\n",
        "\n",
        "  acol_stdsma = f'{acol}_stdsma'\n",
        "  subensemble_df[acol_stdsma].plot(alpha=0.5, label=acol, linewidth=2, linestyle=alinestyle)\n",
        "\n",
        "plt.grid(True)\n",
        "# plt.title(f'{Novel_Title[0]} \\n Sentiment Analysis \\n SMA {Window_Percent}% Smoothing then {Model_Standardization_Method} Standardization', fontsize=20)\n",
        "plt.title(f'{Novel_Title} \\n SentimentArcs for all {len(models_subensemble_ls)} Ensemble Models \\n SMA {Window_Percent}% Smoothing then {Model_Standardization_Method} Standardization', fontsize=20)\n",
        "plt.xlabel('Sentence Number', fontsize=14)\n",
        "plt.ylabel('Sentiment', fontsize=14)\n",
        "\n",
        "plt.legend(title='Model Name', loc='best', fontsize=14)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', title='Model Name', prop={\"size\":16})\n",
        "# plt.tight_layout()\n",
        "\n",
        "if Save_Plot:\n",
        "  # get_fullpath(ftype='data_clean', first_note='vader', last_note='anote bnote', plot_ext='png', no_date=True)\n",
        "  if HiRes_300dpi:\n",
        "    #fig.set_size_inches(30.,18.)\n",
        "    # figsize_tup = (40,20)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'{len(models_subensemble_ls)}models', last_note=f'sma{Window_Percent}_300dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=300)\n",
        "    print(f'Saved hi-res 300dpi Plot to: {plot_fullpath}')\n",
        "  else:\n",
        "    # figsize_tup = (20,10)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'{len(models_subensemble_ls)}models', last_note=f'sma{Window_Percent}_100dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=100)\n",
        "    print(f'Saved low-res 100dpi Plot to: {plot_fullpath}')\n",
        "\n",
        "plt.show();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpk9lxf6NRzF"
      },
      "source": [
        "## General LOWESS Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJH-_YFvumzE"
      },
      "source": [
        "def get_lowess_sma(adf, acol, awin_per=10, afrac=0.08):\n",
        "  '''\n",
        "  Given a DataFrame, Column Name and Frac float\n",
        "  Return an (n x 2)np.array of LOWESS x,y smoothed values and SMA y_sma values\n",
        "  '''\n",
        "\n",
        "  # win_per = 10 # SMA window as % of corpus length\n",
        "  win_size = int(awin_per/100*adf.shape[0])\n",
        "  \n",
        "  # Generate data looking like cosine\n",
        "  x = adf.index # np.random.uniform(0, 4 * np.pi, size=200)\n",
        "  # Experiment: apply LOWESS smoothing after SMA smoothing\n",
        "  y_sma = adf[acol].rolling(win_size, center=True, min_periods=0).mean().values # np.cos(x) + np.random.random(size=len(x))\n",
        "  y = adf[acol]\n",
        "\n",
        "  # Compute a lowess smoothing of the data\n",
        "  smoothed = sm.nonparametric.lowess(exog=x, endog=y, frac=afrac)\n",
        "\n",
        "  return x, y, y_sma, smoothed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsHgTFkGSDDN"
      },
      "source": [
        "# Get LOWESS smoothed values for all Standardized Models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axGb9Tgw91ah"
      },
      "source": [
        "ensemble_df.info(verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S8ukRtNL1v9"
      },
      "source": [
        "ensemble_df.drop(columns=[x for x in ensemble_df.columns if x.endswith('lowess')], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "wXBV1OgLS8gQ"
      },
      "source": [
        "#@title Set LOWESS smoothed values of Model\n",
        "\n",
        "# LOWESS Smoothing Fraction Size\n",
        "LOWESS_Fraction = 20 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "# Confirm Ensemble Models and Optionally Save Plot to File\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P-AvepzEx_C"
      },
      "source": [
        "# Compute LOWESS smoothed and LOWESS smoothed+Standardized time series in ensemble DataFrame\n",
        "\n",
        "# novel_lowess_dt = {}\n",
        "model_labels_dt = {}\n",
        "\n",
        "# If does not exist, calculate LOWESS(*_lowess) and LOWESS+Standardized(*_stdlowess) values/columns\n",
        "if ~subensemble_df.columns.str.contains('_stdlowess').any():\n",
        "  for i, amodel in enumerate(models_subensemble_ls):\n",
        "    print(f'Calculating (a)LOWESS and (b)LOWESS+Standardized values for Model #{i:>2}: {amodel}')\n",
        "    lowess_frac = 1./int(LOWESS_Fraction)\n",
        "    amodel_stdsma = f'{amodel}_stdsma'\n",
        "    amodel_lowess = f'{amodel}_lowess'\n",
        "    amodel_stdlowess = f'{amodel}_stdlowess'\n",
        "\n",
        "    # KEY DECISION: (LOWESS of SMA+STD:amodel_stdsma) > (LOWESS of RAW+STD:amodel)\n",
        "    # _, _, _, smoothed = get_lowess(subensemble_df, amodel, awin_per=Window_Percent, afrac=lowess_frac)\n",
        "    _, _, _, smoothed = get_lowess_sma(subensemble_df, amodel_stdsma, awin_per=Window_Percent, afrac=lowess_frac)\n",
        "    x_vals = smoothed[:, 0]\n",
        "    y_vals = smoothed[:, 1]\n",
        "    # novel_lowess_dt[amodel] = (x_vals, y_vals)\n",
        "    subensemble_df[amodel_lowess] = pd.Series(y_vals)\n",
        "    subensemble_df[amodel_stdlowess] = scaler.fit_transform(subensemble_df[amodel_lowess].values.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnTq9FRyFLbg"
      },
      "source": [
        "subensemble_df.info(verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPrCapqGFwi4"
      },
      "source": [
        "subensemble_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqfwNqA_WAHU"
      },
      "source": [
        "## Plot Ensemble of Selected Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjqVK19mUPTq"
      },
      "source": [
        "subensemble_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0U1efsVUFOM"
      },
      "source": [
        "stdlowess_ls = [x for x in subensemble_df.columns if x.endswith('_stdlowess')]\n",
        "stdlowess_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4veOgVHqFP7I"
      },
      "source": [
        "# EDA plot of All LOWESS Smoothed Standardized+SMA SentimentArcs by Model Family\n",
        "\n",
        "#@markdown Confirm Ensemble SentimentArcs and Option to Saved:\n",
        "Save_Plot = True #@param {type:\"boolean\"}\n",
        "HiRes_300dpi = True #@param {type:\"boolean\"}\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "stdlowess_ls = [x for x in subensemble_df.columns if x.endswith('_stdlowess')]\n",
        "\n",
        "# ax, fig = plt.subplot()\n",
        "\n",
        "for i, an_arc in enumerate(stdlowess_ls):\n",
        "  subensemble_df[an_arc].plot(label=an_arc, alpha=0.3)\n",
        "\n",
        "# plt.title(f'{Novel_Title[0]} \\n Sentiment Analysis \\n LOWESS with {Model_Standardization_Method} Standardization')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.title(f'{Novel_Title} \\n SentimentArcs for All {len(models_subensemble_ls)} Models in Ensemble Models \\n LOWESS frac={LOWESS_Fraction} Smoothing of {Model_Standardization_Method} Standardized SMA {Window_Percent}%', fontsize=20)\n",
        "plt.xlabel('Sentence Number', fontsize=12)\n",
        "plt.ylabel('Sentiment', fontsize=12)\n",
        "plt.legend(title='Model Name', loc='upper right', fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', title='Model Name')\n",
        "plt.tight_layout()\n",
        "\n",
        "if Save_Plot:\n",
        "  # get_fullpath(ftype='data_clean', first_note='vader', last_note='anote bnote', plot_ext='png', no_date=True)\n",
        "  if HiRes_300dpi:\n",
        "    # fig.set_size_inches(30.,18.)\n",
        "    # figsize_tup = (40,20)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'lowess{LOWESS_Fraction}_of_{Model_Standardization_Method}_SMA{Window_Percent}_300dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=300)\n",
        "    print(f'Saved hi-res 300dpi Plot to: {plot_fullpath}')\n",
        "  else:\n",
        "    # figsize_tup = (20,10)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'lowess{LOWESS_Fraction}_of_{Model_Standardization_Method}_SMA{Window_Percent}__100dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=100)\n",
        "    print(f'Saved low-res 100dpi Plot to: {plot_fullpath}')\n",
        "\n",
        "plt.show();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeAMdqnuVBzw"
      },
      "source": [
        "# Save Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc_mjuKDVBzz"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioGZiRJ9VBz3"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUE1nVpmVBz3"
      },
      "source": [
        "# Save newly cleaned (Smoothed then Standardized) Ensemble Model data\n",
        "\n",
        "file_fullpath = get_fullpath(ftype='data_clean')\n",
        "subensemble_df.to_csv(file_fullpath)\n",
        "\n",
        "print(f'Saved to file: {file_fullpath}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYyMPzx7VBz5"
      },
      "source": [
        "# Verify saved file contents\n",
        "\n",
        "!head -n 10 $file_fullpath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "643yixr7U6E9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmlz466gU6SR"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ormrcJA7WRDk"
      },
      "source": [
        "## Plot Individual Model within Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bB_A51EWfso"
      },
      "source": [
        "# Get list and string for all LOWESS smoothed SMA+STD Arcs\n",
        "\n",
        "stdlowess_ls = [x for x in ensemble_df.columns if x.endswith('_stdlowess')]\n",
        "\n",
        "stdlowess_str = ','.join([f\"'{x}'\" for x in stdlowess_ls if x.endswith('_stdlowess')])\n",
        "stdlowess_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH8pF3PiWQ2G"
      },
      "source": [
        "# EDA plot of LOWESS Smoothed Standardized+SMA SentimentArcs by Model Family\n",
        "\n",
        "#@title Select which Model to Plot LOWESS Smoothed of SMA+STD:\n",
        "\n",
        "Model_Name = \"vader_stdlowess\" #@param ['sentimentr_stdlowess','syuzhetr_stdlowess','bing_stdlowess','sentiword_sentimentr_stdlowess','senticnet_sentimentr_stdlowess','nrc_stdlowess','afinn_stdlowess','vader_stdlowess','textblob_stdlowess','pattern_stdlowess','stanza_stdlowess','flair_stdlowess','jockersrinker_sentimentr_stdlowess','jockers_sentimentr_stdlowess','bing_sentimentr_stdlowess','lmcd_sentimentr_stdlowess','roberta15lg_stdlowess','yelp_stdlowess','nlptown_stdlowess','huggingface_stdlowess','hinglish_stdlowess','imdb2way_stdlowess','t5imdb50k_stdlowess','robertaxml8lang_stdlowess','fcn_stdlowess','lstm_stdlowess','cnn_stdlowess','multinb_stdlowess','logreg_stdlowess','logreg_cv_stdlowess','rf_stdlowess','xgb_stdlowess','flaml_stdlowess','autogluon_stdlowess','median_stdlowess']\n",
        "\n",
        "# Novel_Name = {Novel_Title[0]} #@param [Novel_Title[0]]\n",
        "\n",
        "#@markdown Confirm Ensemble SentimentArcs and Option to Saved:\n",
        "Save_Plot = True #@param {type:\"boolean\"}\n",
        "HiRes_300dpi = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "plt.figure(figsize=(30,20))\n",
        "\n",
        "ensemble_df[Model_Name].plot()\n",
        "\n",
        "plt.grid(True)\n",
        "plt.title(f'{Novel_Title[0]} \\n SentimentArcs for {\" \".join(Model_Name.split(\"_\")[:-1]).capitalize()} Model in Ensemble \\n LOWESS frac={LOWESS_Fraction} Smoothing of {Model_Standardization_Method} Standardized SMA {Window_Percent}%', fontsize=20)\n",
        "plt.xlabel('Sentence Number')\n",
        "plt.ylabel('Sentiment')\n",
        "# plt.legend(title='Model Name', loc='best')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', title='Model Name')\n",
        "plt.tight_layout()\n",
        "\n",
        "if Save_Plot:\n",
        "  # get_fullpath(ftype='data_clean', first_note='vader', last_note='anote bnote', plot_ext='png', no_date=True)\n",
        "  if HiRes_300dpi:\n",
        "    # fig.set_size_inches(30.,18.)\n",
        "    # figsize_tup = (40,20)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'{Model_Name.lower()}', last_note=f'lowess{LOWESS_Fraction}_of_{Model_Standardization_Method}_SMA{Window_Percent}_300dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=300)\n",
        "    print(f'Saved hi-res 300dpi Plot to: {plot_fullpath}')\n",
        "  else:\n",
        "    # figsize_tup = (20,10)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'{Model_Name.lower()}', last_note=f'lowess{LOWESS_Fraction}_of_{Model_Standardization_Method}_SMA{Window_Percent}__100dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=100)\n",
        "    print(f'Saved low-res 100dpi Plot to: {plot_fullpath}')\n",
        "\n",
        "plt.show();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkEi2nDuWFbS"
      },
      "source": [
        "## Plot Model Families within Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qm3Nxt9H96R"
      },
      "source": [
        "# EDA plot of LOWESS Smoothed Standardized+SMA SentimentArcs by Model Family\n",
        "\n",
        "#@title Select which Model Family to Plot LOWESS Smoothed of SMA+STD:\n",
        "\n",
        "Model_Family = \"heuristic\" #@param [\"lexicon\", \"heuristic\", \"tradml\", \"dnn\", \"transformer\"]\n",
        "\n",
        "#@markdown Confirm Ensemble SentimentArcs and Option to Saved:\n",
        "Save_Plot = True #@param {type:\"boolean\"}\n",
        "HiRes_300dpi = True #@param {type:\"boolean\"}\n",
        "\n",
        "stdlowess_ls = [x for x in ensemble_df.columns if x.endswith('_stdlowess')]\n",
        "\n",
        "# Select which family to Plot\n",
        "plot_family = ['lexicon','heuristic','tradml','dnn','transformer']\n",
        "\n",
        "plt.figure(figsize=(30,20))\n",
        "\n",
        "# Plot the LOWESS+Standardized (*_stdlowess columns) SentimentArcs for Ensemble Models\n",
        "for acol in cols_ensemble_ls:\n",
        "  # print(f'Processing Model: {acol}')\n",
        "  if (acol in models_lexicon_ls) & (Model_Family == 'lexicon'):\n",
        "    cols_family_ls = models_lexicon_ls\n",
        "    alinestyle = 'solid'\n",
        "    acol_stdlowess = f'{acol}_stdlowess'\n",
        "    ensemble_df[acol_stdlowess].plot(alpha=0.5, label=acol_stdlowess, linewidth=2, linestyle=alinestyle)\n",
        "  elif (acol in models_heuristic_ls) & (Model_Family == 'heuristic'):\n",
        "    cols_family_ls = models_heuristic_ls\n",
        "    alinestyle = 'solid'\n",
        "    # Catch exception with LOWESS error on lmcd_sentimentr (too many zero values?)\n",
        "    # BUGFIX for lmcd_sentimentr, works with LOWESS smoothing of STD+SMA\n",
        "    # if acol.startswith('lmcd_sentimentr'):\n",
        "    if acol.startswith('execute_else_stmt'):\n",
        "      # Keep this error check inplace in case future novels/models/params replicate this problem\n",
        "      continue\n",
        "    else:\n",
        "      acol_stdlowess = f'{acol}_stdlowess'\n",
        "      ensemble_df[acol_stdlowess].plot(alpha=0.5, label=acol_stdlowess, linewidth=2, linestyle=alinestyle)\n",
        "  elif (acol in models_tradml_ls) & (Model_Family == 'tradml'):\n",
        "    cols_family_ls = models_tradml_ls\n",
        "    alinestyle = 'dashdot'\n",
        "    acol_stdlowess = f'{acol}_stdlowess'\n",
        "    ensemble_df[acol_stdlowess].plot(alpha=0.5, label=acol_stdlowess, linewidth=2, linestyle=alinestyle)\n",
        "  elif (acol in models_dnn_ls) & (Model_Family == 'dnn'):\n",
        "    cols_family_ls = models_dnn_ls\n",
        "    alinestyle = 'dotted'\n",
        "    acol_stdlowess = f'{acol}_stdlowess'\n",
        "    ensemble_df[acol_stdlowess].plot(alpha=0.5, label=acol_stdlowess, linewidth=2, linestyle=alinestyle)  \n",
        "  elif (acol in models_transformer_ls) & (Model_Family == 'transformer'):\n",
        "    cols_family_ls = models_transformer_ls\n",
        "    alinestyle='dashed'\n",
        "    acol_stdlowess = f'{acol}_stdlowess'\n",
        "    ensemble_df[acol_stdlowess].plot(alpha=0.5, label=acol_stdlowess, linewidth=2, linestyle=alinestyle)\n",
        "\n",
        "plt.grid(True)\n",
        "plt.title(f'{Novel_Title[0]} \\n SentimentArcs for {len(cols_family_ls)} {Model_Family.capitalize()} Models in Ensemble Models \\n LOWESS frac={LOWESS_Fraction} Smoothing of {Model_Standardization_Method} Standardized SMA {Window_Percent}%', fontsize=20)\n",
        "plt.xlabel('Sentence Number')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.legend(title='Model Name', loc='best')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', title='Model Name')\n",
        "plt.tight_layout()\n",
        "\n",
        "if Save_Plot:\n",
        "  # get_fullpath(ftype='data_clean', first_note='vader', last_note='anote bnote', plot_ext='png', no_date=True)\n",
        "  if HiRes_300dpi:\n",
        "    # fig.set_size_inches(30.,18.)\n",
        "    # figsize_tup = (40,20)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'lowess{LOWESS_Fraction}_of_{Model_Standardization_Method}_SMA{Window_Percent}_300dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=300)\n",
        "    print(f'Saved hi-res 300dpi Plot to: {plot_fullpath}')\n",
        "  else:\n",
        "    # figsize_tup = (20,10)\n",
        "    # figsize_str = f'figsize_{figsize_tup[0]}_{figsize_tup[1]}'\n",
        "    # plt.figure(figsize=figsize_tup)\n",
        "    plot_fullpath = get_fullpath(ftype='plot', first_note=f'lowess{LOWESS_Fraction}_of_{Model_Standardization_Method}_SMA{Window_Percent}__100dpi', no_date=True) # last_note=figsize_str, no_date=True)\n",
        "    plt.savefig(plot_fullpath, dpi=100)\n",
        "    print(f'Saved low-res 100dpi Plot to: {plot_fullpath}')\n",
        "\n",
        "plt.show();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojwqGVxTYJ0U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wbfjnorYKKq"
      },
      "source": [
        "# Save Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SzT_IFxYKKr"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA9pNyrWYKKs"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n4ughBqYKKs"
      },
      "source": [
        "# Save newly cleaned (Smoothed then Standardized) Ensemble Model data\n",
        "\n",
        "file_fullpath = get_fullpath(ftype='data_clean')\n",
        "ensemble_df.to_csv(file_fullpath)\n",
        "\n",
        "print(f'Saved to file: {file_fullpath}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KDYiVRVYKKu"
      },
      "source": [
        "# Verify saved file contents\n",
        "\n",
        "!head -n 10 $file_fullpath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl9mUv_RNg1x"
      },
      "source": [
        "# Explore Cruxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vySHv5-JEywP"
      },
      "source": [
        "list(range(10,15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q98_0WL_jSr"
      },
      "source": [
        "def get_crux_context(adf=ensemble_df, acol_sentraw='sent_raw', asent_no=5, ahalf_win=2, do_upper=True):\n",
        "  '''\n",
        "  Given a DataFrame with acol_sentraw of Sentence raw text along with\n",
        "    a crux sent_no and half_win number of sentences on before and after crux point\n",
        "  Return a list of Sentences that define the (2*ahalf_win + 1) context window around the Crux asent_no\n",
        "  '''\n",
        "\n",
        "  context_ls = []\n",
        "\n",
        "  corpus_len = adf.shape[0]\n",
        "\n",
        "  # Get lower bound of Crux Context Window\n",
        "  context_min = asent_no - ahalf_win\n",
        "  if context_min < 0:\n",
        "    context_min = 0\n",
        "\n",
        "  # Get upper bound of Crux Context Window\n",
        "  context_max = asent_no + ahalf_win + 1 # correct for zero-based indexing \n",
        "  if context_max > corpus_len:\n",
        "    context_max = corpus_len - 1\n",
        "\n",
        "  lineno_ls = list(range(context_min,context_max))\n",
        "  context_ls = adf.iloc[context_min:context_max][acol_sentraw].to_list()\n",
        "\n",
        "  if do_upper:\n",
        "    # [f(x) if condition else g(x) for x in sequence]\n",
        "    crux_str = adf.iloc[asent_no][acol_sentraw]\n",
        "    context_ls = [x.upper() if x == crux_str else x for x in context_ls]\n",
        "\n",
        "  context_tup_ls = list(zip(lineno_ls, context_ls))\n",
        "\n",
        "  return context_tup_ls\n",
        "\n",
        "# Test\n",
        "\n",
        "temp_tup_ls = get_crux_context(ensemble_df, 'sent_raw', 20, 5)\n",
        "temp_tup_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DxcfeGzJ68A"
      },
      "source": [
        "from scipy import interpolate\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy import signal\n",
        "from scipy.signal import argrelextrema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8H13HrsJGnt"
      },
      "source": [
        "def get_crux_points(adf, acol_name, text_type='sentence', win_per=5, sec_y_labels=True, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame and a Time Series Column within it and a LOWESS window\n",
        "  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n",
        "  '''\n",
        "\n",
        "  # print('entered get_crux_points') \n",
        "  crux_ls = []\n",
        "\n",
        "  series_len = adf.shape[0]\n",
        "  # print(f'series_len = {series_len}')\n",
        "\n",
        "  # sent_no_min = adf[].min()\n",
        "  sent_no_min = 0\n",
        "  # sent_no_max = adf.sent_no.max()\n",
        "  sent_no_max = series_len - 1\n",
        "  # print(f'sent_no_min {sent_no_min}')\n",
        "\n",
        "  sm_x = adf.index.values\n",
        "  sm_y = adf[acol_name].values.flatten()\n",
        "\n",
        "  half_win = int((win_per/100)*series_len)\n",
        "  # print(f'half_win = {half_win}')\n",
        "  # print(f'sm_y type = {type(sm_y)}')\n",
        "\n",
        "  # Find peaks(max).\n",
        "  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n",
        "  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n",
        "  # peak_indexes = peak_indexes + sent_no_min\n",
        "  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n",
        "  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n",
        "  # peak_indexes_np = peak_indexes_np + sent_no_min\n",
        "  # print(f'peak_indexes type = {type(peak_indexes)}') # sent_no_start sent\n",
        "  peak_indexes = peak_indexes[0]\n",
        "\n",
        "  peak_x_ls = list(peak_indexes)\n",
        "  peak_x_adj_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "\n",
        "  peak_y_ls = list(sm_y[peak_indexes])\n",
        "\n",
        "  peak_label_ls = ['peak'] * len(peak_y_ls)\n",
        "  peak_coord_ls = tuple(zip(peak_label_ls, peak_x_adj_ls, peak_y_ls))\n",
        "\n",
        "  # peak_y_all_ls = peak_y_ls + valley_y_ls\n",
        "  # crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # Find valleys(min).\n",
        "  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n",
        "  valley_indexes = valley_indexes[0]\n",
        "  \n",
        "  valley_x_ls = list(valley_indexes)\n",
        "  valley_x_adj_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "\n",
        "  valley_y_ls = list(sm_y[valley_indexes])\n",
        "\n",
        "  valley_label_ls = ['valley'] * len(valley_y_ls)\n",
        "  valley_coord_ls = tuple(zip(valley_label_ls, valley_x_adj_ls, valley_y_ls))\n",
        "\n",
        "  # Combine Peaks and Valley Coordinates into List of Tuples(label, x_coord, y_coord)\n",
        "  crux_coord_ls = peak_coord_ls + valley_coord_ls\n",
        "\n",
        "  # Save all peaks/valleys as list of (x,y) coordinate tuples\n",
        "  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n",
        "  #  x_all_ls = peak_x_ls + valley_x_ls\n",
        "  # readjust starting Sentence No to start with first sentence in segement window\n",
        "  #  x_all_ls = [x+sent_no_min for x in x_all_ls]\n",
        "  #  y_all_ls = peak_y_ls + valley_y_ls\n",
        "  # crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n",
        "\n",
        "\n",
        "  if do_plot == True:\n",
        "    # Plot main graph.\n",
        "    (fig, ax) = plt.subplots()\n",
        "    ax.plot(sm_x, sm_y)\n",
        "\n",
        "    # corpus_sects_df\n",
        "    if sec_y_labels == True:\n",
        "      section_sent_no_boundries_ls = list(corpus_sects_df['sent_no_start'])\n",
        "      section_no_ls = list(corpus_sects_df['sect_no'])\n",
        "      for i, asect_no in enumerate(section_sent_no_boundries_ls):\n",
        "        # Plot vertical lines for section boundries\n",
        "        plt.text(asect_no, sec_y_height, f'Section #{section_no_ls[i]}', alpha=0.2, rotation=90)\n",
        "        plt.axvline(asect_no, color='blue', alpha=0.1)    \n",
        "\n",
        "\n",
        "    win_half = 0 # 2500\n",
        "\n",
        "    # Plot peaks.\n",
        "    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n",
        "\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "    ax.scatter(peak_x_ls, peak_y_ls)\n",
        "    for i, txt in enumerate(list(peak_x_ls)):\n",
        "        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, ha='center', va='bottom', annotation_clip=True)\n",
        "\n",
        "    # Plot valleys.\n",
        "    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "    ax.scatter(valley_x_ls, valley_y_ls)\n",
        "    for i, txt in enumerate(list(valley_x_ls)):\n",
        "        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, ha='center', va='top', annotation_clip=True) # xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n",
        "\n",
        "    # for i, txt in enumerate(list(valley_x_ls)):\n",
        "    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n",
        "    # plt.plot(x, y, 'bo')\n",
        "    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n",
        "    # adjust_text(texts)\n",
        "\n",
        "    # Confidence Interval (Min/Max Range)\n",
        "    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n",
        "    plt.suptitle(f'{Novel_Title[0]} \\n SentimentArc Crux Detection for Model: {acol_name} \\n SMA ({Window_Percent}%) Smoothed then Standardized ({Model_Standardization_Method}) \\n {subtitle_str}', size=16, y=1.05);    \n",
        "    # plt.title(f'{Novel_Title[0]} \\n SentimentArc Crux Detection for Model: {acol_name} \\n SMA ({Window_Percent}%) Smoothed then Standardized ({Model_Standardization_Method}) \\n {subtitle_str}')\n",
        "    plt.xlabel(f'Sentence No') # within selected Section #{Select_Section_No}')\n",
        "\n",
        "    # locs, labels = xticks()  # Get the current locations and labels.\n",
        "    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n",
        "\n",
        "    plt.ylabel(f'Sentiment Value')\n",
        "    # plt.legend(loc='best');\n",
        "  \n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plt.title(f'{BOOK_TITLE_FULL} \\n SMA Smoothed Sentence Sentiment Arcs Crux Points')\n",
        "    # plt.legend(loc='best')\n",
        "    plt.savefig(f\"{CORPUS_FILENAME.split('.')[0]}_find_peaks.png\")\n",
        "\n",
        "  return crux_coord_ls;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QVPFLrSKqvm"
      },
      "source": [
        "# Test\n",
        "\n",
        "crux_tup_ls = get_crux_points(ensemble_df, 'vader_stdsma', text_type='sentence', win_per=5, sec_y_labels=False, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdEn08QG8tvX"
      },
      "source": [
        "def crux_sortsents_report(crux_ls, ts_df=ensemble_df, library_type='baseline', top_n=3, get_peaks=True, sort_by='sent_no', n_sideparags=1, sentence_highlight=True):\n",
        "  '''\n",
        "  Wrapper function to produce report based upon 'crux_sortsents() described as:\n",
        "    Given a list of tuples (sent_no, sentiment value), top_n cruxes to retrieve and bool flag get_peaks\n",
        "    Return a sorted list of peaks/valleys (sentiment_value, sent_no, sent_raw) from greatest down for top_n items\n",
        "\n",
        "    # get_sentnocontext_report\n",
        "  '''\n",
        "\n",
        "  if get_peaks == True:\n",
        "    crux_label = 'Peak'\n",
        "  else:\n",
        "    crux_label = 'Valley'\n",
        "\n",
        "  # Filter and keep only the desired crux type in List crux_subset_ls\n",
        "  crux_subset_ls = []\n",
        "  for acrux_tup in crux_ls:\n",
        "    crux_type, crux_x_coord, crux_y_coord = acrux_tup\n",
        "    if crux_type.lower() == crux_label.lower():\n",
        "      crux_subset_ls.append((crux_x_coord,crux_y_coord)) # Append a Tuple to List\n",
        "\n",
        "  flag_2few_cruxes = False\n",
        "\n",
        "  # Check to see if asked for more Cruxes than were found \n",
        "  top_n_found = len(crux_subset_ls)\n",
        "  if top_n_found < top_n:\n",
        "    flag_2few_cruxes = True\n",
        "    print(f'\\n\\nWARNING: You asked for {top_n} {crux_label}s\\n         but there only {top_n_found} were found above.\\n')\n",
        "    print(f'             Displaying as many {crux_label}s as possible,')\n",
        "    print(f'             to retrieve more, go back to the previous code cells and re-run with wider Crux Window.\\n\\n')\n",
        "\n",
        "\n",
        "  # Get Sentence no and raw text for appropriate Crux subset\n",
        "  # print(f'Calling crux_n_top_ls with crux_subset_ls={crux_subset_ls}\\ntop_n={top_n}\\nget_peaks={get_peaks}')\n",
        "  crux_n_top_ls = crux_sortsents(corpus_df = ts_df, crux_ls=crux_subset_ls, atop_n=top_n, get_peaks=get_peaks, sort_key=sort_by)\n",
        "  # print(f'Returning crux_n_top_ls = {crux_n_top_ls}')\n",
        "\n",
        "  # Print appropriate header Select_Section_No sent_no\n",
        "  print('------------------------------')\n",
        "  # print(f'library_type: {library_type}')\n",
        "  if library_type in ['baseline','sentimentr','syuzhetr','transformer','unified']:\n",
        "    if (sort_by != 'sent_no') & (flag_2few_cruxes == False):\n",
        "      print(f'Library: {library_type.capitalize()} ALL Top {top_n} {crux_label}s Found\\n')\n",
        "    else:\n",
        "      print(f'Library #{library_type.capitalize()} ONLY Top {top_n_found} {crux_label}s Found\\n')\n",
        "  else:\n",
        "    if (sort_by != 'sent_no') & (flag_2few_cruxes == False):\n",
        "      print(f'Section #{Select_Section_No} ALL Top {top_n} {crux_label}s Found\\n')\n",
        "    else:\n",
        "      print(f'Section #{Select_Section_No} ONLY Top {top_n_found} {crux_label}s Found\\n')\n",
        "\n",
        "  # Print summary of subset Cruxes\n",
        "  for i,crux_sent_tup in enumerate(crux_n_top_ls):\n",
        "    # crux_type, crux_x_coord, crux_y_coord = crux_sent_tup\n",
        "    crux_x_coord, crux_y_coord, crux_sent_raw = crux_sent_tup\n",
        "    print(f'   {crux_label} #{i} at Sentence #{crux_x_coord} with Sentiment Value {crux_y_coord}')\n",
        "  # print('------------------------------\\n')\n",
        "  # print('Sent_No  Sentiment   Sentence (Raw Text)\\n')\n",
        "  \n",
        "  # Print details of each Crux in subset\n",
        "  for sent_no, sent_pol, sent_raw in crux_n_top_ls: \n",
        "    sent_no = int(sent_no)\n",
        "    print('\\n\\n-------------------------------------------------------------')\n",
        "    print(f'Sentence #{sent_no}   Sentiment: {sent_pol:.3f}\\n') #     {sent_raw}\\n')\n",
        "    # print('------------------------------')\n",
        "    get_sentnocontext_report(ts_df=ts_df, the_sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)\n",
        "    # get_sentnocontext(sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRPog2AY_g-M"
      },
      "source": [
        "def get_lowess_cruxes(ts_df, col_series, text_type='sentence', win_lowess=5, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n",
        "  '''\n",
        "  Given a DataFrame and a Time Series Column within it and a LOWESS window\n",
        "  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n",
        "  '''\n",
        "\n",
        "  crux_ls = []\n",
        "\n",
        "  series_len = ts_df.shape[0]\n",
        "\n",
        "  sent_no_min = ts_df.sent_no.min()\n",
        "  sent_no_max = ts_df.sent_no.max()\n",
        "  # print(f'sent_no_min {sent_no_min}')\n",
        "\n",
        "  sm_x = ts_df.index.values\n",
        "  sm_y = ts_df[col_series].values\n",
        "\n",
        "  half_win = int((win_lowess/100)*series_len)\n",
        "\n",
        "  # Find peaks(max).\n",
        "  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n",
        "  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n",
        "  # peak_indexes = peak_indexes + sent_no_min\n",
        "  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n",
        "  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n",
        "  # peak_indexes_np = peak_indexes_np + sent_no_min\n",
        "  peak_indexes = peak_indexes[0]\n",
        "\n",
        "  peak_x_ls = list(peak_indexes)\n",
        "  peak_y_ls = list(sm_y[peak_indexes])\n",
        "\n",
        "  # Find valleys(min).\n",
        "  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n",
        "  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n",
        "  valley_indexes = valley_indexes[0]\n",
        "  \n",
        "  valley_x_ls = list(valley_indexes)\n",
        "  valley_y_ls = list(sm_y[valley_indexes])\n",
        "\n",
        "  # Save all peaks/valleys as list of (x,y) coordinate tuples\n",
        "  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n",
        "  x_all_ls = peak_x_ls + valley_x_ls\n",
        "  # readjust starting Sentence No to start with first sentence in segement window\n",
        "  x_all_ls = [x+sent_no_min for x in x_all_ls]\n",
        "  y_all_ls = peak_y_ls + valley_y_ls\n",
        "  crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n",
        "\n",
        "  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n",
        "\n",
        "\n",
        "  if do_plot == True:\n",
        "    # Plot main graph.\n",
        "    (fig, ax) = plt.subplots()\n",
        "    ax.plot(sm_x, sm_y)\n",
        "\n",
        "    if text_type == 'sentence':\n",
        "      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "      for i, aparag in enumerate(paragraph_boundries_ls):\n",
        "        if i%5 == 0:\n",
        "          # Plot every 5th paragraph\n",
        "          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "          plt.text(sent_no, sec_y_height, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n",
        "          plt.axvline(sent_no, color='blue', alpha=0.1)\n",
        "    elif text_type == 'paragraph':\n",
        "      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n",
        "      for i, aparag_no in enumerate(paragraph_boundries_ls):\n",
        "        if i%5 == 0:\n",
        "          # Plot every 5th paragraph\n",
        "          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n",
        "          plt.text(aparag_no, sec_y_height, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n",
        "          plt.axvline(aparag_no, color='blue', alpha=0.1)    \n",
        "    else:\n",
        "      print(f\"ERROR: text_type is {text_type} but must be either 'sentence' or 'paragarph'\")\n",
        "\n",
        "    win_half = 0 # 2500\n",
        "\n",
        "    # Plot peaks.\n",
        "    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n",
        "\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n",
        "    ax.scatter(peak_x_ls, peak_y_ls)\n",
        "    for i, txt in enumerate(list(peak_x_ls)):\n",
        "        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n",
        "\n",
        "    # Plot valleys.\n",
        "    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n",
        "    # readjust starting Sentence No to start with first sentence in segement window\n",
        "    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n",
        "    ax.scatter(valley_x_ls, valley_y_ls)\n",
        "    for i, txt in enumerate(list(valley_x_ls)):\n",
        "        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n",
        "\n",
        "    # for i, txt in enumerate(list(valley_x_ls)):\n",
        "    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n",
        "    # plt.plot(x, y, 'bo')\n",
        "    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n",
        "    # adjust_text(texts)\n",
        "\n",
        "    # Confidence Interval (Min/Max Range)\n",
        "    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n",
        "\n",
        "    plt.title(f'{CORPUS_FULL} Raw Sentence Crux Detection in Section #{Select_Section_No}\\nLOWESS Smoothed {subtitle_str} and SciPy find_peaks')\n",
        "    plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n",
        "\n",
        "    # locs, labels = xticks()  # Get the current locations and labels.\n",
        "    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n",
        "\n",
        "    plt.ylabel(f'Sentiment Value')\n",
        "    plt.legend(loc='best');\n",
        "  \n",
        "  if save2file == True:\n",
        "    # Save graph to file.\n",
        "    plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Crux Points via SciPy.argrelextrema')\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig('argrelextrema.png')\n",
        "\n",
        "  return crux_coord_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8QX8tSKM376"
      },
      "source": [
        "## Search Corpus for Substring\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* In [Search_for_Substring] enter a Substring to search for in the Corpus\n",
        "\n",
        "* Enter a Substring long enough/unique enough so only a reasonable number of Sentences will be returned\n",
        "\n",
        "* Substring can contain spaces/punctuation, for example: 'in the garden'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJYjOu9Ks_pL"
      },
      "source": [
        "# Search Corpus Sentences for Substring\n",
        "\n",
        "Search_for_Substring = \"abuse\" #@param {type:\"string\"}\n",
        "\n",
        "match_sentno_ls = ensemble_df[ensemble_df['sent_raw'].str.contains(Search_for_Substring, regex=False)]['sent_no']\n",
        "\n",
        "for i, asentno in enumerate(match_sentno_ls):\n",
        "  # sentno, sentraw = asent\n",
        "  print(f\"\\n\\nMatch #{i}: Sentence #{asentno}\\n\\n\")\n",
        "  sent_highlight = re.sub(Search_for_Substring, Search_for_Substring.upper(), ensemble_df.iloc[asentno]['sent_raw'])\n",
        "  print(f'    {sent_highlight}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajEpjFVPa6jD"
      },
      "source": [
        "# Get Context around Matching Sentences\n",
        "\n",
        "#@title Get Context Around Each Matching Sentence:\n",
        "\n",
        "# Context Details\n",
        "No_Paragraphs_on_Each_Side = 3 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "\n",
        "crux_context_dt = {}\n",
        "for i, asent_no in enumerate(match_sentno_ls):\n",
        "  crux_tup_ls = get_crux_context(adf=ensemble_df, acol_sentraw='sent_raw', asent_no=asent_no, ahalf_win=No_Paragraphs_on_Each_Side, do_upper=Highlight_Sentence)\n",
        "  crux_context_dt[asent_no] = crux_tup_ls\n",
        "\n",
        "\n",
        "for key, val in crux_context_dt.items():\n",
        "  print(f'\\nCrux No: {key}')\n",
        "  for aval in val:\n",
        "    alineno, asent = aval\n",
        "    print(f'  Line #{alineno}: {asent}')\n",
        "  print('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap_K_gpH0FTm"
      },
      "source": [
        "**Plot Top-n Crux Peaks/Valleys for selected Model**\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Crux_Window_Percent] exclusive zone around Crux Points as a percentage of Corpus length\n",
        "\n",
        "* [Sentiment_Model] Select a Sentiment Analysis model\n",
        "\n",
        "* Select [Anomaly_Detction] to plot raw Sentiment values to detect outlier/anomaly Sentences. Leave unchecked to plot SMA smoothed Sentiment arc and detect Crux points\n",
        "\n",
        "* Select [Save_to_File] to also save plot to external *.png file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufnb6YB6HFtL"
      },
      "source": [
        "%whos list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br_uMcIoHJQn"
      },
      "source": [
        "# Generate the list of Model names currently selected to be in the Ensemble\n",
        "#   need to paste into drop down below\n",
        "\n",
        "# TODO: Automatically generate values for all models, not just those selected to be in Ensemble\n",
        "\n",
        "temp_ls = []\n",
        "\n",
        "for acol in cols_models_ls:\n",
        "  temp_ls.append(f\"'{acol}'\")\n",
        "\n",
        "temp_str = ','.join(temp_ls)\n",
        "temp_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHPx5a0xvJYb"
      },
      "source": [
        "Crux_Window_Percent = 5 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "Model_Name = \"vader\" #@param ['afinn','bing','nrc','syuzhetr','sentimentr','pattern','vader','bing_sentimentr','sentiword_sentimentr','senticnet_sentimentr','lmcd_sentimentr','jockers_sentimentr','jockersrinker_sentimentr','logreg','logreg_cv','multinb','textblob','rf','xgb','flaml','autogluon','fcn','lstm','cnn','stanza','flair','huggingface','t5imdb50k','hinglish','yelp','imdb2way','nlptown','robertaxml8lang','roberta15lg','median']\n",
        "# Anomaly_Detection = False #@param {type:\"boolean\"}\n",
        "# Vertical_Labels = True #@param {type:\"boolean\"}\n",
        "# Vertical_Labels_Height = -0.1 #@param {type:\"slider\", min:-50, max:50, step:0.1}\n",
        "# Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "amodel_stdsma = f'{Model_Name}_stdsma'\n",
        "crux_tup_ls = get_crux_points(ensemble_df, amodel_stdsma, text_type='sentence', win_per=5, sec_y_labels=False, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6PU1zR8vJYf"
      },
      "source": [
        "## Context around Top-n Crux Peaks/Valleys\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "* Select [Get_Peak_Cruxes] to retrieve Peaks (if unchecked Valleys are retrieved)\n",
        "\n",
        "* [Get_n_Cruxes] determines how many Top-n Cruxes to retrieve\n",
        "\n",
        "* Enter [No_Paragraphs_on_Each_Side] to retrieve this many Paragraphs before and after the Paragraph containing your Crux Sentence (e.g. 2 will bring back 5 paragraphs centered around the Paragraph containing the Crux Sentence)\n",
        "\n",
        "* Select [Highlight_Crux_Sentence] to have the Crux Sentence converted to ALL CAPS for easier identification. The Paragraph containing the Crux Sentence will be prefaced with a '<*>' as well.\n",
        "\n",
        "* Select [Save_to_File] to also save output to external *.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6U2kzrfOeQ9"
      },
      "source": [
        "crux_tup_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUoJz_nyvJYh"
      },
      "source": [
        "#@title Get Context around Crux Points:\n",
        "\n",
        "#@markdown Crux Point Details\n",
        "# Get_Peak_Cruxes = False #@param {type:\"boolean\"}\n",
        "Get_n_Cruxes = 20 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "Sort_by_SentenceNo = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Context Details\n",
        "No_Paragraphs_on_Each_Side = 5 #@param {type:\"slider\", min:0, max:15, step:1}\n",
        "Highlight_Sentence = True #@param {type:\"boolean\"}\n",
        "# Save_to_Report = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "if Sort_by_SentenceNo == True:\n",
        "  sort_on = 'sent_no'\n",
        "else:\n",
        "  sort_on = Model_Name  # Selected in previous code cell\n",
        "\n",
        "\n",
        "crux_context_dt = {}\n",
        "crux_peaks_dt = {}\n",
        "crux_valleys_dt = {}\n",
        "for i, acrux_tup in enumerate(crux_tup_ls):\n",
        "  crux_type, crux_sentno, crux_sentiment = acrux_tup\n",
        "  acrux_ls = get_crux_context(adf=ensemble_df, acol_sentraw='sent_raw', asent_no=crux_sentno, ahalf_win=No_Paragraphs_on_Each_Side, do_upper=Highlight_Sentence)\n",
        "  if crux_type == 'peak':\n",
        "    if Sort_by_SentenceNo:\n",
        "      crux_peaks_dt[crux_sentno] = ['peak', crux_sentno, crux_sentiment, acrux_ls]\n",
        "    else:\n",
        "      crux_peaks_dt[crux_sentiment] = ['peak', crux_sentno, crux_sentiment, acrux_ls]\n",
        "  elif crux_type == 'valley':\n",
        "    if Sort_by_SentenceNo:\n",
        "      crux_valleys_dt[crux_sentno] = ['valley', crux_sentno, crux_sentiment, acrux_ls]\n",
        "    else:\n",
        "      crux_valleys_dt[crux_sentiment] = ['valley', crux_sentno, crux_sentiment, acrux_ls]\n",
        "\n",
        "if Sort_by_SentenceNo:\n",
        "  print(f'\\n\\n{Novel_Title[0]} \\nCrux Peaks sorted by Sentence No\\n====================\\n')\n",
        "  for key, val in crux_peaks_dt.items():\n",
        "    # print(f'\\nCrux at Sentence No: {key}')\n",
        "    acrux_type, acrux_sentno, acrux_sentiment, acrux_ls = val\n",
        "    print(f'\\nCrux at Sentence No: {key}  ({acrux_type}: {acrux_sentiment:.3f})')\n",
        "    for acrux_sent in acrux_ls:\n",
        "      print(f'    {acrux_sent}')\n",
        "  print(f'\\n\\n{Novel_Title[0]} \\nCrux Valleys sorted by Sentence No\\n====================\\n')\n",
        "  for key, val in crux_valleys_dt.items():\n",
        "    # print(f'\\nCrux at Sentence No: {key}')\n",
        "    acrux_type, acrux_sentno, acrux_sentiment, acrux_ls = val\n",
        "    print(f'\\nCrux at Sentence No: {key}  ({acrux_type}: {acrux_sentiment:.3f})')\n",
        "    for acrux_sent in acrux_ls:\n",
        "      print(f'    {acrux_sent}')\n",
        "\n",
        "\"\"\"\n",
        "else:\n",
        "  for key, val in crux_peaks_dt.items():\n",
        "    print(f'\\nCrux at Polarity: {key}')\n",
        "    acrux_sentno, acrux_sentiment, acrux_ls = val\n",
        "    print(f'  Sentiment: {acrux_sentiment}')\n",
        "    for acrux_sent in acrux_ls:\n",
        "      print(f'    {acrux_sent}')\n",
        "\n",
        "\n",
        "\n",
        "    for aval in val:\n",
        "      acrux_sentno, acrux_sentiment, acrux_ls = aval\n",
        "      print(f'  Sentiment: {acrux_sentiment}')\n",
        "      for aline \n",
        "      print(f'  Line #{alineno}: {asent}')\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "print(f'Crux Report --------------------\\n')\n",
        "print(f'            Corpus: {CORPUS_FULL}')\n",
        "print(f'            Model: {Baseline_SMA_Model}')\n",
        "print(f'            Crux Win%: {Crux_Window_Percent}')\n",
        "print(f'            SMA Win%: {sma_str}')\n",
        "\n",
        "if Save_to_Report == False:\n",
        "  crux_sortsents_report(model_crux_ls, \n",
        "                        ts_df = corpus_sents_df,\n",
        "                        library_type='baseline', \n",
        "                        top_n=Get_n_Cruxes, \n",
        "                        get_peaks=Get_Peak_Cruxes,\n",
        "                        sort_by = sort_on, # sent_no, or abs(polarity)\n",
        "                        n_sideparags=No_Paragraphs_on_Each_Side,\n",
        "                        sentence_highlight=Highlight_Sentence)\n",
        "else:\n",
        "  # import sys\n",
        "  # with open('filename.txt', 'w') as f:\n",
        "  #   print('This message will be written to a file.', file=f)\n",
        "  # https://www.kite.com/python/answers/how-to-get-stdout-and-stderr-from-a-process-as-a-string-in-python\n",
        "  # process = subprocess.run([\"echo\", \"This goes to stdout\"], capture_output=True)\n",
        "  # stdout_as_str = process.stdout.decode(\"utf-8\")\n",
        "  # print(stdout_as_str)\n",
        "  temp_out = StringIO()\n",
        "  sys.stdout = temp_out\n",
        "  crux_sortsents_report(model_crux_ls, top_n=Get_n_Cruxes, get_peaks=Get_Peak_Cruxes, n_sideparags=No_Paragraphs_on_Each_Side)\n",
        "  print(temp_out)\n",
        "  # attempt to save temp_out to generated filename\n",
        "  sys.stdout = sys.__stdout__\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky-Ku-1DU4yq"
      },
      "source": [
        "### Save Crux to File\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad86Yzl-E_yu"
      },
      "source": [
        "%%capture cap --no-stderr\n",
        "\n",
        "# Print Context around each Sentiment Peak\n",
        "\n",
        "\"\"\"\n",
        "novel_sent_len = novel_df.shape[0]\n",
        "halfwin = int(Crux_Sentence_Context_Count/2)\n",
        "crux_sents_ls = []\n",
        "nl = '\\n'\n",
        "\"\"\";\n",
        "\n",
        "print('==================================================')\n",
        "print('============     Peak Crux Points   ==============')\n",
        "print('==================================================\\n\\n')\n",
        "\n",
        "if Sort_by_SentenceNo:\n",
        "  # print(f'\\n\\n{Novel_Title[0]} \\nCrux Peaks sorted by Sentence No\\n====================\\n')\n",
        "  for key, val in crux_peaks_dt.items():\n",
        "    # print(f'\\nCrux at Sentence No: {key}')\n",
        "    acrux_type, acrux_sentno, acrux_sentiment, acrux_ls = val\n",
        "    print(f'\\nCrux at Sentence No: {key}  ({acrux_type}: {acrux_sentiment:.3f})')\n",
        "    for acrux_sent in acrux_ls:\n",
        "      print(f'    {acrux_sent}')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# for i, apeak in enumerate(peaks2):\n",
        "for i, apeak in enumerate(peaks):\n",
        "  crux_sents_ls = []\n",
        "  win_start = max(0, apeak-halfwin)\n",
        "  win_end = min(apeak+halfwin+1, novel_sent_len)\n",
        "  # for sent_idx in range(apeak-halfwin,apeak+halfwin+1):\n",
        "  for sent_idx in range(win_start,win_end):\n",
        "\n",
        "    sent_cur = novel_df.iloc[sent_idx].text_raw\n",
        "    if sent_idx == apeak:\n",
        "      sent_str = sent_cur.upper()\n",
        "    else:\n",
        "      sent_str = sent_cur\n",
        "    crux_sents_ls.append(sent_str)\n",
        "  \n",
        "  # context_ls = novel_df.iloc[apeak-halfwin:apeak+halfwin].text_raw\n",
        "  print(f\"Peak #{i} at Sentence #{apeak}:\\n\\n{nl.join(crux_sents_ls)}\\n\\n\\n\")\n",
        "\"\"\";\n",
        "\n",
        "\n",
        "print('==================================================')\n",
        "print('===========     Crux Valley Points    ============')\n",
        "print('==================================================\\n\\n')\n",
        "\n",
        "if Sort_by_SentenceNo:\n",
        "  # print(f'\\n\\n{Novel_Title[0]} \\nCrux Valleys sorted by Sentence No\\n====================\\n')\n",
        "  for key, val in crux_valleys_dt.items():\n",
        "    # print(f'\\nCrux at Sentence No: {key}')\n",
        "    acrux_type, acrux_sentno, acrux_sentiment, acrux_ls = val\n",
        "    print(f'\\nCrux at Sentence No: {key}  ({acrux_type}: {acrux_sentiment:.3f})')\n",
        "    for acrux_sent in acrux_ls:\n",
        "      print(f'    {acrux_sent}')\n",
        "\n",
        "\"\"\"\n",
        "# for i, avalley in enumerate(valleys2):\n",
        "for i, avalley in enumerate(valleys):\n",
        "  crux_sents_ls = []\n",
        "  win_start = max(0, avalley-halfwin)\n",
        "  win_end = min(avalley+halfwin+1, novel_sent_len)\n",
        "  # for sent_idx in range(avalley-halfwin,avalley+halfwin+1):\n",
        "  for sent_idx in range(win_start,win_end):\n",
        "    sent_cur = novel_df.iloc[sent_idx].text_raw\n",
        "    if sent_idx == avalley:\n",
        "      sent_str = sent_cur.upper()\n",
        "    else:\n",
        "      sent_str = sent_cur\n",
        "    crux_sents_ls.append(sent_str)\n",
        "\n",
        "  # context_ls = novel_df.iloc[avalley-halfwin:avalley+halfwin].text_raw\n",
        "  print(f\"Valley #{i} at Sentence #{avalley}:\\n\\n{nl.join(crux_sents_ls)}\\n\\n\\n\")\n",
        "\"\"\";\n",
        "\n",
        "# Save newly cleaned (Smoothed then Standardized) Ensemble Model data\n",
        "\n",
        "file_fullpath = get_fullpath(ftype='crux_text')\n",
        "# ensemble_df.to_csv(file_fullpath)\n",
        "\n",
        "# print(f'Saved to file: {file_fullpath}')\n",
        "\n",
        "# filename_cruxes = f\"cruxes_context_{Novel_Title[0].replace(' ', '_')}.txt\" \n",
        "\n",
        "with open(file_fullpath, 'w') as f:\n",
        "    f.write(str(cap))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2OSRfUF3el1"
      },
      "source": [
        "# Download Crux Point Report file 'cruxes.txt' to your laptop\n",
        "\n",
        "print(f'Downloading crux text file: {file_fullpath}')\n",
        "files.download(file_fullpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPNOY6adXkiV"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My51TiJcMiBF"
      },
      "source": [
        "# **CONTENT OF NEXT NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcsHB9yUX2q3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK1e6dLkN6Dv"
      },
      "source": [
        "# Smooth, Dimensionally Reduce and Cluster SentimentArcs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXfU_le6KQd2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsTwPtIdOUl2"
      },
      "source": [
        "# Get Correlation Heatmaps of SentimentArcs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7j7XWMiG5yQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kDW1twRG856"
      },
      "source": [
        "# Get SentimentArcs Metrics and Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoAZtW8jG5ui"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxFqZhDQOcW4"
      },
      "source": [
        "# Summarize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QKTy2ZTMjMI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}