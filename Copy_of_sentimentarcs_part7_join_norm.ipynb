{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "name": "Copy of sentimentarcs_part7_join_norm.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "etE1oVtJZpbp",
        "c749tKCSZpbw",
        "LyEr081mZpb1"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "311.997px",
        "left": "719px",
        "top": "111px",
        "width": "416.267px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jon-chun/sentimentarcs/blob/main/Copy_of_sentimentarcs_part7_join_norm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j637s-I4vbg_"
      },
      "source": [
        "# **IMDB Sentiment Analysis with Scikit-Learn**\n",
        "\n",
        "Jon Chun\n",
        "15 Sep 2021\n",
        "\n",
        "* https://colab.research.google.com/github/chengjun/mybook/blob/main/11-4-sentiment-classifier.ipynb#scrollTo=c749tKCSZpbw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_wyG4abZpbK"
      },
      "source": [
        "\n",
        "# 基于机器学习的情感分析\n",
        "\n",
        "\n",
        "![image.png](https://github.com/chengjun/mybook/blob/main/images/author.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUWwlwc0ZpbP"
      },
      "source": [
        "<div><img src=\"https://github.com/chengjun/mybook/blob/main/images/emotion.png?raw=1\" align=\"right\"></div>\n",
        "\n",
        "## Emotion\n",
        "Different types of emotion: anger, disgust, fear, joy, sadness, and surprise. The classification can be performed using different algorithms: e.g., naive Bayes classiﬁer trained on Carlo Strapparava and Alessandro Valitutti’s emotions lexicon.\n",
        "\n",
        "\n",
        "## Polarity\n",
        "\n",
        "To classify some text as positive or negative. In this case, the classification can be done by using a naive Bayes algorithm trained on Janyce Wiebe’s subjectivity lexicon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzn4gNcDZpbQ"
      },
      "source": [
        "![image.png](https://github.com/chengjun/mybook/blob/main/images/tweet.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X27yM9rzZpbS"
      },
      "source": [
        "## Sentiment Analysis with Sklearn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilw75IeETnUU"
      },
      "source": [
        "# **Install and Load Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6StEk2YW1OLY"
      },
      "source": [
        "!pip install texthero"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8nlljoA1P_t"
      },
      "source": [
        "import texthero as hero\n",
        "from texthero import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8ds1sz9kjep"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGfcL2r2kowI"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import io\n",
        "import glob\n",
        "import pickle\n",
        "\n",
        "import re\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McslKTBZZpbU"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8F9Djjr9NA_"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exL5Iu1E3ayR"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer, HashingVectorizer\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR1wMm3mudlN"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler   # To normalize time series\n",
        "from sklearn.preprocessing import StandardScaler # To Standardize time series: center(sub mean) and rescale within 1 SD (only for well-behaved guassian distributions)\n",
        "from sklearn.preprocessing import RobustScaler   # To Standardize time series: center(sub median) and rescale within 25%-75% (1st-3rd) IQR (better for noisy, outliers distributions)\n",
        "\n",
        "minmax_scaler = MinMaxScaler()\n",
        "mean_std_scaler = StandardScaler()\n",
        "median_iqr_scaler = RobustScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKPSE6ZU1bNp"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "porter=PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLpdPoYZswu7"
      },
      "source": [
        "# **Configure Jupyter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CkX9gONAsmp"
      },
      "source": [
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK8zKENjsyig"
      },
      "source": [
        "# Configure Jupyter\n",
        "\n",
        "# Enable multiple outputs from one code cell\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "from IPython.display import display\n",
        "from ipywidgets import widgets, interactive\n",
        "\n",
        "# Configure Google Colab\n",
        "\n",
        "# %load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH1Ad3OxsyqC"
      },
      "source": [
        "# Text wrap\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho0zbRFZTFNe"
      },
      "source": [
        "# Enlarge matplotlib plot size\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 40)\n",
        "\n",
        "# temporarily\n",
        "# from matplotlib.pyplot import figure\n",
        "# figure(figsize=(8, 6), dpi=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d43OmkheT_Vt"
      },
      "source": [
        "# **Connect to gDrive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G64etjAUOOSm"
      },
      "source": [
        "# Connect to Google gDrive\n",
        "\n",
        "# Flag to indicate first run through code \n",
        "flag_first_run = True\n",
        "\n",
        "from google.colab import drive, files\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfpfvAvRUMpW"
      },
      "source": [
        "%cd ./research/2021/sa_book_code/books_sa/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFJtnCYhi_N4"
      },
      "source": [
        "# **Globals**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knAxs8VYU8u5"
      },
      "source": [
        "# List of Corpora\n",
        "\n",
        "corpora_ls = ['cdickens_achristmascarol',\n",
        "              'cdickens_greatexpectations',\n",
        "              'ddefoe_robinsoncrusoe',\n",
        "              'emforster_howardsend',\n",
        "              'fbaum_thewonderfulwizardofoz',\n",
        "              'fdouglass_narrativelifeofaslave',\n",
        "              'fscottfitzgerald_thegreatgatsby',\n",
        "              'geliot_middlemarch',\n",
        "              'hjames_portraitofalady',\n",
        "              'homer-ewilson_odyssey',\n",
        "              'imcewan_machineslikeme',\n",
        "              'jausten_prideandprejudice', # missing RoBERTaXML8lang\n",
        "              'jconrad_heartofdarkness',\n",
        "              'jjoyce_portraitoftheartist',\n",
        "              'jkrowling_1sorcerersstone',  \n",
        "              'mproust-mtreharne_3guermantesway', # missing all Transformers\n",
        "              'mshelley_frankenstein',\n",
        "              'mtwain_huckleberryfinn',\n",
        "              'staugustine_confessions9end',\n",
        "              'tmorrison_beloved',\n",
        "              'vnabokov_palefire',\n",
        "              'vwoolf_mrsdalloway',\n",
        "              'vwoolf_orlando',\n",
        "              'vwoolf_thewaves',\n",
        "              'vwoolf_tothelighthouse']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G-sL4VjU8nA"
      },
      "source": [
        "# Taxonomy of Models \n",
        "\n",
        "groups_ls = ['models_baseline_ls',\n",
        "                'models_sentimentr_ls',\n",
        "                'models_syuzhetr_ls',\n",
        "                'models_transformer_ls']\n",
        "\n",
        "# Could add suffix '_sst2' if classifiers trained on SST2 (currently requires 30m on Colab Pro/GPU+RAM)\n",
        "models_supervised_ls = ['linreg_imdb50k',\n",
        "                   'svc_imdb50k',\n",
        "                   'logreg_imdb50k',\n",
        "                   'dforest_imdb50k',\n",
        "                   'multinb_imdb50k']\n",
        "\n",
        "models_baseline_ls = ['sentimentr',\n",
        "                      'syuzhet',\n",
        "                      'bing',\n",
        "                      'sentiword',\n",
        "                      'senticnet',\n",
        "                      'nrc',\n",
        "                      'afinn',\n",
        "                      'vader',\n",
        "                      'textblob',\n",
        "                      'flair',\n",
        "                      'pattern',\n",
        "                      'stanza']\n",
        "\n",
        "models_sentimentr_ls = ['jockers_rinker',\n",
        "                        'jockers',\n",
        "                        'huliu',\n",
        "                        'senticnet',\n",
        "                        'sentiword',\n",
        "                        'nrc',\n",
        "                        'lmcd']\n",
        "\n",
        "models_syuzhetr_ls = ['syuzhet',\n",
        "                      'bing',\n",
        "                      'afinn',\n",
        "                      'nrc']\n",
        "\n",
        "models_transformer_ls = ['roberta15lg', \n",
        "                         'nlptown', \n",
        "                         'yelp', \n",
        "                         'hinglish',\n",
        "                         'imdb2way', \n",
        "                         'huggingface', \n",
        "                         't5imdb50k', \n",
        "                         'robertaxml8lang']\n",
        "\n",
        "models_ml_ls = ['multinb',\n",
        "             'logreg',\n",
        "             'logreg_cv',\n",
        "             'rf',\n",
        "             'xgb',\n",
        "             'flaml',\n",
        "             'autogluon']\n",
        "             \n",
        "models_dnn_ls = ['fcn',\n",
        "              'lstm',\n",
        "              'cnn']\n",
        "\n",
        "# Temporarily redefine from English to French Transformer Models\n",
        "# models_transformer_ls = ['flaubert', 'nlptown', 'robertaxml8lang']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SSwoDSmjAyd"
      },
      "source": [
        "# Master Dictionary of DataFrames (one per Corpus), each column with raw sentiment polarities from a given model \n",
        "#   declare early to minimize accidental clobbering/deletion\n",
        "\n",
        "corpora_all_dt = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h_KDF-a29ir"
      },
      "source": [
        "# **Custom Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qoLaZ-U2_hz"
      },
      "source": [
        "# https://www.kaggle.com/aditya6040/7-models-on-imdb-dataset-best-score-88-2/notebook\n",
        "\n",
        "def get_metrics(model,x,y):\n",
        "    y_pred = model.predict(x)\n",
        "    acc = accuracy_score(y, y_pred)\n",
        "    f1=f1_score(y, y_pred)\n",
        "    cm=confusion_matrix(y, y_pred)\n",
        "    report=classification_report(y,y_pred)\n",
        "    plt.figure(figsize=(4,4))\n",
        "    sns.heatmap(cm,annot=True,cmap='Blues',xticklabels=[0,1],fmt='d',annot_kws={\"fontsize\":19})\n",
        "    plt.xlabel(\"Predicted\",fontsize=16)\n",
        "    plt.ylabel(\"Actual\",fontsize=16)\n",
        "    plt.show()\n",
        "    print(\"\\nAccuracy: \",round(acc,2))\n",
        "    print(\"\\nF1 Score: \",round(f1,2))\n",
        "#     print(\"\\nConfusion Matrix: \\n\",cm)\n",
        "    print(\"\\nReport:\",report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT3eoNO5To5X"
      },
      "source": [
        "# **Read Every Model Sentiment Data**\n",
        "\n",
        "* https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EidAawQwn861"
      },
      "source": [
        "# Get list of files in data subdir\n",
        "\n",
        "data_dir = './data_corpora_sa'\n",
        "\n",
        "filenames_ls = os.listdir(data_dir)\n",
        "filenames_ls\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTseQdxTILEE"
      },
      "source": [
        "# Read in all models sentiment values in *.csv files located in data_dir\n",
        "#   into Global Dict (corpora_sa_dt) \n",
        "#   with keys=corpora and values=models sentiment values\n",
        "\n",
        "corpora_sa_dt = {}\n",
        "model_group_set = set()\n",
        "\n",
        "def read_csvfiles(folder_path):\n",
        "\n",
        "  for i,afile in enumerate(filenames_ls):\n",
        "    print(f'Reading in afile #{i}: {afile}')\n",
        "    full_path = f'{folder_path}/{afile}'\n",
        "    print(f'  full_path: {full_path}')\n",
        "    model_name = '_'.join(afile.split('_')[1:])\n",
        "    model_name = model_name.split('.')[0]\n",
        "    print(f'  model_name: {model_name}')\n",
        "    corpora_sa_dt[model_name] = pd.read_csv(full_path) # .to_dict()\n",
        "    model_group = model_name.split('_')[0]\n",
        "    print(f'  model_group: {model_group}')\n",
        "    model_group_set.add(model_group)\n",
        "\n",
        "read_csvfiles(data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QLHuLhddpI-"
      },
      "source": [
        "# Show model groups based upon datafile prefix (e.g. 'baseline_' or 'dnn_')\n",
        "\n",
        "print(f'model_group_set:\\n  {model_group_set}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOGOpDEzKw5B"
      },
      "source": [
        "# Verify the number and names of Corpora read\n",
        "\n",
        "[i for i in corpora_sa_dt.keys()]\n",
        "\n",
        "print(f'\\n\\n    Read {len(corpora_sa_dt.keys())} Corpora')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUSWMSxRckWP"
      },
      "source": [
        "corpora_sa_dt['ml_vwoolf_thewaves'].columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfNCyat2IK_u"
      },
      "source": [
        "corpora_sa_dt['baseline_cdickens_achristmascarol'].head(1)\n",
        "corpora_sa_dt['baseline_cdickens_achristmascarol'].info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbNrTHDrH3KT"
      },
      "source": [
        "# Get common corpus roots \n",
        "\n",
        "corpus_root_set = set()\n",
        "\n",
        "# print(f'\\nStarting with {len(filenames_ls)} total corpus x model combinations\\n')\n",
        "for acorpus_model in filenames_ls:\n",
        "  corpus_model_root = '_'.join(acorpus_model.split('_')[2:])\n",
        "  corpus_model_root = corpus_model_root.split('.')[0]\n",
        "  print(f'corpus_model_root: {corpus_model_root}')\n",
        "  corpus_root_set.add(corpus_model_root)\n",
        "\n",
        "corpus_root_ls = list(corpus_root_set)\n",
        "print(f'\\nThese {len(filenames_ls)} original (corpus)x(model) combination files\\n  were reduced to {len(corpus_root_ls)} unique corpus roots')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C28Kt6rfCs9"
      },
      "source": [
        "corpora_sa_dt.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRLHxCvqe3ry"
      },
      "source": [
        "# corpora_sa_dt['fdouglass_narrativelifeofaslave']\n",
        "corpora_sa_dt['ml_fdouglass_narrativelifeofaslave'].info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl0cIoHpgfx0"
      },
      "source": [
        "corpora_all_dt = {}\n",
        "\n",
        "models_dnn_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnskdy8CppY9"
      },
      "source": [
        "# corpora_all_dt.keys()\n",
        "model_dfs_ls = []\n",
        "\n",
        "for i,acorpus in enumerate(corpus_root_ls):\n",
        "  model_dfs_ls = []\n",
        "\n",
        "  # Get all the 'corpus_model' keys for each 'corpus'\n",
        "  for j, agroup in enumerate(model_group_set):\n",
        "    corpus_model = f'{agroup}_{acorpus}'\n",
        "    print(f'\\n\\n{agroup.upper()} Models #{i*len(model_group_set) + j}: {corpus_model}.csv')\n",
        "    model_cols_ls = corpora_sa_dt[corpus_model].columns\n",
        "    print(f'  Cols: {model_cols_ls}')\n",
        "    corpus_model_path = f'{data_dir}/models_{corpus_model}.csv'\n",
        "    # adf = pd.DataFrame()\n",
        "    adf = pd.read_csv(corpus_model_path, index_col=None)\n",
        "    model_dfs_ls.append(adf)\n",
        "  \n",
        "  # Merge the 3 DataFrames (baseline_, ml_, dnn_)\n",
        "  corpus_all_df = model_dfs_ls[0].merge(model_dfs_ls[1], on='sent_no').merge(model_dfs_ls[2], on='sent_no') # pd.concat(model_dfs_ls, axis=0, ignore_index=True)\n",
        "  corpus_all_df = corpus_all_df.loc[:, ~corpus_all_df.columns.str.contains('Unnamed')]\n",
        "  corpus_all_df = corpus_all_df.loc[:, ~corpus_all_df.columns.str.contains('stdscaler')]\n",
        "  corpus_all_df = corpus_all_df.loc[:, ~corpus_all_df.columns.str.contains('scores')]\n",
        "  corpus_all_df = corpus_all_df.loc[:, ~corpus_all_df.columns.str.contains('_len')]\n",
        "  corpus_all_df = corpus_all_df.loc[:, ~corpus_all_df.columns.str.contains('_x')]\n",
        "  corpus_all_df = corpus_all_df.loc[:, ~corpus_all_df.columns.str.contains('_y')]\n",
        "  print('\\n')\n",
        "  corpora_all_dt[acorpus] = corpus_all_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWJMWIVsqegZ"
      },
      "source": [
        "# Check copora and model counts\n",
        "\n",
        "col_nonmodel_ct = 5 # non model columns (e.g. sent_no, parag_no, sect_no, sent_raw, sent_clean)\n",
        "\n",
        "print('SUMMARY ----------')\n",
        "print(f'{len(corpora_all_dt)} Corpora in dataset')\n",
        "print(f\"{len(corpora_all_dt['hjames_portraitofalady'].columns) - col_nonmodel_ct} Models for each Corpus\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GumwGGXkj3C"
      },
      "source": [
        "corpora_sa_dt['ml_hjames_portraitofalady'].head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIlIrHb3j_aI"
      },
      "source": [
        "corpora_all_dt['hjames_portraitofalady'].head(2)\n",
        "# corpora_all_dt['hjames_portraitofalady'].info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTLDEAers6n_"
      },
      "source": [
        "# **[GOTO] Standardize and Smooth**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YItU-oR7juUz"
      },
      "source": [
        "corpora_all_dt.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2kYjvEOPadx"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# Merge all the DataFrames for the same corpus\n",
        "\n",
        "corpus_model_ls = corpora_sa_dt.keys()\n",
        "\n",
        "acorpus_temp_dt = {}\n",
        "\n",
        "# for acorpus_model in filenames_ls:\n",
        "#   # print(f'acorpus_model: {acorpus_model}')\n",
        "  \n",
        "for acorpus in corpus_root_ls:\n",
        "  print(f'test if model substrings: [{acorpus}]') # ' in data filename: {acorpus_model}')\n",
        "\n",
        "  for acorpus_model in filenames_ls:\n",
        "    print(f'  is in data filename: [{acorpus_model}]')\n",
        "    if acorpus in acorpus_model:\n",
        "      print(f'    MATCH!')\n",
        "      # if acorpus in list(acorpus_temp_dt.keys()):\n",
        "      acorpus_temp_dt[acorpus] = acorpus_temp_dt.get(acorpus, [])\n",
        "      acorpus_temp_dt[acorpus].append(acorpus_model) #  = acorpus_temp_dt[acorpus].append(acorpus_model)\n",
        "      # else:\n",
        "      # acorpus_temp_dt[acorpus] = [acorpus_model]\n",
        "    # acorpus_temp_ls.append(acorpus_model)\n",
        "  # print(f'[{acorpus}]:\\n    {[i for i in acorpus_temp_ls]}')\n",
        "\n",
        "# for acorpus in corpus_root_set:\n",
        "#   print(f'acorpus: {acorpus}')\n",
        "\n",
        "# res = [ele for ele in test_list if(ele in test_string)]\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_Pqy70vrCZD"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "corpora_union_dt = {}\n",
        "\n",
        "for acorpus in acorpus_temp_dt.keys():\n",
        "  model_ct = 0\n",
        "  print(f'acorpus: {acorpus}')\n",
        "  for acorpus_model in acorpus_temp_dt[acorpus]:\n",
        "    model_ct +=1\n",
        "    print(f'  {acorpus_model}')\n",
        "    temp_df = pd.read_csv(f'./{data_dir}/{acorpus_model}')\n",
        "    print(f'Concat {acorpus_model}')\n",
        "    if model_ct == 0:\n",
        "      corpora_union_dt[acorpus] = temp_df.copy(deep=True)\n",
        "    else:\n",
        "      temp2_df = corpora_dt[acorpus]\n",
        "      corpora_union_dt[acorpus] = pd.concat([temp2, temp_df], axis=1)\n",
        "    # corpora_union_dt[acorpus] = pd.concat([corpora_sa_dt[acorpus], temp_df], axis=1)\n",
        "    # df_concat = pd.concat([df1, df2], axis=1)\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfnTAe2ETsZq"
      },
      "source": [
        "# Filename string patterns for other Sentiment datafiles\n",
        "\n",
        "# e.g. Transformers: 'sum_sentiments_sents_trans_cdickens_achristmascarol.csv'\n",
        "\n",
        "filename_baseline_sa = 'corpus_sents_baseline_'\n",
        "# filename_sentimentr_sa = 'sum_sentiments_sents_sentimentr_'\n",
        "filename_sentimentr_sa = 'sum_sentiments_sentimentR_7models_'\n",
        "# filename_syuzhetr_sa = 'sum_sentiments_sents_syuzhetr_'\n",
        "filename_syuzhetr_sa = 'sum_sentiments_syuzhetR_4models_'\n",
        "filename_trans_sa = 'sum_sentiments_sents_trans_'\n",
        "\n",
        "filenames_sa_ls = [filename_baseline_sa,filename_sentimentr_sa,filename_syuzhetr_sa,filename_trans_sa]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2jxNgdwU15G"
      },
      "source": [
        "filenames_sa_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W17fTZOLXbVl"
      },
      "source": [
        "import re\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnylrxSMW9DO"
      },
      "source": [
        "# https://stackoverflow.com/questions/39293968/how-do-i-search-directories-and-find-files-that-match-regex\n",
        "\n",
        "def glob_re(path, regex=\"\", glob_mask=\"**/*\", inverse=False):\n",
        "    p = Path(path)\n",
        "    if inverse:\n",
        "        res = [str(f) for f in p.glob(glob_mask) if not re.search(regex, str(f))]\n",
        "    else:\n",
        "        res = [str(f) for f in p.glob(glob_mask) if re.search(regex, str(f))]\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N17sgU8WW_Du"
      },
      "source": [
        "glob_re(path='./cdickens_greatexpectations/', regex='corpus_sents_baseline_*', glob_mask=\"**/*\", inverse=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1lznriPivSR"
      },
      "source": [
        "# Master Dictionary of DataFrames (one per Corpus), each column with raw sentiment polarities from a given model \n",
        "#   declare at top of notebook to minimize accidental clobbering/deletion\n",
        "\n",
        "copora_all_dt = {}\n",
        "\n",
        "for acorpus in corpora_ls:\n",
        "  corpora_all_dt[acorpus] = pd.DataFrame()\n",
        "\n",
        "print(corpora_all_dt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrFTIrtqTsWe"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "corpora_all_dt = {}\n",
        "\n",
        "error_ct = 0\n",
        "corpora_bad_set = set()\n",
        "\n",
        "# For each individual corpus in our collection\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "\n",
        "  filenames_ls = os.listdir(acorpus)\n",
        "  # print(f'In subdir #{i}: {acorpus} there are {len(filenames_ls)} files')  \n",
        "\n",
        "  # Apply each ML Model to compute sentiment values on each sentence\n",
        "  for j, file_pattern in enumerate(filenames_sa_ls):\n",
        "\n",
        "    # filenames_ls = os.listdir(acorpus)\n",
        "    print(f'  Looking for filename starting with: {file_pattern.upper()}')\n",
        "\n",
        "    hit_ls = glob_re(path=f'./{acorpus}/', regex=f'{file_pattern}{acorpus}\\.csv', glob_mask=\"**/*\", inverse=False)\n",
        "\n",
        "    if len(hit_ls) < 1:\n",
        "      print('    >>>ERROR<<<: Missing file')\n",
        "      error_ct += 1\n",
        "      corpora_bad_set.add(acorpus)\n",
        "\n",
        "    elif len(hit_ls) > 1:\n",
        "      print('    >>>ERROR<<<: Exta file(s)')\n",
        "      error_ct += 1\n",
        "      corpora_bad_set.add(acorpus)\n",
        "    else:\n",
        "      # Here we have exactly one (hopefully) correct datafile to read\n",
        "      adatafile = hit_ls[0]\n",
        "      print(f'        Reading: {adatafile}')\n",
        "      temp_df = pd.read_csv(adatafile, encoding='ISO-8859–1')\n",
        "\n",
        "      # Drop unused columns\n",
        "      regex_str = 'roll|lnorm|medianiqr|stdscaler|minmax|Unnamed|\\.[\\d]'\n",
        "      temp_df.drop(temp_df.columns[temp_df.columns.str.contains(regex_str)], axis=1, inplace=True)\n",
        "\n",
        "      # temp_df = temp_df[temp_df.columns.drop(list(df.filter(regex='roll')))]\n",
        "      print(f'          Shape: {temp_df.shape}')\n",
        "      corpus_df = corpora_all_dt.get(acorpus)\n",
        "      # corpus_df == None (if not exists) or is set to DataFrame (if exists in dict corpora_all_dt)\n",
        "      if corpus_df is None:\n",
        "        # This Corpus DataFrame DOES NOT exist, so use the new models's temp_df DataFrame to create it\n",
        "        corpora_all_dt[acorpus] = temp_df.copy(deep=True)\n",
        "        print('            Appending new Model to exiting Corpus DataFrame')\n",
        "      else:\n",
        "        # This Corpus DataFrame exists, so just horizontally concat new model's temp_df\n",
        "        print('            Creating new Corpus DataFrame')\n",
        "        corpora_all_dt[acorpus] = pd.concat([corpora_all_dt[acorpus], temp_df], axis=1)\n",
        "\n",
        "        # Drop duplicate columns\n",
        "        corpora_all_dt[acorpus] = corpora_all_dt[acorpus].loc[:,~corpora_all_dt[acorpus].columns.duplicated()]\n",
        "        # temp_df.drop_duplicates(subset=, inplace=True)\n",
        "        # corpora_all_dt[acorpus] = corpora_all_dt[acorpus].T.drop_duplicates().T\n",
        "        # temp_df.loc[:,~temp_df.T.duplicated(keep='first')]\n",
        "\n",
        "\n",
        "    for ahit in hit_ls:\n",
        "      print(f'    {ahit}')\n",
        "\n",
        "  print('\\n')\n",
        "\n",
        "print(f'{error_ct} ERRORS were found in the {len(corpora_ls)} corpora:\\n\\nCorpus with Errors:\\n  {corpora_bad_set}\\n')\n",
        "\n",
        "\"\"\"\n",
        "rootdir = \"/mnt/externa/Torrents/completed\"\n",
        "regex = re.compile('(.*zip$)|(.*rar$)|(.*r01$)')\n",
        "\n",
        "for root, dirs, files in os.walk(rootdir):\n",
        "  for file in files:\n",
        "    if regex.match(file):\n",
        "       print(file)\n",
        "\n",
        "\n",
        "\n",
        "    # Vectorize Corpus ----------\n",
        "    best_emb_type = best_emb_dt[amodel] \n",
        "\n",
        "    # Select the best embedding technique on IMDB to vectorize Corpus\n",
        "    if best_emb_type == 'count':\n",
        "      # CountVectorizer on Corpus\n",
        "      X_corpus_emb =  count_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf':\n",
        "      # TF-IDF (Words) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf_ngram':\n",
        "      # TF-IDF (ngrams) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect_ngram.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf_ngram_chars':\n",
        "      # TF-IDF (chars) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect_ngram_chars.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'hash':\n",
        "      # TF-IDF (chars) on Corpus\n",
        "      X_corpus_emb =  hash_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    else:\n",
        "      # ERROR\n",
        "      print(f'ERROR: Illegal value for best embedding technique (best_emb_type): {best_emb_type}')\n",
        "\n",
        "    # Predict Sentiments ----------\n",
        "    corpus_ml_dt[acorpus][amodel] =  pd.Series(predict_cls(amodel, X_corpus_emb))\n",
        "\n",
        "    # Standardize and create Simple Moving Average (SMA 10%)\n",
        "    print(f'\\n\\nPlot #{i*len(models_ml_ls) + j}: {acorpus} with model {amodel}')\n",
        "    win10per = int(corpus_ml_dt[acorpus].shape[0]*0.1)\n",
        "    # del temp_df\n",
        "    # temp_df = pd.DataFrame()\n",
        "    # temp_df['zscore'] = zscore(corpus_ml_dt[acorpus][amodel],win10per)\n",
        "    # corpus_ml_dt[acorpus][amodel].rolling(win10per, center=True).mean().plot();\n",
        "    \n",
        "    float_array = corpus_ml_dt[acorpus][amodel].values.astype(float)\n",
        "    amodel_stdscaler = f'{amodel}_stdscaler'\n",
        "    corpus_ml_dt[acorpus][amodel_stdscaler] = mean_std_scaler.fit_transform(float_array.reshape(-1,1))\n",
        "    # temp_df['zscore'].rolling(win10per, center=True).mean().plot()\n",
        "    # zscore(corpus_ml_dt[acorpus][amodel],win10per).rolling(win10per, center=True).mean().plot()\n",
        "    corpus_ml_dt[acorpus][amodel_stdscaler].rolling(win10per, center=True).mean().plot()\n",
        "    plt.title(f'Corpus: {acorpus} with Model: {amodel}\\nz-Score Standardardized SMA 10%');\n",
        "    plt.legend(loc='best')  \n",
        "  plt.show();\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un00sUMVsUFs"
      },
      "source": [
        "type(corpora_all_dt['cdickens_achristmascarol'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zyCb0nUvgtg"
      },
      "source": [
        "corpora_all_dt['fdouglass_narrativelifeofaslave'][['token_len','char_len']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuPkKU9TTsTL"
      },
      "source": [
        "model_ct = 32\n",
        "error_ct = 0\n",
        "error_corpora_ls = []\n",
        "\n",
        "for i,acorpus in enumerate(corpora_ls):\n",
        "  print(f'\\nCORPUS: {acorpus}\\n---------------------------\\n')\n",
        "  # print(f'{corpora_all_dt[acorpus].columns}\\n')\n",
        "  # print('\\n')\n",
        "  col_ct = len(corpora_all_dt[acorpus].columns)\n",
        "  if col_ct != model_ct:\n",
        "    error_ct += 1\n",
        "    error_corpora_ls.append(acorpus)\n",
        "    print(f'{corpora_all_dt[acorpus].columns}\\n')\n",
        "    print(f'>>>ERROR<<< Illegal Column Count: {col_ct}')\n",
        "  else:\n",
        "    print(f'                    Column Count: {col_ct}')\n",
        "\n",
        "print('\\n------------------------------------------------------\\n')\n",
        "if error_ct != 0:\n",
        "  print(f'{error_ct} Errors were detected in these Corpora:\\n   {error_corpora_ls}')\n",
        "else:\n",
        "  print(f'No errors detected in the column counts: {model_ct} for any Corpus')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86M8Auu51B-2"
      },
      "source": [
        "corpora_all_dt['cdickens_achristmascarol'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyXm9xWA1dUj"
      },
      "source": [
        "models_baseline_ls + models_sentimentr_ls + models_syuzhetr_ls + models_transformer_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgwueG8_z6zJ"
      },
      "source": [
        "# Save each individual corpus in our collection\n",
        "\n",
        "models_base_ct = len(models_baseline_ls + models_sentimentr_ls + models_syuzhetr_ls + models_transformer_ls)\n",
        "\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "\n",
        "  data_dir = 'data_corpora_sa'\n",
        "  model_file = f'./{data_dir}/models_all_{acorpus}.csv'\n",
        "  print(f'\\n\\nDataFrame #{i*models_base_ct + j}: [{acorpus}]\\n')\n",
        "  # corpora_all_dt[acorpus].head()\n",
        "  corpora_all_dt[acorpus].columns\n",
        "\n",
        "  print(f'\\n    Saving contents to file: [{model_file}]\\n\\n')\n",
        "  corpora_all_dt[acorpus].to_csv(model_file)\n",
        "\n",
        "print(f'\\n\\nEach Corpus has Sentiment Values computed from {models_base_ct} core models: (12)Baseline, (7)SentimentR, (4)SyuzhetR and (8)Transformer\\n\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYJMygS1265e"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axnUdaPIsYNI"
      },
      "source": [
        "type(corpora_all_dt['cdickens_greatexpectations'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CT3XqOrTsPN"
      },
      "source": [
        "corpora_all_dt['cdickens_greatexpectations'].columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO8XzeaFTsLj"
      },
      "source": [
        "corpus_df = corpora_all_dt.get('cdickens_achristmascarol')\n",
        "corpus_df = corpora_all_dt.get('cdickens_greatexpectations')\n",
        "type(corpus_df)\n",
        "print(corpus_df == None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejk5yGmrTsHg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPvDpZPltVdD"
      },
      "source": [
        "# **Standardize and Smooth**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw-8Pr1SEZ1H"
      },
      "source": [
        "## **Option (a): Read in zScore/SMA 10% DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uSPn44UtX6l"
      },
      "source": [
        "# Save all models with orignal and zscore values\n",
        "\n",
        "subdir_all = 'data_corpora_all'\n",
        "\n",
        "corpora_temp_dt = {}\n",
        "\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "\n",
        "  filepath = f'./{subdir_all}/models_all_{acorpus}.csv'\n",
        "\n",
        "  print(f'Reading {acorpus} from:\\n    {filepath}\\n')\n",
        "\n",
        "  corpora_temp_dt[acorpus] = pd.read_csv(filepath, index_col=[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTf-S3I3Ghem"
      },
      "source": [
        "corpora_temp_dt['cdickens_achristmascarol'].columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkGeLuCwEfty"
      },
      "source": [
        "## **Option (b): Generate zScore/SMA 10% DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NosICvotX3B"
      },
      "source": [
        "models_ls = corpora_all_dt.keys()\n",
        "print(models_ls)\n",
        "print(f'There are {len(models_ls)} Corpora')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tRkCLrjtXzd"
      },
      "source": [
        "model_all_cols_ls = corpora_all_dt['cdickens_achristmascarol'].columns\n",
        "model_all_cols_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4MadSUvuCGH"
      },
      "source": [
        "model_noncols_ls = ['sent_no', 'parag_no', 'sect_no', 'sent_raw', 'sent_clean']\n",
        "\n",
        "model_cols_ls = list(set(model_all_cols_ls) - set(model_noncols_ls))\n",
        "print(f'{len(model_cols_ls)} Model column names:\\n\\n  {[i for i in model_cols_ls]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYGwlXO8vhHZ"
      },
      "source": [
        "corpora_all_dt.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMUbr-euFOoN"
      },
      "source": [
        "# adf.astype(np.float64)\n",
        "\n",
        "corpora_all_dt['cdickens_achristmascarol'].info()\n",
        "# corpora_all_dt[acorpus][amodel].astype(np.float64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4KxYFAlG7O0"
      },
      "source": [
        "corpora_all_dt['cdickens_achristmascarol']['xgb'] = corpora_all_dt['cdickens_achristmascarol']['xgb'].astype('float')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E1KgrekHx9q"
      },
      "source": [
        "corpora_all_dt['cdickens_achristmascarol'].info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yulk0H-IoaT"
      },
      "source": [
        "zscore = lambda x: (x - x.mean()) / x.std()\n",
        "# tmp.rolling(5).apply(zscore)\n",
        "\n",
        "def zscore_func(x):\n",
        "    return (x[-1] - x[:-1].mean())/x[:-1].std(ddof=0)\n",
        "# df.rolling(window=3).apply(zscore_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uigsoBWaKa-q"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "stdscaler = StandardScaler()\n",
        "\n",
        "# fit and transform the data\n",
        "# scaled_data = scaler.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKtMMpBINLc6"
      },
      "source": [
        "type(stdscaler.fit_transform(np.asarray(corpora_all_dt['cdickens_achristmascarol']['xgb']).reshape(1,-1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhKDeen6Pb1n"
      },
      "source": [
        "# corpora_all_dt['cdickens_achristmascarol']['xgb_stdscaler'] = stdscaler.fit_transform(np.asarray(corpora_all_dt['cdickens_achristmascarol']['xgb']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW4dy2AzNdFk"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "temp_df = pd.DataFrame()\n",
        "\n",
        "# df[cols] = StandardScaler.fit_transform(df[cols])\n",
        "\n",
        "# temp_df['test'] = stdscaler.fit_transform(np.asarray(corpora_all_dt['cdickens_achristmascarol']['xgb']).reshape(1,-1)).ravel()\n",
        "# corpora_all_dt['cdickens_achristmascarol']['xgb'].rolling(140, center=True).mean().plot()\n",
        "\n",
        "temp_ser = corpora_all_dt['cdickens_achristmascarol']['vader']\n",
        "temp_std_ser = stdscaler.fit_transform(np.asarray(temp_ser).reshape(1,-1)).ravel()\n",
        "# pd.Series(temp_std_ser)\n",
        "temp_df['test'] = pd.Series(temp_std_ser)\n",
        "temp_df['test'].rolling(140, center=True).mean().plot()\n",
        "# corpora_all_dt['cdickens_achristmascarol']['xgb'].rolling(140,center=True).mean().plot()\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JlbXeI5R0zc"
      },
      "source": [
        "from scipy.stats import zscore\n",
        "\n",
        "temp_df = pd.DataFrame()\n",
        "\n",
        "# stats.zscore(a)\n",
        "temp_np = np.asarray(corpora_all_dt['cdickens_achristmascarol']['vader'])\n",
        "temp_z_np = zscore(temp_np)\n",
        "\n",
        "# temp_std_ser = stdscaler.fit_transform(np.asarray(temp_ser).reshape(1,-1)).ravel()\n",
        "# pd.Series(temp_std_ser)\n",
        "temp_df['test'] = pd.Series(temp_z_np)\n",
        "temp_df['test'].rolling(140, center=True).mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WikuAp_UV7D0"
      },
      "source": [
        "# Compute z-scores for all Model Time Series\n",
        "\n",
        "temp_df = pd.DataFrame()\n",
        "\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "  print(f'Processing model: {acorpus}')\n",
        "  for j, amodel in enumerate(model_cols_ls):\n",
        "    # print(f'Corpus #{i} {acorpus} with Model #{j} {amodel}: has {len(corpora_all_dt[acorpus].columns)} columns but only {len(model_cols_ls)} model columns')\n",
        "\n",
        "    temp_np = np.asarray(corpora_all_dt[acorpus][amodel])\n",
        "    temp_z_np = zscore(temp_np)\n",
        "\n",
        "    amodel_z = f'{amodel}_z'\n",
        "    corpora_all_dt[acorpus][amodel_z] = pd.Series(temp_z_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFsmuZhRW7P1"
      },
      "source": [
        "corpora_all_dt['cdickens_achristmascarol']['median_z'] = corpora_all_dt['cdickens_achristmascarol'].loc[:, corpora_all_dt['cdickens_achristmascarol'].columns.str.contains('_z')].median()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYde92c4Wgi9"
      },
      "source": [
        "# Compute the median of z-Score of all Models for each Corpus\n",
        "\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "  print(f'Processing model: {acorpus}')\n",
        "  corpora_all_dt[acorpus]['median_z'] = corpora_all_dt[acorpus].loc[:, corpora_all_dt[acorpus].columns.str.contains('_z')].median()\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3HOkX14uvAj"
      },
      "source": [
        "temp_df = pd.DataFrame()\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (40,20)\n",
        "\n",
        "for i, acorpus in enumerate(corpora_ls): # [:2]):\n",
        "  print(acorpus)\n",
        "  for j, amodel in enumerate(model_cols_ls):\n",
        "\n",
        "    win10per = int(0.1*corpora_all_dt[acorpus]['median_z'].shape[0])\n",
        "\n",
        "    if j == 0:\n",
        "      corpora_all_dt[acorpus]['median_z'].rolling(win10per, center=True).mean().plot(label='z-Score Median', style=['r*-'], linewidth=2.0)\n",
        "\n",
        "    print(f'Corpus #{i} {acorpus} with Model #{j} {amodel}: has {len(corpora_all_dt[acorpus].columns)} columns but only {len(model_cols_ls)} model columns')\n",
        "    win10per = int(0.10*corpora_all_dt[acorpus].shape[0])\n",
        "\n",
        "    temp_np = np.asarray(corpora_all_dt[acorpus][amodel])\n",
        "    temp_z_np = zscore(temp_np)\n",
        "\n",
        "    # temp_std_ser = stdscaler.fit_transform(np.asarray(temp_ser).reshape(1,-1)).ravel()\n",
        "    # pd.Series(temp_std_ser)\n",
        "    amodel_z = f'{amodel}_z'\n",
        "    corpora_all_dt[acorpus][amodel_z] = pd.Series(temp_z_np)\n",
        "    corpora_all_dt[acorpus][amodel_z].rolling(win10per, center=True).mean().plot(label=amodel_z, alpha=0.3)\n",
        "\n",
        "    # corpora_all_dt[acorpus][amodel].astype(np.float64)\n",
        "    # temp_df[amodel] = corpora_all_dt[acorpus][amodel].value.rolling(win10per, min_periods=1,center=True).apply(zscore)\n",
        "    \n",
        "    # temp_df[amodel] = pd.Series(type(stdscaler.fit_transform(np.asarray(corpora_all_dt[acorpus][amodel]).reshape(-1,1)))) # .reshape(-1,1))\n",
        "    # print(f'temp_df[amodel]: {temp_df[amodel]}')\n",
        "    # print(f'len(temp_df[amodel]: {len(temp_df[amodel])}')\n",
        "    # temp_df[amodel].rolling(win10per, min_periods=1,center=True).mean().plot() # .mean().plot(label=amodel)\n",
        "\n",
        "    # plt.title(f'{acorpus} Simple Moving Average')\n",
        "    # plt.legend(loc='best') \n",
        "\n",
        "  # if i == (len(corpora_ls)):\n",
        "  #   corpora_all_dt[amodel]['median_z'].rolling(win10per, center=True).mean().plot(alpha=0.6, color='black', width=2)\n",
        "  #   plt.title(f'{acorpus} Simple Moving Average')\n",
        "  #  plt.legend(loc='best')\n",
        "  plt.title(f'{acorpus}\\n SMA 10%')    \n",
        "  plt.legend(loc='best')\n",
        "  corpora_all_dt[acorpus]['median_z'].rolling(win10per, center=True).mean().plot(label='z-Score Median', style=['r*-'], linewidth=2.0)\n",
        "\n",
        "  filename_plt = f'./{subdir_name}/plt_sma10_{amodel}.png'\n",
        "  plt.savefig(filename_plt)\n",
        "\n",
        "  plt.show();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYxAIGbm7uzE"
      },
      "source": [
        "# Verify columns headers/models\n",
        "\n",
        "acorpus = 'cdickens_achristmascarol'\n",
        "\n",
        "len(corpora_all_dt[acorpus].filter(regex='_z').columns)\n",
        "print('\\n')\n",
        "corpora_all_dt[acorpus].filter(regex='_z$').columns # columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ8M0zj6FCRC"
      },
      "source": [
        "## **Save zScore/SMA 10% DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY9g47jj7i8O"
      },
      "source": [
        "# Save all models with orignal and zscore values\n",
        "\n",
        "\n",
        "subdir_out = 'data_corpora_all'\n",
        "\n",
        "temp_df = pd.DataFrame()\n",
        "\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "  print(acorpus)\n",
        "  # acorpus_sent_len = corpora_all_dt[acorpus]['xgb'].shape[0]\n",
        "\n",
        "  filename_out = f'models_all_{acorpus}.csv'\n",
        "  fullpath_out = f'./{subdir_out}/{filename_out}'\n",
        "\n",
        "  print(f'\\nSaving Corpus: {acorpus}...')\n",
        "  corpora_all_dt[acorpus].to_csv(fullpath_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2uyFWyLbt7_"
      },
      "source": [
        "# **Downsample with LTTB**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0iyVTICEwTi"
      },
      "source": [
        "## **Option (a): Read in SMA 10%/LTTB DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiSG9vASE3Ip"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ysO-EHFE4A3"
      },
      "source": [
        "## **Option (b): Generate SMA 10%/LTTB DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "059Ty0n6btrn"
      },
      "source": [
        "!pip install lttb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwL2wKM9cnre"
      },
      "source": [
        "import lttb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7P_EV7CckTB"
      },
      "source": [
        "# Generate an example data set of 100 random points:\n",
        "#  - column 0 represents time values (strictly increasing)\n",
        "#  - column 1 represents the metric of interest: CPU usage, stock price, etc.\n",
        "\n",
        "data = np.array([range(100), np.random.random(100)])\n",
        "plt.plot(data[0],data[1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St8VkqAPckEQ"
      },
      "source": [
        "# Downsample it to 20 points:\n",
        "\n",
        "small_data = lttb.downsample(data.T, n_out=20)\n",
        "assert small_data.shape == (20, 2)\n",
        "\n",
        "plt.plot(small_data.T[0],small_data.T[1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEjycZNecj_4"
      },
      "source": [
        "acorpus = 'cdickens_achristmascarol'\n",
        "\n",
        "print(f'Corpus: {acorpus} has {corpora_all_dt[acorpus].shape[0]} time values')\n",
        "\n",
        "win10per = int(0.1*corpora_all_dt[acorpus].shape[0])\n",
        "corpora_all_dt[acorpus]['vader_z'].rolling(win10per, center=True, min_periods=1).mean().plot(label='z-Score Median', style=['r*-'], linewidth=2.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67HtX5mefOeC"
      },
      "source": [
        "acorpus = 'cdickens_achristmascarol'\n",
        "model_z = 'nrc_z'\n",
        "\n",
        "type(corpora_all_dt[acorpus][model_z].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APkuuQpre5e4"
      },
      "source": [
        "# Downsample to n_pts points:\n",
        "\n",
        "n_pts = 50\n",
        "\n",
        "x_np = corpora_all_dt[acorpus]['vader_z'].shape[0]\n",
        "\n",
        "# Generate an example data set of 100 random points:\n",
        "#  - column 0 represents time values (strictly increasing)\n",
        "#  - column 1 represents the metric of interest: CPU usage, stock price, etc.\n",
        "\n",
        "y_np = corpora_all_dt[acorpus]['vader_z'].rolling(win10per, center=True, min_periods=1).mean().values\n",
        "data = np.array([range(x_np), y_np]).T # np.random.random(100)]).T\n",
        "\n",
        "# Downsample it to 20 points:\n",
        "small_data = lttb.downsample(data, n_out=n_pts)\n",
        "assert small_data.shape == (n_pts, 2)\n",
        "\n",
        "# temp_np = corpora_all_dt[acorpus]['vader_z'].values.reshape(1,-1) # ).ravel()\n",
        "# print(f'Shape temp_np: {temp_np.shape}')\n",
        "# small_data = lttb.downsample(temp_np, n_out=20)\n",
        "# assert small_data.shape == (20, 2)\n",
        "\n",
        "plt.plot(small_data.T[0],small_data.T[1])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IHlfu-Th0nB"
      },
      "source": [
        "n_pts = 25\n",
        "\n",
        "x_np = corpora_all_dt[acorpus]['vader_z'].shape[0]\n",
        "\n",
        "# Generate an example data set of 100 random points:\n",
        "#  - column 0 represents time values (strictly increasing)\n",
        "#  - column 1 represents the metric of interest: CPU usage, stock price, etc.\n",
        "\n",
        "y_np = corpora_all_dt[acorpus]['vader_z'].rolling(win10per, center=True, min_periods=1).mean().values\n",
        "data = np.array([range(x_np), y_np]).T # np.random.random(100)]).T\n",
        "\n",
        "# Downsample it to 20 points:\n",
        "small_data = lttb.downsample(data, n_out=n_pts)\n",
        "assert small_data.shape == (n_pts, 2)\n",
        "print(f'type small_data: {type(small_data)}')\n",
        "print(f'small_data.shape: {small_data.shape}')\n",
        "\n",
        "# temp_np = corpora_all_dt[acorpus]['vader_z'].values.reshape(1,-1) # ).ravel()\n",
        "# print(f'Shape temp_np: {temp_np.shape}')\n",
        "# small_data = lttb.downsample(temp_np, n_out=20)\n",
        "# assert small_data.shape == (20, 2)\n",
        "\n",
        "plt.plot(small_data.T[0],small_data.T[1])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOY3ek-PCF-k"
      },
      "source": [
        "# Downsample all z-Score Model Series to n_pts with LTTB Algo\n",
        "\n",
        "lttb_pts = 50\n",
        "\n",
        "subdir_name = 'data_corpora_plots'\n",
        "\n",
        "corpora_all50_dt = {}\n",
        "\n",
        "for acorpus in corpora_ls:\n",
        "  corpora_all50_dt[acorpus] = pd.DataFrame()\n",
        "\n",
        "temp_df = pd.DataFrame()\n",
        "\n",
        "for i, acorpus in enumerate(corpora_ls): # [:2]):\n",
        "  print(acorpus)\n",
        "  # acorpus_sent_len = corpora_all_dt[acorpus]['xgb'].shape[0]\n",
        "\n",
        "  for j, amodel in enumerate(model_cols_ls):\n",
        "    if j == 0:\n",
        "      corpora_all_dt[acorpus]['median_z'].rolling(win10per, center=True).mean().plot(label='z-Score Median', style=['r*-'], linewidth=2.0)\n",
        "\n",
        "    print(f'Corpus #{i} {acorpus} with Model #{j} {amodel}: has {len(corpora_all_dt[acorpus].columns)} columns but only {len(model_cols_ls)} model columns')\n",
        "\n",
        "    amodel_z = f'{amodel}_z'\n",
        "\n",
        "    x_np = corpora_all_dt[acorpus][amodel_z].shape[0]\n",
        "    y_np = corpora_all_dt[acorpus][amodel_z].rolling(win10per, center=True, min_periods=1).mean().values\n",
        "    data = np.array([range(x_np), y_np]).T\n",
        "\n",
        "    # Downsample it to lttb_pts points:\n",
        "    small_data = lttb.downsample(data, n_out=lttb_pts)\n",
        "    assert small_data.shape == (lttb_pts, 2)\n",
        "\n",
        "    corpora_all50_dt[acorpus][amodel] = pd.Series(small_data.tolist()) # small_data.T[0], small_data.T[1])\n",
        "\n",
        "    # temp_np = corpora_all_dt[acorpus]['vader_z'].values.reshape(1,-1) # ).ravel()\n",
        "    # print(f'Shape temp_np: {temp_np.shape}')\n",
        "    # small_data = lttb.downsample(temp_np, n_out=20)\n",
        "    # assert small_data.shape == (20, 2)\n",
        "\n",
        "    plt.plot(small_data.T[0],small_data.T[1], label=amodel)\n",
        "\n",
        "  plt.title(f'{acorpus}\\n SMA 10% Followed by LTTB Downsampled to {lttb_pts} Points')\n",
        "  plt.legend(loc='best') \n",
        "  filename_plt = f'./{subdir_name}/plt_lttb{lttb_pts}_{amodel}.png'\n",
        "  plt.savefig(filename_plt)\n",
        "  plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5Dtwn9QD0KP"
      },
      "source": [
        "corpora_all50_dt['cdickens_achristmascarol']['vader'][:5]\n",
        "type(corpora_all50_dt['cdickens_achristmascarol']['vader'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO8-1qjN_zs-"
      },
      "source": [
        "# Save all models with orignal and zscore values\n",
        "\n",
        "\n",
        "subdir_out = 'data_corpora_all'\n",
        "\n",
        "temp_df = pd.DataFrame()\n",
        "\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "  print(acorpus)\n",
        "  # acorpus_sent_len = corpora_all_dt[acorpus]['xgb'].shape[0]\n",
        "\n",
        "  filename_out = f'models_all50_{acorpus}.csv'\n",
        "  fullpath_out = f'./{subdir_out}/{filename_out}'\n",
        "\n",
        "  print(f'\\nSaving Corpus: {acorpus}...')\n",
        "  corpora_all50_dt[acorpus].to_csv(fullpath_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUnEPAU_FJ8l"
      },
      "source": [
        "## **Save SMA 10%/LTTB DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9g1eqwzFJrX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z02wV6y6_w0C"
      },
      "source": [
        "# **LOWESS Smoothing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGuuDFcb3LNz"
      },
      "source": [
        "# from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "\n",
        "import statsmodels.api as sm\n",
        "lowess = sm.nonparametric.lowess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAMP4J6bEc26"
      },
      "source": [
        "# Downsample all z-Score Model Series to n_pts with LTTB Algo\n",
        "\n",
        "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "\n",
        "afrac = 0.1 \n",
        "\n",
        "def make_lowess(series, frac=0.1):\n",
        "    endog = series.values\n",
        "    exog = series.index.values\n",
        "\n",
        "    smooth = lowess(endog, exog, frac)\n",
        "    index, data = np.transpose(smooth)\n",
        "\n",
        "    return pd.Series(data, index=pd.to_datetime(index)) \n",
        "\n",
        "for acorpus in corpora_ls:\n",
        "  corpora_all50_dt[acorpus] = pd.DataFrame()\n",
        "\n",
        "temp_df = pd.DataFrame()\n",
        "\n",
        "for i, acorpus in enumerate(corpora_ls[:2]):\n",
        "  print(acorpus)\n",
        "  # acorpus_sent_len = corpora_all_dt[acorpus]['xgb'].shape[0]\n",
        "\n",
        "  for j, amodel in enumerate(model_cols_ls):\n",
        "    if j == 0:\n",
        "      corpora_all_dt[acorpus]['median_z'].rolling(win10per, center=True).mean().plot(label='z-Score Median', style=['r*-'], linewidth=2.0)\n",
        "\n",
        "    print(f'Corpus #{i} {acorpus} with Model #{j} {amodel}: has {len(corpora_all_dt[acorpus].columns)} columns but only {len(model_cols_ls)} model columns')\n",
        "\n",
        "    amodel_lowess = f\"{amodel}_lowess{''.join(str(afrac).split('.'))}\"\n",
        "    print(f'amodel_lowess: {amodel_lowess}')\n",
        "\n",
        "    temp_np = make_lowess(corpora_all_dt[acorpus][amodel], frac=afrac)\n",
        "    # plt.plot(temp_np[0], temp_np[1], label=amodel)\n",
        "\n",
        "    corpora_all_dt[acorpus][amodel_lowess] = pd.Series(temp_np.tolist()) # small_data.T[0], small_data.T[1])\n",
        "\n",
        "\n",
        "\n",
        "    # temp_np = corpora_all_dt[acorpus]['vader_z'].values.reshape(1,-1) # ).ravel()\n",
        "    # print(f'Shape temp_np: {temp_np.shape}')\n",
        "    # small_data = lttb.downsample(temp_np, n_out=20)\n",
        "    # assert small_data.shape == (20, 2)\n",
        "\n",
        "    # plt.plot(small_data.T[0],small_data.T[1], label=amodel)\n",
        "\n",
        "  plt.title(f'{acorpus} LTTB Downsampled SMA 10%')\n",
        "  plt.legend(loc='best') \n",
        "  plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OceHxv73Zbi"
      },
      "source": [
        "x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)\n",
        "y = np.sin(x) + np.random.normal(size=len(x))\n",
        "\n",
        "z = lowess(y, x)\n",
        "w = lowess(y, x, frac=1./3)\n",
        "\n",
        "# plt.plot(w)\n",
        "\n",
        "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "\n",
        "def make_lowess(series, frac=0.1):\n",
        "    endog = series.values\n",
        "    exog = series.index.values\n",
        "\n",
        "    smooth = lowess(endog, exog, frac)\n",
        "    index, data = np.transpose(smooth)\n",
        "\n",
        "    return pd.Series(data, index=pd.to_datetime(index)) \n",
        "\n",
        "temp_df = pd.DataFrame()\n",
        "temp_df['test'] = make_lowess(corpora_all_dt['cdickens_achristmascarol']['vader_z'], frac=0.1)\n",
        "\n",
        "temp_df['test'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnR-Hmjk7Okb"
      },
      "source": [
        "temp_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pk8LZyRPdmTh"
      },
      "source": [
        "# Downsample all z-Score Model Series to n_pts with LTTB Algo\n",
        "\n",
        "n_pts = 100\n",
        "\n",
        "temp_df = pd.DataFrame()\n",
        "\n",
        "for i, acorpus in enumerate(corpora_ls[:2]):\n",
        "  print(acorpus)\n",
        "  # acorpus_sent_len = corpora_all_dt[acorpus]['xgb'].shape[0]\n",
        "\n",
        "  for j, amodel in enumerate(model_cols_ls):\n",
        "    if j == 0:\n",
        "      corpora_all_dt[acorpus]['median_z'].rolling(win10per, center=True).mean().plot(label='z-Score Median', style=['r*-'], linewidth=2.0)\n",
        "\n",
        "    print(f'Corpus #{i} {acorpus} with Model #{j} {amodel}: has {len(corpora_all_dt[acorpus].columns)} columns but only {len(model_cols_ls)} model columns')\n",
        "\n",
        "    amodel_z = f'{amodel}_z'\n",
        "    \n",
        "    x_np = corpora_all_dt[acorpus][amodel_z].shape[0]\n",
        "    y_np = corpora_all_dt[acorpus][amodel_z].rolling(win10per, center=True, min_periods=1).mean().values\n",
        "    data = np.array([range(x_np), y_np]).T\n",
        "\n",
        "    # Downsample it to n_pts points:\n",
        "    small_data = lttb.downsample(data, n_out=n_pts)\n",
        "    assert small_data.shape == (n_pts, 2)\n",
        "\n",
        "    # temp_np = corpora_all_dt[acorpus]['vader_z'].values.reshape(1,-1) # ).ravel()\n",
        "    # print(f'Shape temp_np: {temp_np.shape}')\n",
        "    # small_data = lttb.downsample(temp_np, n_out=20)\n",
        "    # assert small_data.shape == (20, 2)\n",
        "\n",
        "    plt.plot(small_data.T[0],small_data.T[1], label=amodel)\n",
        "\n",
        "  plt.title(f'{acorpus} LTTB Downsampled SMA 10%')\n",
        "  plt.legend(loc='best') \n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    win10per = int(0.10*corpora_all_dt[acorpus].shape[0])\n",
        "\n",
        "    temp_np = np.asarray(corpora_all_dt[acorpus][amodel])\n",
        "    temp_z_np = zscore(temp_np)\n",
        "\n",
        "    # temp_std_ser = stdscaler.fit_transform(np.asarray(temp_ser).reshape(1,-1)).ravel()\n",
        "    # pd.Series(temp_std_ser)\n",
        "    amodel_z = f'{amodel}_z'\n",
        "    corpora_all_dt[acorpus][amodel_z] = pd.Series(temp_z_np)\n",
        "    corpora_all_dt[acorpus][amodel_z].rolling(win10per, center=True).mean().plot(label=amodel_z, alpha=0.3)\n",
        "\n",
        "    # corpora_all_dt[acorpus][amodel].astype(np.float64)\n",
        "    # temp_df[amodel] = corpora_all_dt[acorpus][amodel].value.rolling(win10per, min_periods=1,center=True).apply(zscore)\n",
        "    \n",
        "    # temp_df[amodel] = pd.Series(type(stdscaler.fit_transform(np.asarray(corpora_all_dt[acorpus][amodel]).reshape(-1,1)))) # .reshape(-1,1))\n",
        "    # print(f'temp_df[amodel]: {temp_df[amodel]}')\n",
        "    # print(f'len(temp_df[amodel]: {len(temp_df[amodel])}')\n",
        "    # temp_df[amodel].rolling(win10per, min_periods=1,center=True).mean().plot() # .mean().plot(label=amodel)\n",
        "\n",
        "    # plt.title(f'{acorpus} Simple Moving Average')\n",
        "    # plt.legend(loc='best') \n",
        "\n",
        "  # if i == (len(corpora_ls)):\n",
        "  #   corpora_all_dt[amodel]['median_z'].rolling(win10per, center=True).mean().plot(alpha=0.6, color='black', width=2)\n",
        "  #   plt.title(f'{acorpus} Simple Moving Average')\n",
        "  #  plt.legend(loc='best')\n",
        "  plt.title(f'{acorpus} Simple Moving Average')    \n",
        "  plt.legend(loc='best')\n",
        "  corpora_all_dt[acorpus]['median_z'].rolling(win10per, center=True).mean().plot(label='z-Score Median', style=['r*-'], linewidth=2.0)\n",
        "  plt.show();\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx_98LiHdmPe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF4_XIt2bxUj"
      },
      "source": [
        "# **Regularize**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGAyLsV2btn2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYRtM17Pb11c"
      },
      "source": [
        "# **Cluster**\n",
        "\n",
        "* DTW w/Hierachical Clustering https://github.com/wannesm/dtaidistance\n",
        "* https://github.com/markdregan/K-Nearest-Neighbors-with-Dynamic-Time-Warping\n",
        "* https://github.com/timeseriesAI/tsai\n",
        "* https://github.com/tslearn-team/tslearn\n",
        "* sklearn for time\n",
        "* https://stats.stackexchange.com/questions/131281/dynamic-time-warping-clustering\n",
        "* https://stats.stackexchange.com/questions/109343/dynamic-time-warping-for-irregular-time-series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shTeMM6abtjy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYQ3UterjMJF"
      },
      "source": [
        "# **Get Corpora**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6BTm4_mk-M7"
      },
      "source": [
        "corpora_ls[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1uV3CSqk8TB"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6Fts10_lF6C"
      },
      "source": [
        "corpus_name = corpora_ls[0]\n",
        "\n",
        "!ls -altr $corpus_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s42B_qHLlYwd"
      },
      "source": [
        "temp_df = pd.read_csv(f'./{corpus_name}/corpus_text_sents_clean_{corpus_name}.csv')\n",
        "temp_df.rename(columns={'Unnamed: 0':'sent_no'}, inplace=True)\n",
        "temp_df['sent_clean'] = temp_df['sent_clean'].astype('string')\n",
        "temp_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtplBhyWk6z0"
      },
      "source": [
        "temp_df = f'./{corpora_ls[0]}/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0l-WY269us0"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ApuWfaAkSTv"
      },
      "source": [
        "# Get a Dictionary of clean text for every item in the corpora (listed in corpora_ls)\n",
        "\n",
        "corpora_dt = {}\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "  print(f'\\nREADING #{i}: {acorpus}\\n========================================\\n')\n",
        "  # os.listdir(adir)\n",
        "  corpus_csv = f'./{acorpus}/corpus_text_sents_clean_{acorpus}.csv'\n",
        "  print(f'{os.getcwd()}\\n     {corpus_csv}')\n",
        "  temp_df = pd.DataFrame()\n",
        "  temp_df = pd.read_csv(corpus_csv, encoding='ISO-8859–1')\n",
        "  temp_df.rename(columns={'Unnamed: 0':'sent_no'}, inplace=True)\n",
        "  temp_df['sent_clean'] = temp_df['sent_clean'].astype('string')\n",
        "  corpora_dt[acorpus] = temp_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3BgRRT5NBiu"
      },
      "source": [
        "corpora_dt['cdickens_achristmascarol'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd2F-Onkm5n8"
      },
      "source": [
        "len(corpora_dt.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCtS7awhkR4S"
      },
      "source": [
        "[f'{corpus_name}' for corpus_name in corpora_dt.keys()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auUsC0WWod8N"
      },
      "source": [
        "corpora_dt['cdickens_achristmascarol'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLtU2fu9sYvs"
      },
      "source": [
        "corpora_dt['cdickens_achristmascarol'].info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESGThFMlojv0"
      },
      "source": [
        "corpora_dt['cdickens_achristmascarol'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS8w2uYfTiVW"
      },
      "source": [
        "# **IMDB Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zLlsxHvslin"
      },
      "source": [
        "imdb_clean_filename = 'imdb_clean.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olLKIGAs9eBe"
      },
      "source": [
        "## **Option (a): Read previously cleaned IMDB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5vwnUuX-c0p"
      },
      "source": [
        "training_df = pd.read_csv(imdb_clean_filename)\n",
        "training_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPsKuo5u5w97"
      },
      "source": [
        "training_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5rI03285a8b"
      },
      "source": [
        "training_df = pd.read_csv(imdb_clean_filename)\n",
        "\n",
        "training_df.rename(columns={'Unnamed: 0':'sent_no'}, inplace=True)\n",
        "training_df.drop(columns=['Unnamed: 0.1'], inplace=True)\n",
        "\n",
        "training_df['text_raw'] = training_df['text_raw'].astype('string')\n",
        "training_df['text_clean'] = training_df['text_clean'].astype('string')\n",
        "\n",
        "training_df.info()\n",
        "training_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksIVK3OT926P"
      },
      "source": [
        "## **Option (b): Create IMDB DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpem18fLTh2U"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMl8txx8J4v8"
      },
      "source": [
        "# Create expected subdirectory for kaggle.json\n",
        "# NOTE: may already exist (if so, continue to next step)\n",
        "\n",
        "!mkdir /root/.kaggle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG1ZvWZkJ4wB"
      },
      "source": [
        "# Move auth file kaggle.json to expected subdirectory\n",
        "\n",
        "!mv kaggle.json /root/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVKBbVIvThw6"
      },
      "source": [
        "# Option #3 : Read Dataset from Kaggle in [*.csv format]\n",
        "\n",
        "# NOTE: Kaggle API periodically expires and must be renewed\n",
        "\n",
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n",
        "!unzip imdb-dataset-of-50k-movie-reviews.zip\n",
        "\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEKU5-pVpHot"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDSLSC4LpNC9"
      },
      "source": [
        "# Read *.csv file into DataFrame\n",
        "\n",
        "training_df = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "training_df[\"polarity\"] = training_df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n",
        "training_df[\"text_raw\"] = training_df[\"review\"].astype('string')\n",
        "training_df.drop(columns=['review'], inplace=True)\n",
        "training_df.drop(columns=['sentiment'], inplace=True)\n",
        "training_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lki2LNQExlxf"
      },
      "source": [
        "training_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwa-XzqXAnuA"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: (w/  stem) 2m37s on 20210918 at 13:44 with imdb\n",
        "# NOTE: (w/o stem)   20s on 20210918 at 12:06 with imdb\n",
        "\n",
        "custom_pipeline = [preprocessing.fillna,\n",
        "                   preprocessing.lowercase,\n",
        "                   preprocessing.remove_digits,\n",
        "                   preprocessing.remove_punctuation,\n",
        "                   preprocessing.remove_diacritics,\n",
        "                   preprocessing.remove_whitespace,\n",
        "                   preprocessing.stem]\n",
        "\n",
        "# Option (a): With Stemming\n",
        "training_df['text_clean'] = hero.clean(training_df['text_raw'], custom_pipeline)\n",
        "\n",
        "# Option (b): Without Stemming\n",
        "# df['clean_text'] = hero.clean(df['text'], custom_pipeline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X0-1fArpM_a"
      },
      "source": [
        "training_df['text_clean'] = training_df['text_clean'].astype('string')\n",
        "training_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UurEDAQBqabG"
      },
      "source": [
        "training_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRcNRjCGpM7n"
      },
      "source": [
        "# Check raw Class balance\n",
        "\n",
        "training_df['polarity'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvyD4s3qpj8T"
      },
      "source": [
        "def clean_dataframe(df_clean):\n",
        "  '''\n",
        "  Given a Pandas DataFrame\n",
        "  Return same DataFrame after dropping duplicate rows and rows with empty values\n",
        "  '''\n",
        "\n",
        "  # Check for null values \n",
        "  null_values = df_clean.isnull().sum()\n",
        "\n",
        "  print('Check for null values in DataFrame:')\n",
        "  print(f'  [{null_values.index[0]}] has {null_values[0]} missing values')\n",
        "  print(f'  [{null_values.index[1]}] has {null_values[1]} missing values')\n",
        "\n",
        "  df_clean.dropna(axis=0, inplace=True)\n",
        "  print('\\n')\n",
        "\n",
        "  # Check for duplicate rows\n",
        "  print('Check for duplicate rows in DataFrame:')\n",
        "  num_duplicates = df_clean.duplicated().sum() \n",
        "\n",
        "  print(f'  {num_duplicates} duplicate rows\\n')\n",
        "\n",
        "  if num_duplicates > 0:\n",
        "    # View duplicate reviews\n",
        "    reviews = df_clean['text_raw']\n",
        "    dup_reviews = df_clean[reviews.isin(reviews[reviews.duplicated()])].sort_values(\"text_raw\")\n",
        "\n",
        "    print(f'  First duplicated rows to be dropped:')\n",
        "    dup_reviews.head()\n",
        "\n",
        "    #drop duplicate reviews\n",
        "    print(f'  Original DataFrame: #{df_clean.shape[0]} reviews')\n",
        "    df_clean.drop_duplicates(inplace = True)\n",
        "    print(f'                      #{df_clean.shape[0]} reviews after dropping duplicates\\n')\n",
        "\n",
        "  return df_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVMGQEGlpM4A"
      },
      "source": [
        "# Drop missing and duplicate rows in DataFrame\n",
        "\n",
        "training_df = clean_dataframe(training_df)\n",
        "\n",
        "print(f'DataFrame.shape after cleaning: {training_df.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwyT-7qI0Pjf"
      },
      "source": [
        "training_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3XTTPVW-O2d"
      },
      "source": [
        "# Save Clean IMDB DataFrame\n",
        "\n",
        "training_df.to_csv(imdb_clean_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxPxyN3IuK2Q"
      },
      "source": [
        "# **Embeddings**\n",
        "\n",
        "Options:\n",
        "\n",
        "* Custom\n",
        "* CountVectorizer\n",
        "* TFIDF\n",
        "* HashingVectorizer\n",
        "* GLoVE\n",
        "* BERT\n",
        "\n",
        "References:\n",
        "\n",
        "* https://www.kaggle.com/saikumar587/imdb-text-classification-tf-idf-and-elmo\n",
        "\n",
        "* https://www.kaggle.com/avnika22/imdb-perform-sentiment-analysis-with-scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSCS3JRrCF9_"
      },
      "source": [
        "## **Split into Train/Test DataSets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWqs2WO5q1yM"
      },
      "source": [
        "training_df.head(5)['text_clean']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPNsb1VhpM0t"
      },
      "source": [
        "# Create DataFrames for current model under investigation\n",
        "#   Separate text features (X) from labels (y)\n",
        "\n",
        "X = training_df['text_clean']\n",
        "y = training_df['polarity']\n",
        "\n",
        "print(f'X.shape: {X.shape}\\n')\n",
        "\n",
        "print(f'y.shape: {y.shape}\\n')\n",
        "\n",
        "print(f'type(y[0]): {type(y[0])}\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFLnRwBn6jPO"
      },
      "source": [
        "# Split test/train\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.5, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5ndUvQ9MmJ8"
      },
      "source": [
        "type(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjo-KT10MnxD"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcCp6WztEUzS"
      },
      "source": [
        "X_train.shape\n",
        "y_train.shape\n",
        "X_test.shape\n",
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhqScwKr57Au"
      },
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    return accuracy_score(predictions, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emMJZbEFGfGQ"
      },
      "source": [
        "## **QUESTION: fit on train only or train+test**\n",
        "\n",
        "* https://stats.stackexchange.com/questions/154660/tfidfvectorizer-should-it-be-used-on-train-only-or-traintest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nozpPCDqMx60"
      },
      "source": [
        "## **CountVectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wm2suBlGowS"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 12.4s\n",
        "\n",
        "# Count Vectors as features\n",
        "\n",
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(X_train) # X_test)\n",
        "\n",
        "# transform the training and test data using count vectorizer object\n",
        "X_train_count =  count_vect.transform(X_train)\n",
        "X_test_count =  count_vect.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8bPLKwwMdHE"
      },
      "source": [
        "type(X_train_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYCc8GkAM1F8"
      },
      "source": [
        "## **TF-IDF: Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNdSEeBvE5uN"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 12s\n",
        "\n",
        "# Word level TF-IDF\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(X_train) # +X_test)\n",
        "X_train_tfidf =  tfidf_vect.transform(X_train)\n",
        "X_test_tfidf =  tfidf_vect.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf7GcN31M55f"
      },
      "source": [
        "## **TF-IDF: nGrams (IMDB best 4 of 5)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78zqGjxTFIpN"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 1m16s\n",
        "\n",
        "# ngram level TF-IDF\n",
        "\n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(X_train) # +X_test)\n",
        "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
        "X_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1-4zxu-M9bS"
      },
      "source": [
        "## **IF-IDF: Chars**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiKHFTouFIcO"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 1m41s\n",
        "\n",
        "# character level TF-IDF\n",
        "\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(X_train) # +X_test)\n",
        "X_train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
        "X_test_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_test) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYBp3iRff4ov"
      },
      "source": [
        "## **Hashing Vectors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAWJvwpZf8Lz"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 7s\n",
        "\n",
        "# Hashing vectors\n",
        "\n",
        "# getting train features\n",
        "hash_vect = HashingVectorizer(n_features=5000)\n",
        "hash_vect.fit(X_train) # +x_test)\n",
        "X_train_hash_vect =  hash_vect.transform(X_train) \n",
        "X_test_hash_vect =  hash_vect.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dO8mr7Y-29z"
      },
      "source": [
        "# **Sklearn ML Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILHYLWnwV2iw"
      },
      "source": [
        "# List of all ML models tested\n",
        "models_ml_ls = ['multinb', 'logreg', 'logreg_cv', 'rf', 'xgb']\n",
        "\n",
        "# Dictionary of best embedding (value) for each model (key)\n",
        "#   start with default value set to 'tfidf_ngram'\n",
        "best_emb_dt = {'multinb':'tfidf_ngram','logreg':'tfidf_ngram','logreg_cv':'tfidf_ngram','rf':'tfidf_ngram','xgb':'tfidf_ngram'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEflobrojBNQ"
      },
      "source": [
        "acc_emb_tech_ls = ['count', 'tfidf', 'tfidf_ngram', 'tfidf_ngram_chars', 'hash']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kweL1xZcbfMb"
      },
      "source": [
        "## **Option (a): Reading Pretrained ML Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6Ec-4vtbfSf"
      },
      "source": [
        "dir_trained_models = 'models_ml_trained'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yMQTUiPblFZ"
      },
      "source": [
        "# !mkdir $dir_trained_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Nz0E0GSbc47"
      },
      "source": [
        "!ls models_ml_trained"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6kPTSuvP0Ls"
      },
      "source": [
        "models_ml_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbnbFEG2dDT9"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFtra1sMbv1q"
      },
      "source": [
        "# Read models from model directory\n",
        "\n",
        "for amodel in models_ml_ls:\n",
        "  amodel_path = f'./{dir_trained_models}/imdb_{amodel}.pkl'\n",
        "  amodel_file = f'imdb_{amodel}.pkl' \n",
        "  amodel_name = f'clf_{amodel}'\n",
        "  print(f'Reading model: [{amodel}]\\n  from file: [{amodel_file}]\\n  path: [{amodel_path}]')\n",
        "  with open(amodel_path, 'rb') as fp:\n",
        "    print(f'    {amodel_name}')\n",
        "    globals()[amodel_name] = pickle.load(fp)\n",
        "\n",
        "type(clf_multinb) # .predict(X[0:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfzEXftDpbQ2"
      },
      "source": [
        "## **Option (b): Training ML Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWzzBUAi8gJ9"
      },
      "source": [
        "# Save Best Model Metrics (acc for various embedding)\n",
        "\n",
        "metrics_embed_dt = {}\n",
        "# models_metrics_df = {}\n",
        "\n",
        "\"\"\"\n",
        "{'logreg': {'accuracy_count': 0.8778185631882538,\n",
        "  'accuracy_hash': 0.8549070227098544,\n",
        "  'accuracy_tfidf': 0.8871767980315437,\n",
        "  'accuracy_tfidf_ngram': 0.8862087047718931,\n",
        "  'accuracy_tfidf_ngram_chars': 0.850792626356339},\n",
        " 'logreg_cv': {'accuracy_count': 0.8879028679762817,\n",
        "  'accuracy_hash': 0.8701544915493525,\n",
        "  'accuracy_tfidf': 0.8877415190996732,\n",
        "  'accuracy_tfidf_ngram': 0.8884675890444113,\n",
        "  'accuracy_tfidf_ngram_chars': 0.8657980718809245},\n",
        " 'multinb': {'accuracy_count': 0.8348190875721028,\n",
        "  'accuracy_hash': -99,\n",
        "  'accuracy_tfidf': 0.844661369045218,\n",
        "  'accuracy_tfidf_ngram': 0.8513573474244686,\n",
        "  'accuracy_tfidf_ngram_chars': 0.8037594288249768},\n",
        " 'rf': {'accuracy_count': 0.7407123552902263,\n",
        "  'accuracy_hash': 0.7452704610544149,\n",
        "  'accuracy_tfidf': 0.7606389415513695,\n",
        "  'accuracy_tfidf_ngram': 0.7643096284942116,\n",
        "  'accuracy_tfidf_ngram_chars': 0.6831108063410108},\n",
        " 'xgb': {'accuracy_count': 0.8183211649388891,\n",
        "  'accuracy_hash': 0.8086805695615344,\n",
        "  'accuracy_tfidf': 0.816828687830261,\n",
        "  'accuracy_tfidf_ngram': 0.8207817353071679,\n",
        "  'accuracy_tfidf_ngram_chars': 0.8057359525634302}}\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CnGm9RyastG"
      },
      "source": [
        "best_emb_dt\n",
        "\n",
        "\"\"\"\n",
        "{'logreg': 'tfidf',\n",
        " 'logreg_cv': 'tfidf_ngram',\n",
        " 'multinb': 'tfidf_ngram',\n",
        " 'rf': 'tfidf_ngram',\n",
        " 'xgb': 'tfidf_ngram'}\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfCLYj9b2v3f"
      },
      "source": [
        "### **Multinomal Naive Bayes (default)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-GmXOEp2ze_"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bES5uulNt9C"
      },
      "source": [
        "#### **Train and Get Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AL5m9Z62yiT"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 391ms\n",
        "\n",
        "# Compute Accuracy for most common simple embedding/vectorizing techniques\n",
        "\n",
        "# Naive Bayes on Count Vectors\n",
        "accuracy_count = train_model(MultinomialNB(), X_train_count, y_train, X_test_count)\n",
        "print(\"NB, Count Vectors: \", accuracy_count)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "accuracy_tfidf = train_model(MultinomialNB(), X_train_tfidf, y_train, X_test_tfidf)\n",
        "print(\"NB, WordLevel TF-IDF: \", accuracy_tfidf)\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "accuracy_tfidf_ngram = train_model(MultinomialNB(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
        "print(\"NB, N-Gram Vectors: \", accuracy_tfidf_ngram)\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "accuracy_tfidf_ngram_chars = train_model(MultinomialNB(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
        "print(\"NB, CharLevel Vectors: \", accuracy_tfidf_ngram_chars)\n",
        "\n",
        "# HashVectorization creates negative values to avoid collisions (negative values don't work with NB)\n",
        "accuracy_hash = 0\n",
        "\n",
        "acc_emb_vals_ls = [accuracy_count, accuracy_tfidf, accuracy_tfidf_ngram, accuracy_tfidf_ngram_chars, accuracy_hash]\n",
        "\n",
        "# Save Embedding Metrics\n",
        "metrics_embed_dt['multinb'] = {'accuracy_count':accuracy_count,\n",
        "                            'accuracy_tfidf':accuracy_tfidf,\n",
        "                            'accuracy_tfidf_ngram':accuracy_tfidf_ngram,\n",
        "                            'accuracy_tfidf_ngram_chars':accuracy_tfidf_ngram_chars,\n",
        "                            'accuracy_hash':-99}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7LSlgJxHT0o"
      },
      "source": [
        "acc_emb_vals_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URUYiEOjJdyh"
      },
      "source": [
        "max_value = max(acc_emb_vals_ls)\n",
        "max_value_idx = acc_emb_vals_ls.index(max_value)\n",
        "best_emb_tech = acc_emb_tech_ls[max_value_idx]\n",
        "\n",
        "best_emb_dt['multinb'] = best_emb_tech\n",
        "\n",
        "print(f'Maximum value: {max_value}\\nAt index: {max_value_idx}\\nBest Embedding: {best_emb_tech}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBGfYdv_K7GG"
      },
      "source": [
        "# Automatically select the best embedding method\n",
        "\n",
        "X_train = globals()[f'X_train_{best_emb_tech}']\n",
        "X_test = globals()[f'X_test_{best_emb_tech}']\n",
        "\n",
        "X_train.shape\n",
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RuHx9MRGcqA"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfSWvC8_2zbp"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 314ms\n",
        "\n",
        "# Naive Bayes\n",
        "\n",
        "clf_multinb = MultinomialNB()\n",
        "clf_multinb.fit(X_train, y_train)\n",
        "\n",
        "# Get Metrics on Fitting IMDB\n",
        "get_metrics(clf_multinb, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FShNX3wI99X"
      },
      "source": [
        "# Save Model to Pickle File\n",
        "\n",
        "amodel_file = f'imdb_multinb.pkl'\n",
        "print(f'Saving IMDB trained Multinomial Naive Bayes model to file: {amodel_file}')\n",
        "with open(amodel_file,'wb') as fp:\n",
        "  pickle.dump(clf_multinb,fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3TXiNU8fA2B"
      },
      "source": [
        "### **LogisticRegression (Default)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzMXtTCffTwr"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTihpnIUfD-Y"
      },
      "source": [
        "#### **Train and Get Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IuKwxevfCm1"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 51s\n",
        "\n",
        "# Compute Accuracy for most common simple embedding/vectorizing techniques\n",
        "\n",
        "# Linear Classifier on Count Vectors\n",
        "accuracy_count = train_model(LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=4000), X_train_count, y_train, X_test_count)\n",
        "print(\"LR, Count Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy_tfidf = train_model(LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=4000), X_train_tfidf, y_train, X_test_tfidf)\n",
        "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "accuracy_tfidf_ngram = train_model(LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=4000), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
        "print(\"LR, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "accuracy_tfidf_ngram_chars = train_model(LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=4000), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
        "print(\"LR, CharLevel Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Hash Vectors\n",
        "accuracy_hash = train_model(LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=4000), X_train_hash_vect, y_train, X_test_hash_vect)\n",
        "print(\"LR, Hash Vectors: \", accuracy)\n",
        "\n",
        "acc_emb_vals_ls = [accuracy_count, accuracy_tfidf, accuracy_tfidf_ngram, accuracy_tfidf_ngram_chars, accuracy_hash]\n",
        "\n",
        "# Save Embedding Metrics\n",
        "metrics_embed_dt['logreg'] = {'accuracy_count':accuracy_count,\n",
        "                            'accuracy_tfidf':accuracy_tfidf,\n",
        "                            'accuracy_tfidf_ngram':accuracy_tfidf_ngram,\n",
        "                            'accuracy_tfidf_ngram_chars':accuracy_tfidf_ngram_chars,\n",
        "                            'accuracy_hash':accuracy_hash}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDXGZ6kgfLas"
      },
      "source": [
        "max_value = max(acc_emb_vals_ls)\n",
        "max_value_idx = acc_emb_vals_ls.index(max_value)\n",
        "best_emb_tech = acc_emb_tech_ls[max_value_idx]\n",
        "\n",
        "best_emb_dt['logreg'] = best_emb_tech\n",
        "\n",
        "print(f'Maximum value: {max_value}\\nAt index: {max_value_idx}\\nBest Embedding: {best_emb_tech}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eJFGykQfLas"
      },
      "source": [
        "# Automatically select the best embedding method\n",
        "\n",
        "X_train = globals()[f'X_train_{best_emb_tech}']\n",
        "X_test = globals()[f'X_test_{best_emb_tech}']\n",
        "\n",
        "X_train.shape\n",
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhDC_OkxfLat"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 1s\n",
        "\n",
        "# Logistic Regression\n",
        "\n",
        "clf_logreg = LogisticRegression()\n",
        "clf_logreg.fit(X_train, y_train)\n",
        "\n",
        "# Get Metrics on Fitting IMDB  \n",
        "get_metrics(clf_logreg, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl0j5gleJEQ5"
      },
      "source": [
        "# Save Model to Pickle File\n",
        "\n",
        "amodel_file = f'imdb_logreg.pkl'\n",
        "print(f'Saving IMDB trained Logistic Regression model to file: {amodel_file}')\n",
        "with open(amodel_file,'wb') as fp:\n",
        "  pickle.dump(clf_logreg,fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLdEHaeo12T9"
      },
      "source": [
        "### **LogisticRegressionCV (cv=6, max_iter=500)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rboyD6Cy2Num"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU2otYfj95W0"
      },
      "source": [
        "#### **Train and Get Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXGpX771-QVz"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 16m19s\n",
        "\n",
        "# Compute Accuracy for most common simple embedding/vectorizing techniques\n",
        "\n",
        "# Linear Classifier on Count Vectors\n",
        "accuracy_count = train_model(LogisticRegressionCV(cv=6,scoring='accuracy',random_state=42,n_jobs=-1,verbose=3,max_iter=500), X_train_count, y_train, X_test_count)\n",
        "print(\"LR, Count Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy_tfidf = train_model(LogisticRegressionCV(cv=6,scoring='accuracy',random_state=42,n_jobs=-1,verbose=3,max_iter=500), X_train_tfidf, y_train, X_test_tfidf)\n",
        "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "accuracy_tfidf_ngram = train_model(LogisticRegressionCV(cv=6,scoring='accuracy',random_state=42,n_jobs=-1,verbose=3,max_iter=500), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
        "print(\"LR, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "accuracy_tfidf_ngram_chars = train_model(LogisticRegressionCV(cv=6,scoring='accuracy',random_state=42,n_jobs=-1,verbose=3,max_iter=500), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
        "print(\"LR, CharLevel Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Hash Vectors\n",
        "accuracy_hash = train_model(LogisticRegressionCV(cv=6,scoring='accuracy',random_state=42,n_jobs=-1,verbose=3,max_iter=500), X_train_hash_vect, y_train, X_test_hash_vect)\n",
        "print(\"LR, Hash Vectors: \", accuracy)\n",
        "\n",
        "acc_emb_vals_ls = [accuracy_count, accuracy_tfidf, accuracy_tfidf_ngram, accuracy_tfidf_ngram_chars, accuracy_hash]\n",
        "\n",
        "\n",
        "# Save Embedding Metrics\n",
        "metrics_embed_dt['logreg_cv'] = {'accuracy_count':accuracy_count,\n",
        "                            'accuracy_tfidf':accuracy_tfidf,\n",
        "                            'accuracy_tfidf_ngram':accuracy_tfidf_ngram,\n",
        "                            'accuracy_tfidf_ngram_chars':accuracy_tfidf_ngram_chars,\n",
        "                            'accuracy_hash':accuracy_hash}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_Pxo9zq_1Vm"
      },
      "source": [
        "max_value = max(acc_emb_vals_ls)\n",
        "max_value_idx = acc_emb_vals_ls.index(max_value)\n",
        "best_emb_tech = acc_emb_tech_ls[max_value_idx]\n",
        "\n",
        "best_emb_dt['logreg_cv'] = best_emb_tech\n",
        "\n",
        "print(f'Maximum value: {max_value}\\nAt index: {max_value_idx}\\nBest Embedding: {best_emb_tech}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh2QLCyc_1Vm"
      },
      "source": [
        "# Automatically select the best embedding method\n",
        "\n",
        "X_train = globals()[f'X_train_{best_emb_tech}']\n",
        "X_test = globals()[f'X_test_{best_emb_tech}']\n",
        "\n",
        "X_train.shape\n",
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ANk5toM_1Vm"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Logistic Regression\n",
        "\n",
        "clf_logreg_cv = LogisticRegressionCV()\n",
        "clf_logreg_cv.fit(X_train, y_train)\n",
        "\n",
        "# Get Metrics on Fitting IMDB\n",
        "get_metrics(clf_logreg_cv, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URQQG1vJJNRY"
      },
      "source": [
        "# Save Model to Pickle File\n",
        "\n",
        "amodel_file = f'imdb_logreg_cv.pkl'\n",
        "print(f'Saving IMDB trained Logistic Regression-Cross Validation model to file: {amodel_file}')\n",
        "with open(amodel_file,'wb') as fp:\n",
        "  pickle.dump(clf_logreg_cv,fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNWE5ht-iQL4"
      },
      "source": [
        "### **Random Forest Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MFl5dXTiWN-"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpCJZRpDk3Ax"
      },
      "source": [
        "#### **Train and Get Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62-E92FkiPMt"
      },
      "source": [
        "# RF on Count Vectors\n",
        "accuracy_count = train_model(RandomForestClassifier(n_estimators=10), X_train_count, y_train, X_test_count)\n",
        "print(\"RF, Count Vectors: \", accuracy)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "accuracy_tfidf = train_model(RandomForestClassifier(n_estimators=10), X_train_tfidf, y_train, X_test_tfidf)\n",
        "print(\"RF, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# RF on Ngram Level TF IDF Vectors\n",
        "accuracy_tfidf_ngram = train_model(RandomForestClassifier(n_estimators=10), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
        "print(\"RF, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# RF on Character Level TF IDF Vectors\n",
        "accuracy_tfidf_ngram_chars = train_model(RandomForestClassifier(n_estimators=10), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
        "print(\"RF, CharLevel Vectors: \", accuracy)\n",
        "\n",
        "# RF on Hash Vectors\n",
        "accuracy_hash = train_model(RandomForestClassifier(n_estimators=10), X_train_hash_vect, y_train, X_test_hash_vect)\n",
        "print(\"RF, Hash Vectors: \", accuracy)\n",
        "\n",
        "acc_emb_vals_ls = [accuracy_count, accuracy_tfidf, accuracy_tfidf_ngram, accuracy_tfidf_ngram_chars, accuracy_hash]\n",
        "\n",
        "# Save Embedding Metrics\n",
        "metrics_embed_dt['rf'] = {'accuracy_count':accuracy_count,\n",
        "                            'accuracy_tfidf':accuracy_tfidf,\n",
        "                            'accuracy_tfidf_ngram':accuracy_tfidf_ngram,\n",
        "                            'accuracy_tfidf_ngram_chars':accuracy_tfidf_ngram_chars,\n",
        "                            'accuracy_hash':accuracy_hash}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sPttILBk06-"
      },
      "source": [
        "max_value = max(acc_emb_vals_ls)\n",
        "max_value_idx = acc_emb_vals_ls.index(max_value)\n",
        "best_emb_tech = acc_emb_tech_ls[max_value_idx]\n",
        "\n",
        "best_emb_dt['rf'] = best_emb_tech\n",
        "\n",
        "print(f'Maximum value: {max_value}\\nAt index: {max_value_idx}\\nBest Embedding: {best_emb_tech}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI_ERMYQk06_"
      },
      "source": [
        "# Automatically select the best embedding method\n",
        "\n",
        "X_train = globals()[f'X_train_{best_emb_tech}']\n",
        "X_test = globals()[f'X_test_{best_emb_tech}']\n",
        "\n",
        "X_train.shape\n",
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiAylZ6dk06_"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 1m32s\n",
        "\n",
        "# Random Forests\n",
        "\n",
        "clf_rf = RandomForestClassifier()\n",
        "clf_rf.fit(X_train, y_train)\n",
        "\n",
        "# Get Metrics on Fitting IMDB\n",
        "get_metrics(clf_rf, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SE_L7fkJVOX"
      },
      "source": [
        "# Save Model to Pickle File\n",
        "\n",
        "amodel_file = f'imdb_rf.pkl'\n",
        "print(f'Saving IMDB trained Random Forest Classifier model to file: {amodel_file}')\n",
        "with open(amodel_file,'wb') as fp:\n",
        "  pickle.dump(clf_rf,fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAbLLeeg5mNp"
      },
      "source": [
        "### **Extreme Gradient Boosting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYk9ULk_57J7"
      },
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXfoGbX05q_p"
      },
      "source": [
        "#### **Train and Get Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl_yGyur5lu9"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 7m13s\n",
        "\n",
        "# Extereme Gradient Boosting on Count Vectorsy_train\n",
        "accuracy_count = train_model(XGBClassifier(), X_train_count.tocsc(), y_train, X_test_count.tocsc())\n",
        "print(\"Xgb, Count Vectors: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
        "accuracy_tfidf = train_model(XGBClassifier(), X_train_tfidf.tocsc(), y_train, X_test_tfidf.tocsc())\n",
        "print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Ngram Level TF IDF Vectors\n",
        "accuracy_tfidf_ngram = train_model(XGBClassifier(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
        "print(\"Xgb, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
        "accuracy_tfidf_ngram_chars = train_model(XGBClassifier(), X_train_tfidf_ngram_chars.tocsc(), y_train, X_test_tfidf_ngram_chars.tocsc())\n",
        "print(\"Xgb, CharLevel Vectors: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Hash Vectors\n",
        "accuracy_hash = train_model(XGBClassifier(), X_train_hash_vect, y_train, X_test_hash_vect)\n",
        "print(\"Xgb, Hash Vectors: \", accuracy)\n",
        "\n",
        "acc_emb_vals_ls = [accuracy_count, accuracy_tfidf, accuracy_tfidf_ngram, accuracy_tfidf_ngram_chars, accuracy_hash]\n",
        "\n",
        "metrics_embed_dt['xgb'] = {'accuracy_count':accuracy_count,\n",
        "                            'accuracy_tfidf':accuracy_tfidf,\n",
        "                            'accuracy_tfidf_ngram':accuracy_tfidf_ngram,\n",
        "                            'accuracy_tfidf_ngram_chars':accuracy_tfidf_ngram_chars,\n",
        "                            'accuracy_hash':accuracy_hash}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWnnshPS6nbb"
      },
      "source": [
        "max_value = max(acc_emb_vals_ls)\n",
        "max_value_idx = acc_emb_vals_ls.index(max_value)\n",
        "best_emb_tech = acc_emb_tech_ls[max_value_idx]\n",
        "\n",
        "best_emb_dt['xgb'] = best_emb_tech\n",
        "\n",
        "print(f'Maximum value: {max_value}\\nAt index: {max_value_idx}\\nBest Embedding: {best_emb_tech}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFaE0_cR6nbc"
      },
      "source": [
        "# Automatically select the best embedding method\n",
        "\n",
        "X_train = globals()[f'X_train_{best_emb_tech}']\n",
        "X_test = globals()[f'X_test_{best_emb_tech}']\n",
        "\n",
        "X_train.shape\n",
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enkBTAgu6nbd"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 1m08s\n",
        "\n",
        "# Random Forests\n",
        "\n",
        "clf_xgb = XGBClassifier()\n",
        "clf_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Get Metrics on Fitting IMDB\n",
        "get_metrics(clf_xgb, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_4lCWIdJaSi"
      },
      "source": [
        "# Save Model to Pickle File\n",
        "\n",
        "amodel_file = f'imdb_xgboost.pkl'\n",
        "print(f'Saving IMDB trained XGBoost model to file: {amodel_file}')\n",
        "with open(amodel_file,'wb') as fp:\n",
        "  pickle.dump(clf_xgb,fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DOIKVgtCveP"
      },
      "source": [
        "### **Save IMDB-Trained Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz6puK7fCx6F"
      },
      "source": [
        "# Save all IMDB-trained Models to pickle files\n",
        "\n",
        "# Save Multinomial Naive Bayes\n",
        "amodel_file = f'./models_ml_trained/imdb_multinb.pkl'\n",
        "print(f'Saving IMDB trained Multinomial Naive Bayes model to file: {amodel_file}')\n",
        "with open(amodel_file,'wb') as fp:\n",
        "  pickle.dump(clf_multinb,fp)\n",
        "\n",
        "# Save LogisticRegression\n",
        "amodel_file = f'./models_ml_trained/imdb_logreg.pkl'\n",
        "print(f'Saving IMDB trained Logistic Regression model to file: {amodel_file}')\n",
        "with open(amodel_file,'wb') as fp:\n",
        "  pickle.dump(clf_logreg,fp)\n",
        "\n",
        "# Save LogisticRegressionCV\n",
        "amodel_file = f'./models_ml_trained/imdb_logreg_cv.pkl'\n",
        "print(f'Saving IMDB trained Logistic Regression-Cross Validation model to file: {amodel_file}')\n",
        "with open(amodel_file,'wb') as fp:\n",
        "  pickle.dump(clf_logreg_cv,fp)\n",
        "\n",
        "# Save Random Forest Classifier\n",
        "amodel_file = f'./models_ml_trained/imdb_rf.pkl'\n",
        "print(f'Saving IMDB trained Random Forest Classifier model to file: {amodel_file}')\n",
        "with open(amodel_file,'wb') as fp:\n",
        "  pickle.dump(clf_rf,fp)\n",
        "\n",
        "# Save XGBoost\n",
        "amodel_file = f'./models_ml_trained/imdb_xgboost.pkl'\n",
        "print(f'Saving IMDB trained XGBoost model to file: {amodel_file}')\n",
        "with open(amodel_file,'wb') as fp:\n",
        "  pickle.dump(clf_xgb,fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B9yERauWldM"
      },
      "source": [
        "## **Inferring Sentiment on every Corpus with all Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_N_WAxIUEk1"
      },
      "source": [
        "corpus_ml_dt = {}\n",
        "\n",
        "# Read from Corpus DataFrame Files\n",
        "for key, value in corpora_dt.items():\n",
        "  corpus_ml_dt[key] = value\n",
        "\n",
        "# Create empty DataFrames for each Corpus\n",
        "# for acorpus in corpora_ls:\n",
        "#   corpus_ml_dt[acorpus] = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xndgLtUdNTNB"
      },
      "source": [
        "corpus_ml_dt['cdickens_achristmascarol']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFH3cSVxUQvp"
      },
      "source": [
        "# Create Dictionary with one DataFrame for each corpus\n",
        "#   The columns of DataFrames hold model sentiment values computed for each sentence\n",
        "\n",
        "# corpus_ml_dt.values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcF9zjXBX8ll"
      },
      "source": [
        "def predict_cls(amodel, X_corpus_emb):\n",
        "  '''\n",
        "  Given a ML model and an embedded corpus\n",
        "  Return a pd.Series of Sentiment Values\n",
        "  '''\n",
        "\n",
        "  if amodel == 'multinb':\n",
        "    return pd.Series(clf_multinb.predict(X_corpus_emb))\n",
        "  elif amodel == 'logreg':\n",
        "    return pd.Series(clf_logreg.predict(X_corpus_emb))\n",
        "  elif amodel == 'logreg_cv':\n",
        "    return pd.Series(clf_logreg_cv.predict(X_corpus_emb))\n",
        "  elif amodel == 'rf':\n",
        "    return pd.Series(clf_rf.predict(X_corpus_emb))\n",
        "  elif amodel == 'xgb':\n",
        "    return pd.Series(clf_xgb.predict(X_corpus_emb))\n",
        "  else:\n",
        "    print(f'ERROR: In predict_cls() with invalid amodel argument: {amodel}')\n",
        "    return -99"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2niTQ2GYx_h"
      },
      "source": [
        "# Reminder: (defined above) models_ml_ls = ['multinb', 'logreg', 'logreg_cv', 'rf', 'xgb']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYxsh7TPwNC1"
      },
      "source": [
        "corpus_ml_dt['fbaum_thewonderfulwizardofoz'].head(10)\n",
        "corpus_ml_dt['fbaum_thewonderfulwizardofoz'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2BkYiEHtd_P"
      },
      "source": [
        "corpus_ml_dt['cdickens_achristmascarol'].head(10)\n",
        "corpus_ml_dt['cdickens_achristmascarol'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h8h_HS5bC4f"
      },
      "source": [
        "corpus_ml_dt['cdickens_achristmascarol'].rolling(340, center=True).mean().plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgwIphlpv_hl"
      },
      "source": [
        "corpus_ml_dt['cdickens_greatexpectations'].rolling(340, center=True).mean().plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwDTgZXqbLpc"
      },
      "source": [
        "corpus_ml_dt['cdickens_achristmascarol']['logreg'].rolling(130, center=True).mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRi_zgCTdY0q"
      },
      "source": [
        "corpora_dt['fbaum_thewonderfulwizardofoz']['sent_clean']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6T1ZxzEySwC"
      },
      "source": [
        "corpora_dt['cdickens_achristmascarol']['sent_clean']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jaxg366BGh7"
      },
      "source": [
        "best_emb_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy_KEz1zBGbK"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "# Option (a): SA Timeseries -> (z-Score Standardization) -> (SMA 10%)\n",
        "\n",
        "# Rolling z-score: https://stackoverflow.com/questions/47164950/compute-rolling-z-score-in-pandas-dataframe\n",
        "\n",
        "def zscore(x, window):\n",
        "    r = x.rolling(window=window)\n",
        "    m = r.mean().shift(1)\n",
        "    s = r.std(ddof=0).shift(1)\n",
        "    z = (x-m)/s\n",
        "    return z\n",
        "\n",
        "# df['zscore'] = zscore(df['value'],window)\n",
        "\n",
        "# For each individual corpus in our collection\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "\n",
        "  # Apply each ML Model to compute sentiment values on each sentence\n",
        "  for j, amodel in enumerate(models_ml_ls):\n",
        "\n",
        "    # Vectorize Corpus ----------\n",
        "    best_emb_type = best_emb_dt[amodel] \n",
        "\n",
        "    # Select the best embedding technique on IMDB to vectorize Corpus\n",
        "    if best_emb_type == 'count':\n",
        "      # CountVectorizer on Corpus\n",
        "      X_corpus_emb =  count_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf':\n",
        "      # TF-IDF (Words) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf_ngram':\n",
        "      # TF-IDF (ngrams) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect_ngram.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf_ngram_chars':\n",
        "      # TF-IDF (chars) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect_ngram_chars.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'hash':\n",
        "      # TF-IDF (chars) on Corpus\n",
        "      X_corpus_emb =  hash_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    else:\n",
        "      # ERROR\n",
        "      print(f'ERROR: Illegal value for best embedding technique (best_emb_type): {best_emb_type}')\n",
        "\n",
        "    # Predict Sentiments ----------\n",
        "    corpus_ml_dt[acorpus][amodel] =  pd.Series(predict_cls(amodel, X_corpus_emb))\n",
        "\n",
        "    # Standardize and create Simple Moving Average (SMA 10%)\n",
        "    print(f'\\n\\nPlot #{i*len(models_ml_ls) + j}: {acorpus} with model {amodel}')\n",
        "    win10per = int(corpus_ml_dt[acorpus].shape[0]*0.1)\n",
        "    # del temp_df\n",
        "    # temp_df = pd.DataFrame()\n",
        "    # temp_df['zscore'] = zscore(corpus_ml_dt[acorpus][amodel],win10per)\n",
        "    # corpus_ml_dt[acorpus][amodel].rolling(win10per, center=True).mean().plot();\n",
        "    \n",
        "    float_array = corpus_ml_dt[acorpus][amodel].values.astype(float)\n",
        "    amodel_stdscaler = f'{amodel}_stdscaler'\n",
        "    corpus_ml_dt[acorpus][amodel_stdscaler] = mean_std_scaler.fit_transform(float_array.reshape(-1,1))\n",
        "    # temp_df['zscore'].rolling(win10per, center=True).mean().plot()\n",
        "    # zscore(corpus_ml_dt[acorpus][amodel],win10per).rolling(win10per, center=True).mean().plot()\n",
        "    corpus_ml_dt[acorpus][amodel_stdscaler].rolling(win10per, center=True).mean().plot()\n",
        "    plt.title(f'Corpus: {acorpus} with Model: {amodel}\\nz-Score Standardardized SMA 10%');\n",
        "    plt.legend(loc='best')  \n",
        "  plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvbm5i_kVKeA"
      },
      "source": [
        "### **Save Inferred Sentiment Values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAwGYPpNgFyn"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIyarK9JfXH1"
      },
      "source": [
        "!ls data_corpora_sa/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ91Fiwx0B-o"
      },
      "source": [
        "!ls -altr ./data_corpora_sa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVwQQHE5V7ff"
      },
      "source": [
        "# Save DataFrame with new model sentiment values\n",
        "\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "\n",
        "  data_dir = 'data_corpora_sa'\n",
        "  model_file = f'./{data_dir}/models_ml_{acorpus}.csv'\n",
        "  print(f'\\n\\nDataFrame #{i*len(models_ml_ls) + j}: [{acorpus}]\\n')\n",
        "  corpus_ml_dt[acorpus].head()\n",
        "  \n",
        "  print(f'\\n    Saving contents to file: [{model_file}]\\n\\n')\n",
        "  corpus_ml_dt[acorpus].to_csv(model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6f4oX9_VRSN"
      },
      "source": [
        "# For each individual corpus in our collection\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "\n",
        "  # Apply each ML Model to compute sentiment values on each sentence\n",
        "  for j, amodel in enumerate(models_ml_ls):\n",
        "    \n",
        "    print(f'\\nDataFrame #{i*len(models_ml_ls) + j}: [{acorpus}] with model [{amodel}]\\n')\n",
        "    # corpus_ml_dt[acorpus][amodel].head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDquLiQPNNOZ"
      },
      "source": [
        "## **Standardize, Smooth and Plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGp55S5ESOPH"
      },
      "source": [
        "# Option (a): SA Timeseries -> (z-Score Standardization) -> (SMA 10%)\n",
        "\n",
        "# Rolling z-score: https://stackoverflow.com/questions/47164950/compute-rolling-z-score-in-pandas-dataframe\n",
        "\n",
        "def zscore(x, window):\n",
        "    r = x.rolling(window=window)\n",
        "    m = r.mean().shift(1)\n",
        "    s = r.std(ddof=0).shift(1)\n",
        "    z = (x-m)/s\n",
        "    return z\n",
        "\n",
        "# df['zscore'] = zscore(df['value'],window)\n",
        "\n",
        "# For each individual corpus in our collection\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "\n",
        "  # Apply each ML Model to compute sentiment values on each sentence\n",
        "  for j, amodel in enumerate(models_ml_ls):\n",
        "\n",
        "    # Vectorize Corpus ----------\n",
        "\n",
        "    best_emb_type = best_emb_dt[amodel] \n",
        "\n",
        "    # Select the best embedding technique on IMDB to vectorize Corpus\n",
        "    if best_emb_type == 'count':\n",
        "      # CountVectorizer on Corpus\n",
        "      X_corpus_emb =  count_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf':\n",
        "      # TF-IDF (Words) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf_ngram':\n",
        "      # TF-IDF (ngrams) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect_ngram.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf_ngram_chars':\n",
        "      # TF-IDF (chars) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect_ngram_chars.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'hash':\n",
        "      # TF-IDF (chars) on Corpus\n",
        "      X_corpus_emb =  hash_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    else:\n",
        "      # ERROR\n",
        "      print(f'ERROR: Illegal value for best embedding technique (best_emb_type): {best_emb_type}')\n",
        "\n",
        "    # Predict Sentiments ----------\n",
        "    corpus_ml_dt[acorpus][amodel] =  pd.Series(predict_cls(amodel, X_corpus_emb))\n",
        "\n",
        "    # Standardize and create Simple Moving Average (SMA 10%)\n",
        "    print(f'\\n\\nPlot #{i*len(models_ml_ls) + j}: {acorpus} with model {amodel}')\n",
        "    win10per = int(corpus_ml_dt[acorpus].shape[0]*0.1)\n",
        "    # del temp_df\n",
        "    # temp_df = pd.DataFrame()\n",
        "    # temp_df['zscore'] = zscore(corpus_ml_dt[acorpus][amodel],win10per)\n",
        "    # corpus_ml_dt[acorpus][amodel].rolling(win10per, center=True).mean().plot();\n",
        "    \n",
        "    float_array = corpus_ml_dt[acorpus][amodel].values.astype(float)\n",
        "    amodel_stdscaler = f'{amodel}_stdscaler'\n",
        "    corpus_ml_dt[acorpus][amodel_stdscaler] = mean_std_scaler.fit_transform(float_array.reshape(-1,1))\n",
        "    # temp_df['zscore'].rolling(win10per, center=True).mean().plot()\n",
        "    # zscore(corpus_ml_dt[acorpus][amodel],win10per).rolling(win10per, center=True).mean().plot()\n",
        "    corpus_ml_dt[acorpus][amodel_stdscaler].rolling(win10per, center=True).mean().plot()\n",
        "    plt.title(f'Corpus: {acorpus} with Model: {amodel}\\nz-Score Standardardized SMA 10%');\n",
        "    plt.legend(loc='best')  \n",
        "  plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCn2g-ub4rSj"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD2XXud6SMH8"
      },
      "source": [
        "# Option (b): SA Timeseries -> (SMA 10%) -> (z-Score Standardization)\n",
        "\n",
        "# Rolling z-score: https://stackoverflow.com/questions/47164950/compute-rolling-z-score-in-pandas-dataframe\n",
        "\n",
        "def zscore(x, window):\n",
        "    r = x.rolling(window=window)\n",
        "    m = r.mean().shift(1)\n",
        "    s = r.std(ddof=0).shift(1)\n",
        "    z = (x-m)/s\n",
        "    return z\n",
        "\n",
        "# df['zscore'] = zscore(df['value'],window)\n",
        "\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "  for j, amodel in enumerate(models_ml_ls):\n",
        "\n",
        "    # Vectorize Corpus ----------\n",
        "\n",
        "    best_emb_type = best_emb_dt[amodel] \n",
        "\n",
        "    # Select the best embedding technique on IMDB to vectorize Corpus\n",
        "    if best_emb_type == 'count':\n",
        "      # CountVectorizer on Corpus\n",
        "      X_corpus_emb =  count_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf':\n",
        "      # TF-IDF (Words) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf_ngram':\n",
        "      # TF-IDF (ngrams) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect_ngram.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf_ngram_chars':\n",
        "      # TF-IDF (chars) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect_ngram_chars.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'hash':\n",
        "      # TF-IDF (chars) on Corpus\n",
        "      X_corpus_emb =  hash_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    else:\n",
        "      # ERROR\n",
        "      print(f'ERROR: Illegal value for best embedding technique (best_emb_type): {best_emb_type}')\n",
        "\n",
        "    # Predict Sentiments ----------\n",
        "    corpus_ml_dt[acorpus][amodel] =  pd.Series(predict_cls(amodel, X_corpus_emb))\n",
        "\n",
        "    # Standardize and create Simple Moving Average (SMA 10%)\n",
        "    print(f'\\n\\nPlot #{i*len(models_ml_ls) + j}: {acorpus} with model {amodel}')\n",
        "    win10per = int(corpus_ml_dt[acorpus].shape[0]*0.1)\n",
        "    # del temp_df\n",
        "    # temp_df = pd.DataFrame()\n",
        "    # temp_df['zscore'] = zscore(corpus_ml_dt[acorpus][amodel],win10per)\n",
        "    # corpus_ml_dt[acorpus][amodel].rolling(win10per, center=True).mean().plot();\n",
        "    \n",
        "    # float_array = corpus_ml_dt[acorpus][amodel].values.astype(float)\n",
        "    amodel_roll10_stdscaler = f'{amodel}_roll10_stdscaler'\n",
        "    corpus_ml_dt[acorpus][amodel_roll10_stdscaler] = corpus_ml_dt[acorpus][amodel].rolling(win10per, min_periods=(win10per//2), center=True).mean()\n",
        "    float_array = corpus_ml_dt[acorpus][amodel_roll10_stdscaler].values.astype(float)\n",
        "    mean_std_scaler.fit_transform(float_array.reshape(-1,1))\n",
        "    corpus_ml_dt[acorpus][amodel_roll10_stdscaler] = mean_std_scaler.fit_transform(float_array.reshape(-1,1))\n",
        "    # temp_df['zscore'].rolling(win10per, center=True).mean().plot()\n",
        "    # zscore(corpus_ml_dt[acorpus][amodel],win10per).rolling(win10per, center=True).mean().plot()\n",
        "    corpus_ml_dt[acorpus][amodel_roll10_stdscaler].plot() # .rolling(win10per, center=True).mean().plot()\n",
        "    plt.title(f'Corpus: {acorpus} with Model: {amodel}\\nSMA 10%');\n",
        "    plt.legend(loc='best')  \n",
        "  plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1RCRZy2YsNP"
      },
      "source": [
        "# **AutoML Solutions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3IylymREqhc"
      },
      "source": [
        "automl_ls = ['flaml']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh4_HfWZKE9_"
      },
      "source": [
        "## **Read Prior Sentiments for Corpora**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMx-fEtBGPMQ"
      },
      "source": [
        "!ls -d */"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CKD1w6XJBdU"
      },
      "source": [
        "corpus_ml_dt = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faqynWdpHsny"
      },
      "source": [
        "listing = glob.glob('./data_corpora_sa/models_ml_*.csv')\n",
        "for i,filename in enumerate(listing):\n",
        "  fileparts_ls = filename.split('/')\n",
        "  datafile_str = ''.join(fileparts_ls[-1])\n",
        "  corpus_str = '_'.join(datafile_str.split('_')[2:])\n",
        "  corpus_str = corpus_str.split('.')[0]\n",
        "  print(f'\\nReading #{i}: {filename}\\n   datafile.csv: {datafile_str}\\n         corpus: {corpus_str}')\n",
        "  corpus_ml_dt[corpus_str] = pd.read_csv(f'./data_corpora_sa/{datafile_str}', index_col=[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVdNrsIohD2e"
      },
      "source": [
        "corpus_ml_dt['cdickens_achristmascarol'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWz273NKJ4t3"
      },
      "source": [
        "corpus_ml_dt['cdickens_achristmascarol'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3ORgo89Z8XR"
      },
      "source": [
        "## **Microsoft FLAML [RESTART RUNTIME]**\n",
        "\n",
        "* https://github.com/microsoft/FLAML/blob/main/notebook/flaml_automl.ipynb\n",
        "* https://www.youtube.com/watch?v=bJfDJhe-O-c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIu-pNag7KK2"
      },
      "source": [
        "from flaml import AutoML\n",
        "\n",
        "automl = AutoML()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-Ui-Gba63qO"
      },
      "source": [
        "# Pick the best embedding based upon IMDB train/test metrics (4/5 X_train_tfidf_ngram)\n",
        "\n",
        "X_train = X_train_tfidf_ngram\n",
        "X_test = X_test_tfidf_ngram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLSsY2_13fMl"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 1m12s (default time budget)\n",
        "\n",
        "# Default Classification Search:  ['lgbm', 'rf', 'catboost', 'xgboost', 'extra_tree', 'lrl1']\n",
        "\n",
        "# automl.fit(X_train, y_train, task=\"classification\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwWGyFMAaHHp"
      },
      "source": [
        "# from flaml import AutoML\n",
        "# from sklearn.datasets import load_iris"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c34HbgCs72sm"
      },
      "source": [
        "# !mkdir logs_flaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daeo7RxBZCDh"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 22m28s on 20210918 at 23:10 (IMDB)\n",
        "\n",
        "# Customize FLAML Search\n",
        "\n",
        "# Initialize an AutoML instance\n",
        "automl = AutoML()\n",
        "\n",
        "# Specify automl goal and constraint\n",
        "automl_settings = {\n",
        "    \"time_budget\": 600, # 900,  # in seconds\n",
        "    \"metric\": 'accuracy',\n",
        "    \"task\": 'classification',\n",
        "    \"log_file_name\": \"logs_flaml/flaml.log\",\n",
        "}\n",
        "\n",
        "# Train with labeled input data\n",
        "automl.fit(X_train=X_train, y_train=y_train, **automl_settings)\n",
        "\n",
        "# Predict\n",
        "print(automl.predict_proba(X_train))\n",
        "\n",
        "# Export the best model\n",
        "print(automl.model)\n",
        "\n",
        "\"\"\"\n",
        "[flaml.automl: 09-19 03:01:20] {1735} INFO - iteration 175, current learner lrl1\n",
        "No low-cost partial config given to the search algorithm. For cost-frugal search, consider providing low-cost values for cost-related hps via 'low_cost_partial_config'.\n",
        "[flaml.automl: 09-19 03:01:31] {1920} INFO -  at 908.4s,\tbest lrl1's error=0.5050,\tbest catboost's error=0.4796\n",
        "[flaml.automl: 09-19 03:01:31] {2021} INFO - selected model: <catboost.core.CatBoostClassifier object at 0x7fd8fa364710>\n",
        "[flaml.automl: 09-19 03:08:51] {2084} INFO - retrain catboost for 440.1s\n",
        "[flaml.automl: 09-19 03:08:51] {2088} INFO - retrained model: <catboost.core.CatBoostClassifier object at 0x7fd8ee43b8d0>\n",
        "[flaml.automl: 09-19 03:08:51] {1529} INFO - fit succeeded\n",
        "[flaml.automl: 09-19 03:08:51] {1531} INFO - Time taken to find the best model: 852.8518950939178\n",
        "[flaml.automl: 09-19 03:08:51] {1545} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQSmfTHs8VW2"
      },
      "source": [
        "# Retrieve best config and best learner\n",
        "\n",
        "print('Best ML leaner:', automl.best_estimator)\n",
        "print('Best hyperparmeter config:', automl.best_config)\n",
        "print('Best accuracy on validation data: {0:.4g}'.format(1-automl.best_loss))\n",
        "print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))\n",
        "\n",
        "\"\"\"\n",
        "Best ML leaner: catboost\n",
        "Best hyperparmeter config: {'early_stopping_rounds': 13, 'learning_rate': 0.05455865754453469}\n",
        "Best accuracy on validation data: 0.5204\n",
        "Training duration of best run: 39.15 s\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGRlOYb480R2"
      },
      "source": [
        "automl.model.estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DPL2BqR80Nx"
      },
      "source": [
        "# compute predictions of testing dataset\n",
        "\n",
        "y_pred = automl.predict(X_test)\n",
        "print('Predicted labels', y_pred)\n",
        "print('\\nTrue labels', y_test)\n",
        "\n",
        "# y_pred_proba = automl.predict_proba(X_test)[:,1]\n",
        "\n",
        "# Get Metrics on Fitting IMDB\n",
        "# get_metrics(clf_flaml, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sbU1BkDDEnJ"
      },
      "source": [
        "acc = accuracy_score(y_test, y_pred)\n",
        "f1=f1_score(y_test, y_pred)\n",
        "cm=confusion_matrix(y_test, y_pred)\n",
        "report=classification_report(y_test,y_pred)\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "sns.heatmap(cm,annot=True,cmap='Blues',xticklabels=[0,1],fmt='d',annot_kws={\"fontsize\":19})\n",
        "plt.xlabel(\"Predicted\",fontsize=16)\n",
        "plt.ylabel(\"Actual\",fontsize=16)\n",
        "plt.show();\n",
        "print(\"\\nAccuracy: \",round(acc,2))\n",
        "print(\"\\nF1 Score: \",round(f1,2))\n",
        "print(\"\\nConfusion Matrix: \\n\",cm)\n",
        "print(\"\\nReport:\",report);\n",
        "\n",
        "\"\"\"\n",
        "budget time = 900s \n",
        "\n",
        "Accuracy:  0.87\n",
        "\n",
        "F1 Score:  0.88\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbxR3qH78-Lc"
      },
      "source": [
        "# Metric matthews_corrcoef\n",
        "\n",
        "mcc_y_test_predict = matthews_corrcoef(y_test, y_pred)\n",
        "mcc_y_test_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz3p8ceyLK4F"
      },
      "source": [
        "corpus_ml_dt['cdickens_achristmascarol'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLSKeVWiFV3A"
      },
      "source": [
        "for i, acorpus in enumerate(corpora_ls):\n",
        "  print(f'Corpus #{i}: {acorpus}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WqvNvASE-Zo"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "# Option (a): SA Timeseries -> (z-Score Standardization) -> (SMA 10%)\n",
        "\n",
        "# Rolling z-score: https://stackoverflow.com/questions/47164950/compute-rolling-z-score-in-pandas-dataframe\n",
        "\n",
        "def zscore(x, window):\n",
        "    r = x.rolling(window=window)\n",
        "    m = r.mean().shift(1)\n",
        "    s = r.std(ddof=0).shift(1)\n",
        "    z = (x-m)/s\n",
        "    return z\n",
        "\n",
        "# df['zscore'] = zscore(df['value'],window)\n",
        "\n",
        "# For each individual corpus in our collection\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "\n",
        "  # Apply each ML Model to compute sentiment values on each sentence\n",
        "  for j, amodel in enumerate(models_ml_ls):\n",
        "\n",
        "    # Vectorize Corpus ----------\n",
        "    best_emb_type = best_emb_dt[amodel] \n",
        "\n",
        "    # Select the best embedding technique on IMDB to vectorize Corpus\n",
        "    if best_emb_type == 'count':\n",
        "      # CountVectorizer on Corpus\n",
        "      X_corpus_emb =  count_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf':\n",
        "      # TF-IDF (Words) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf_ngram':\n",
        "      # TF-IDF (ngrams) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect_ngram.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf_ngram_chars':\n",
        "      # TF-IDF (chars) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect_ngram_chars.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'hash':\n",
        "      # TF-IDF (chars) on Corpus\n",
        "      X_corpus_emb =  hash_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    else:\n",
        "      # ERROR\n",
        "      print(f'ERROR: Illegal value for best embedding technique (best_emb_type): {best_emb_type}')\n",
        "\n",
        "    # Predict Sentiments ----------\n",
        "    corpus_ml_dt[acorpus][amodel] =  pd.Series(predict_cls(amodel, X_corpus_emb))\n",
        "\n",
        "    # Standardize and create Simple Moving Average (SMA 10%)\n",
        "    print(f'\\n\\nPlot #{i*len(models_ml_ls) + j}: {acorpus} with model {amodel}')\n",
        "    win10per = int(corpus_ml_dt[acorpus].shape[0]*0.1)\n",
        "    # del temp_df\n",
        "    # temp_df = pd.DataFrame()\n",
        "    # temp_df['zscore'] = zscore(corpus_ml_dt[acorpus][amodel],win10per)\n",
        "    # corpus_ml_dt[acorpus][amodel].rolling(win10per, center=True).mean().plot();\n",
        "    \n",
        "    float_array = corpus_ml_dt[acorpus][amodel].values.astype(float)\n",
        "    amodel_stdscaler = f'{amodel}_stdscaler'\n",
        "    corpus_ml_dt[acorpus][amodel_stdscaler] = mean_std_scaler.fit_transform(float_array.reshape(-1,1))\n",
        "    # temp_df['zscore'].rolling(win10per, center=True).mean().plot()\n",
        "    # zscore(corpus_ml_dt[acorpus][amodel],win10per).rolling(win10per, center=True).mean().plot()\n",
        "    corpus_ml_dt[acorpus][amodel_stdscaler].rolling(win10per, center=True).mean().plot()\n",
        "    plt.title(f'Corpus: {acorpus} with Model: {amodel}\\nz-Score Standardardized SMA 10%');\n",
        "    plt.legend(loc='best')  \n",
        "  plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGauv7w5E-TX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JDRHKiNE-Nl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g932RCX3-d1B"
      },
      "source": [
        "\"\"\"\n",
        "%%time\n",
        "\n",
        "# NOTE: 1m08s\n",
        "\n",
        "# Random Forests\n",
        "\n",
        "clf_flaml = automl.predict() # XGBClassifier()\n",
        "clf_flaml.fit(X_train, y_train)\n",
        "\n",
        "# Get Metrics on Fitting IMDB\n",
        "get_metrics(clf_flaml, X_test, y_test)\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEggV-ht_n-8"
      },
      "source": [
        "## **HyperOpt-Sklearn**\n",
        "\n",
        "* https://machinelearningmastery.com/hyperopt-for-automated-machine-learning-with-scikit-learn/ \n",
        "\n",
        "* https://github.com/hyperopt/hyperopt-sklearn\n",
        "\n",
        "* https://github.com/hyperopt/hyperopt-sklearn/blob/master/notebooks/Demo-Iris.ipynb \n",
        "\n",
        "svc\n",
        "svc_linear\n",
        "svc_rbf\n",
        "svc_poly\n",
        "svc_sigmoid\n",
        "liblinear_svc\n",
        "\n",
        "knn\n",
        "\n",
        "ada_boost\n",
        "gradient_boosting\n",
        "\n",
        "random_forest\n",
        "extra_trees\n",
        "decision_tree\n",
        "\n",
        "sgd\n",
        "\n",
        "xgboost_classification\n",
        "\n",
        "multinomial_nb\n",
        "gaussian_nb\n",
        "\n",
        "passive_aggressive\n",
        "\n",
        "linear_discriminant_analysis\n",
        "quadratic_discriminant_analysis\n",
        "\n",
        "one_vs_rest\n",
        "one_vs_one\n",
        "output_code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5S-FIpT_rdc"
      },
      "source": [
        "!pip install hyperopt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKZj2n2XATnI"
      },
      "source": [
        "!pip show hyperopt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNqykhi9AThS"
      },
      "source": [
        "!pip install git+https://github.com/hyperopt/hyperopt-sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq1rqFXJntKE"
      },
      "source": [
        "!ls -d */"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RHN9MlnngXG"
      },
      "source": [
        "!ls -d */\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlautdoQncrT"
      },
      "source": [
        "%cd hyperopt-sklearn\n",
        "\n",
        "!sudo pip install .\n",
        "\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNSD6ti_ATeb"
      },
      "source": [
        "!pip show hpsklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P30-bkSboQEn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9MKjLEUATZY"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "# example of hyperopt-sklearn for the sonar classification dataset\n",
        "# https://machinelearningmastery.com/hyperopt-for-automated-machine-learning-with-scikit-learn/\n",
        "\n",
        "# from pandas import read_csv\n",
        "# from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from hpsklearn import HyperoptEstimator\n",
        "from hpsklearn import any_classifier\n",
        "from hpsklearn import any_preprocessing\n",
        "\n",
        "from hyperopt import tpe\n",
        "\n",
        "\"\"\"\n",
        "# load dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "\n",
        "# split into input and output elements\n",
        "data = dataframe.values\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "\n",
        "# minimally prepare dataset\n",
        "X = X.astype('float32')\n",
        "y = LabelEncoder().fit_transform(y.astype('str'))\n",
        "\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "\"\"\";\n",
        "\n",
        "# define search\n",
        "model = HyperoptEstimator(classifier=any_classifier('cla'), preprocessing=any_preprocessing('pre'), algo=tpe.suggest, max_evals=50, trial_timeout=30)\n",
        "\n",
        "# perform the search\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYLvzBf_ox-X"
      },
      "source": [
        "from hpsklearn import HyperoptEstimator, svc\n",
        "from sklearn import svm\n",
        "\n",
        "# Load Data\n",
        "# ...\n",
        "\n",
        "use_hpsklearn = True\n",
        "\n",
        "if use_hpsklearn:\n",
        "    estim = HyperoptEstimator(classifier=svc('mySVC'))\n",
        "else:\n",
        "    estim = svm.SVC()\n",
        "\n",
        "estim.fit(X_train, y_train)\n",
        "\n",
        "print(estim.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lp3fOXLBp1e"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "# summarize performance\n",
        "acc = model.score(X_test, y_test)\n",
        "print(\"Accuracy: %.3f\" % acc)\n",
        "\n",
        "# summarize the best model\n",
        "print(model.best_model())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFYzG-D6pwDQ"
      },
      "source": [
        "## **AutoGluon Text**\n",
        "\n",
        "References: \n",
        "\n",
        "* https://auto.gluon.ai/stable/tutorials/text_prediction/beginner.html\n",
        "\n",
        "* https://auto.gluon.ai/tutorials/text_classification/beginner.html\n",
        "\n",
        "* https://linuxtut.com/en/22cf80a4be80f7ad458e/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m6QXbaEp-Kd"
      },
      "source": [
        "!pip install gluonnlp==0.8.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjs_SGilp9_U"
      },
      "source": [
        "!pip install autogluon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3b90Jhep95t"
      },
      "source": [
        "# !pip install mxnet\n",
        "\n",
        "!pip install --upgrade mxnet-cu100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iBncwpE_OI3"
      },
      "source": [
        "training_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEHfIbFu_S8z"
      },
      "source": [
        "training_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7SV6_4c_c5L"
      },
      "source": [
        "# Manually create train/test split from training_df DataFrame\n",
        "\n",
        "train_per = 0.9 # 80% with large dataset 50k labeled sentiments\n",
        "\n",
        "split_idx = int(train_per*training_df.shape[0])\n",
        "train_data = training_df[:split_idx]\n",
        "print(f'train_data.shape: {train_data.shape}')\n",
        "test_data = training_df[split_idx:]\n",
        "print(f'test_data.shape: {test_data.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiKt0dmrAZnn"
      },
      "source": [
        "train_data.head()\n",
        "train_data.drop(columns=['sent_no','text_raw'], axis=1, inplace=True)\n",
        "train_data.rename(columns={'text_clean':'sentence', 'polarity':'label'}, inplace=True)\n",
        "train_data['sentence'] = train_data['sentence'].astype('string')\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6AGhf4jAyY4"
      },
      "source": [
        "test_data.head()\n",
        "test_data.drop(columns=['sent_no','text_raw'], axis=1, inplace=True)\n",
        "test_data.rename(columns={'text_clean':'sentence', 'polarity':'label'}, inplace=True)\n",
        "test_data['sentence'] = test_data['sentence'].astype('string')\n",
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdyDIJtG9-uX"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "from autogluon.core.utils.loaders.load_pd import load\n",
        "\n",
        "# train_data = load('https://autogluon-text.s3-accelerate.amazonaws.com/glue/sst/train.parquet')\n",
        "# test_data = load('https://autogluon-text.s3-accelerate.amazonaws.com/glue/sst/dev.parquet')\n",
        "\n",
        "# subsample_size = 1000  # subsample data for faster demo, try setting this to larger values\n",
        "\n",
        "# train_data = train_data.sample(n=subsample_size, random_state=0)\n",
        "train_data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkQiID_A-F2Z"
      },
      "source": [
        "type(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq7DBncwBUSm"
      },
      "source": [
        "!ls -d */"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNzEtzGN-Fy6"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 11m01s on 20210919 at 18:00 (train_max = 60) \n",
        "#       [Iter 66/3300, Epoch 0] valid f1=8.9398e-01, \n",
        "#        mcc=7.8881e-01, roc_auc=9.5905e-01, accuracy=8.9440e-01, \n",
        "#        log_loss=2.6857e-01, time spent=57.956s, total time spent=10.11min. \n",
        "#        Find new best=True, Find new top-3=True\n",
        "\n",
        "train_max = 60 # in seconds\n",
        "from autogluon.text import TextPredictor\n",
        "\n",
        "predictor = TextPredictor(label='label', eval_metric='acc', path='./ag_imdb')\n",
        "predictor.fit(train_data, time_limit=train_max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olw4e2J--Fuf"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "test_score = predictor.evaluate(test_data)\n",
        "print('Accuracy = {:.2f}%'.format(test_score * 100))\n",
        "\n",
        "# Demo: Accuracy = 87.50%\n",
        "# IMDB: "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCIs_F_9-S3C"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 1m58s\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "test_score = predictor.evaluate(test_data, metrics=['acc', 'f1'])\n",
        "print(test_score)\n",
        "\n",
        "# Demo: {'acc': 0.875, 'f1': 0.8816503800217155}\n",
        "# IMDB: "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imjG9CIQ-SyY"
      },
      "source": [
        "sentence1 = \"it's a charming and often affecting journey.\"\n",
        "sentence2 = \"It's slow, very, very, very slow.\"\n",
        "predictions = predictor.predict({'sentence': [sentence1, sentence2]})\n",
        "print('\"Sentence\":', sentence1, '\"Predicted Sentiment\":', predictions[0])\n",
        "print('\"Sentence\":', sentence2, '\"Predicted Sentiment\":', predictions[1])\n",
        "\n",
        "\"\"\"\n",
        "Demo:\n",
        "\"Sentence\": it's a charming and often affecting journey. \"Predicted Sentiment\": 1\n",
        "\"Sentence\": It's slow, very, very, very slow. \"Predicted Sentiment\": 0\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSLYm8o39-qB"
      },
      "source": [
        "corpus_ls = corpora_dt['cdickens_achristmascarol']['sent_clean'].tolist()\n",
        "type(corpus_ls)\n",
        "print('\\n')\n",
        "print(corpus_ls[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNrk3ZbG9-lH"
      },
      "source": [
        "%%time\n",
        "\n",
        "corpus_pred = predictor.predict({'sentence':corpus_ls})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dNAdtu8LNm1"
      },
      "source": [
        "corpus_ml_dt['cdickens_achristmascarol'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjZbYupnLJXx"
      },
      "source": [
        "type(corpus_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cLRkN46I7dR"
      },
      "source": [
        "for i in range(5):\n",
        "  print(f'{corpus_pred[i]}: {corpus_ls[i]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1odBsRzCI7Z0"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "amodel = 'autogluon'\n",
        "\n",
        "# For each individual corpus in our collection\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "\n",
        "  print(f'\\nCorpus #{i}: {acorpus} processing...')\n",
        "\n",
        "  # Create a list of Sentences for each Corpus\n",
        "  corpus_ls = corpus_ml_dt[acorpus]['sent_clean'].tolist()\n",
        "\n",
        "  # Predict Sentiments for each Sentence\n",
        "  corpus_pred = predictor.predict({'sentence':corpus_ls})\n",
        "\n",
        "  # Save AutoGluon Sentiment \n",
        "  corpus_ml_dt[acorpus][amodel] = pd.Series(corpus_pred)\n",
        "\n",
        "  # Standardize and create Simple Moving Average (SMA 10%)\n",
        "  print(f'\\n\\nPlot #{i}: {acorpus} with model {amodel}')\n",
        "  win10per = int(corpus_ml_dt[acorpus].shape[0]*0.1)\n",
        "  # del temp_df\n",
        "  # temp_df = pd.DataFrame()\n",
        "  # temp_df['zscore'] = zscore(corpus_ml_dt[acorpus][amodel],win10per)\n",
        "  # corpus_ml_dt[acorpus][amodel].rolling(win10per, center=True).mean().plot();\n",
        "  \n",
        "  float_array = corpus_ml_dt[acorpus][amodel].values.astype(float)\n",
        "  amodel_stdscaler = f'{amodel}_stdscaler'\n",
        "  corpus_ml_dt[acorpus][amodel_stdscaler] = mean_std_scaler.fit_transform(float_array.reshape(-1,1))\n",
        "  # temp_df['zscore'].rolling(win10per, center=True).mean().plot()\n",
        "  # zscore(corpus_ml_dt[acorpus][amodel],win10per).rolling(win10per, center=True).mean().plot()\n",
        "  corpus_ml_dt[acorpus][amodel_stdscaler].rolling(win10per, center=True).mean().plot()\n",
        "  plt.title(f'Corpus: {acorpus} with Model: {amodel}\\nz-Score Standardardized SMA 10%');\n",
        "  plt.legend(loc='best')  \n",
        "  plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEBMrW_sNHN5"
      },
      "source": [
        "# Verify new model sentiment values appended to DataFrames\n",
        "\n",
        "corpus_ml_dt['cdickens_achristmascarol'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI0pckx-I7Vp"
      },
      "source": [
        "# Save DataFrame with new model sentiment values\n",
        "\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "\n",
        "  data_dir = 'data_corpora_sa'\n",
        "  model_file = f'./{data_dir}/models_ml_{acorpus}.csv'\n",
        "  print(f'\\n\\nDataFrame #{i}: [{acorpus}]\\n')\n",
        "  corpus_ml_dt[acorpus].head()\n",
        "  \n",
        "  print(f'\\n    Saving contents to file: [{model_file}]\\n\\n')\n",
        "  corpus_ml_dt[acorpus].to_csv(model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyAH1mh-N833"
      },
      "source": [
        "## **ONYX Transformers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhFH-XFnPF5h"
      },
      "source": [
        "!pip install -qqq --upgrade torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNxv7I6XPFz6"
      },
      "source": [
        "!git clone https://github.com/patil-suraj/onnx_transformers.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6tpVsGlOPJd"
      },
      "source": [
        "!pip install -U -qqq -e ./onnx_transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng45742MPbsf"
      },
      "source": [
        "!lscpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HlPXG-UOD4n"
      },
      "source": [
        "from onnx_transformers import pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOupnKYUOArb"
      },
      "source": [
        "# Initialize a pipeline by passing the task name and \n",
        "# set onnx to True (default value is also True)\n",
        "\n",
        "nlp = pipeline(\"sentiment-analysis\", onnx=True)\n",
        "nlp(\"Transformers and onnx runtime is an awesome combo!\")\n",
        "\n",
        "# [{'label': 'POSITIVE', 'score': 0.999721109867096}]  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9LPKzEVOAnN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KJPrxH49b93"
      },
      "source": [
        "## **Inferring Sentiment with Trained Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVfg9jatkJVy"
      },
      "source": [
        "corpora_dt['cdickens_achristmascarol']['sent_clean']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFzqY_JZ9g84"
      },
      "source": [
        "corpus_automl_dt = {}\n",
        "\n",
        "for acorpus in corpora_ls:\n",
        "  corpus_automl_dt[acorpus] = pd.DataFrame(corpora_dt['cdickens_achristmascarol']['sent_clean'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7Pvfmkc9g84"
      },
      "source": [
        "# Create Dictionary with one DataFrame for each corpus\n",
        "#   The columns of DataFrames hold model sentiment values computed for each sentence\n",
        "\n",
        "for key, value in corpus_automl_dt.items():\n",
        "  print(f'\\n\\nCorpus: {key}\\n{value.head()}')\n",
        "\n",
        "# corpus_automl_dt.keys()\n",
        "# corpus_automl_dt.values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISaIg-OK9g84"
      },
      "source": [
        "def predict_cls(amodel, X_corpus_emb):\n",
        "  '''\n",
        "  Given a ML model and an embedded corpus\n",
        "  Return a pd.Series of Sentiment Values\n",
        "  '''\n",
        "\n",
        "  if amodel == 'flaml':\n",
        "    # return pd.Series(clf_flaml.predict(X_corpus_emb))\n",
        "    return pd.Series(automl.predict(X_corpus_emb))\n",
        "  elif amodel == 'logreg':\n",
        "    return pd.Series(clf_logreg.predict(X_corpus_emb))\n",
        "  elif amodel == 'logreg_cv':\n",
        "    return pd.Series(clf_logreg_cv.predict(X_corpus_emb))\n",
        "  elif amodel == 'rf':\n",
        "    return pd.Series(clf_rf.predict(X_corpus_emb))\n",
        "  elif amodel == 'xgb':\n",
        "    return pd.Series(clf_xgb.predict(X_corpus_emb))\n",
        "  else:\n",
        "    print(f'ERROR: In predict_cls() with invalid amodel argument: {amodel}')\n",
        "    return -99"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFMWc97v9g84"
      },
      "source": [
        "# Reminder: (defined above) models_ml_ls = ['multinb', 'logreg', 'logreg_cv', 'rf', 'xgb']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2iou9dm9g84"
      },
      "source": [
        "corpus_ml_dt['fbaum_thewonderfulwizardofoz'].head(10)\n",
        "corpus_ml_dt['fbaum_thewonderfulwizardofoz'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8Z6MaFY9g85"
      },
      "source": [
        "corpus_ml_dt['cdickens_achristmascarol'].head(10)\n",
        "corpus_ml_dt['cdickens_achristmascarol'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDX1CTy99g85"
      },
      "source": [
        "corpus_ml_dt['cdickens_achristmascarol'].rolling(340, center=True).mean().plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAd72pA59g85"
      },
      "source": [
        "corpus_ml_dt['cdickens_greatexpectations'].rolling(340, center=True).mean().plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwINFYN49g85"
      },
      "source": [
        "corpus_ml_dt['cdickens_achristmascarol']['logreg'].rolling(130, center=True).mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjlWF74X9g85"
      },
      "source": [
        "corpora_dt['fbaum_thewonderfulwizardofoz']['sent_clean']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yQjZtdG9g85"
      },
      "source": [
        "corpora_dt['cdickens_achristmascarol']['sent_clean']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hddpt0nq9g85"
      },
      "source": [
        "del best_emb_dt['xgboost']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n88WpCOx9g85"
      },
      "source": [
        "best_emb_dt['flaml'] = 'tfidf_ngram'\n",
        "best_emb_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoDYrkCnkqbj"
      },
      "source": [
        "automl_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3Q6fGPH9g85"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "# Option (a): SA Timeseries -> (z-Score Standardization) -> (SMA 10%)\n",
        "\n",
        "# Rolling z-score: https://stackoverflow.com/questions/47164950/compute-rolling-z-score-in-pandas-dataframe\n",
        "\n",
        "def zscore(x, window):\n",
        "    r = x.rolling(window=window)\n",
        "    m = r.mean().shift(1)\n",
        "    s = r.std(ddof=0).shift(1)\n",
        "    z = (x-m)/s\n",
        "    return z\n",
        "\n",
        "# df['zscore'] = zscore(df['value'],window)\n",
        "\n",
        "# For each individual corpus in our collection\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "\n",
        "  # Apply each ML Model to compute sentiment values on each sentence\n",
        "  for j, amodel in enumerate(automl_ls):\n",
        "\n",
        "    # Vectorize Corpus ----------\n",
        "\n",
        "    best_emb_type = best_emb_dt[amodel] \n",
        "\n",
        "    # Select the best embedding technique on IMDB to vectorize Corpus\n",
        "    if best_emb_type == 'count':\n",
        "      # CountVectorizer on Corpus\n",
        "      X_corpus_emb =  count_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf':\n",
        "      # TF-IDF (Words) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf_ngram':\n",
        "      # TF-IDF (ngrams) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect_ngram.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'tfidf_ngram_chars':\n",
        "      # TF-IDF (chars) on Corpus\n",
        "      X_corpus_emb =  tfidf_vect_ngram_chars.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    elif best_emb_type == 'hash':\n",
        "      # TF-IDF (chars) on Corpus\n",
        "      X_corpus_emb =  hash_vect.transform(corpora_dt[acorpus]['sent_clean'])\n",
        "\n",
        "    else:\n",
        "      # ERROR\n",
        "      print(f'ERROR: Illegal value for best embedding technique (best_emb_type): {best_emb_type}')\n",
        "\n",
        "    # Predict Sentiments ----------\n",
        "    corpus_ml_dt[acorpus][amodel] =  pd.Series(predict_cls(amodel, X_corpus_emb))\n",
        "\n",
        "    # Standardize and create Simple Moving Average (SMA 10%)\n",
        "    print(f'\\n\\nPlot #{i*len(models_ml_ls) + j}: {acorpus} with model {amodel}')\n",
        "    win10per = int(corpus_ml_dt[acorpus].shape[0]*0.1)\n",
        "    # del temp_df\n",
        "    # temp_df = pd.DataFrame()\n",
        "    # temp_df['zscore'] = zscore(corpus_ml_dt[acorpus][amodel],win10per)\n",
        "    # corpus_ml_dt[acorpus][amodel].rolling(win10per, center=True).mean().plot();\n",
        "    \n",
        "    float_array = corpus_ml_dt[acorpus][amodel].values.astype(float)\n",
        "    amodel_stdscaler = f'{amodel}_stdscaler'\n",
        "    corpus_ml_dt[acorpus][amodel_stdscaler] = mean_std_scaler.fit_transform(float_array.reshape(-1,1))\n",
        "    # temp_df['zscore'].rolling(win10per, center=True).mean().plot()\n",
        "    # zscore(corpus_ml_dt[acorpus][amodel],win10per).rolling(win10per, center=True).mean().plot()\n",
        "    corpus_ml_dt[acorpus][amodel_stdscaler].rolling(win10per, center=True).mean().plot()\n",
        "    plt.title(f'Corpus: {acorpus} with Model: {amodel}\\nz-Score Standardardized SMA 10%');\n",
        "    plt.legend(loc='best')  \n",
        "  plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai2KAdlPmZ6-"
      },
      "source": [
        "corpus_ml_dt['cdickens_achristmascarol'].iloc[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OjBAg9tmOz1"
      },
      "source": [
        "# For each individual corpus in our collection\n",
        "for i, acorpus in enumerate(corpora_ls):\n",
        "\n",
        "  data_dir = 'data_corpora_sa'\n",
        "  model_file = f'./{data_dir}/models_ml_{acorpus}.csv'\n",
        "  print(f'\\n\\nDataFrame #{i*len(models_ml_ls) + j}: [{acorpus}]\\n')\n",
        "  corpus_ml_dt[acorpus].head()\n",
        "  \n",
        "  print(f'\\n    Saving contents to file: [{model_file}]\\n\\n')\n",
        "  corpus_ml_dt[acorpus].to_csv(model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrHfF-nbYr4U"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xi7N5MmmOuq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0Xf92Ue8-Bt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7IjZ7_4aXvk"
      },
      "source": [
        "\n",
        "X_train, y_train = load_iris(return_X_y=True)\n",
        "# Train with labeled input data\n",
        "automl.fit(X_train=X_train, y_train=y_train,\n",
        "           **automl_settings)\n",
        "# Predict\n",
        "print(automl.predict_proba(X_train))\n",
        "# Export the best model\n",
        "print(automl.model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9ayZrGpaXqg"
      },
      "source": [
        "y_train.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnnMnBa6aXnF"
      },
      "source": [
        "settings = {\n",
        "    \"time_budget\": 1000,  # total running time in seconds\n",
        "    \"metric\": 'accuracy',  # primary metrics can be chosen from: ['accuracy','roc_auc','f1','log_loss','mae','mse','r2']\n",
        "    \"task\": 'classification',  # task type    \n",
        "    \"log_file_name\": 'airlines_experiment.log',  # flaml log file\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQstaiBNaXh-"
      },
      "source": [
        "'''The main flaml automl API'''\n",
        "automl.fit(X_train=X_train, y_train=y_train, **settings)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "[flaml.automl: 08-18 05:53:11] {1461} INFO - selected model: LGBMClassifier(colsample_bytree=0.7733707792852584,\n",
        "               learning_rate=0.11190988982157068, max_bin=128,\n",
        "               min_child_samples=62, n_estimators=701, num_leaves=12,\n",
        "               objective='binary', reg_alpha=0.001291764523034099,\n",
        "               reg_lambda=0.5058442385321611, verbose=-1)\n",
        "[flaml.automl: 08-18 05:53:11] {1184} INFO - fit succeeded\n",
        "[flaml.automl: 08-18 05:53:11] {1185} INFO - Time taken to find the best model: 567.884330034256\n",
        "\n",
        "\n",
        "[flaml.automl: 08-18 05:31:22] {1411} INFO -  at 291.7s,\tbest extra_tree's error=0.2469,\tbest lgbm's error=0.1504\n",
        "[flaml.automl: 08-18 05:31:27] {1438} INFO - retrain extra_tree for 5.0s\n",
        "[flaml.automl: 08-18 05:31:27] {1253} INFO - iteration 46, current learner lrl1\n",
        "No low-cost partial config given to the search algorithm. For cost-frugal search, consider providing low-cost values for cost-related hps via 'low_cost_partial_config'.\n",
        "[flaml.automl: 08-18 05:31:41] {1411} INFO -  at 310.7s,\tbest lrl1's error=0.1454,\tbest lrl1's error=0.1454\n",
        "[flaml.automl: 08-18 05:31:41] {1461} INFO - selected model: LogisticRegression(n_jobs=-1, penalty='l1', solver='saga')\n",
        "[flaml.automl: 08-18 05:31:41] {1184} INFO - fit succeeded\n",
        "[flaml.automl: 08-18 05:31:41] {1185} INFO - Time taken to find the best model: 310.6792550086975\n",
        "[flaml.automl: 08-18 05:31:41] {1191} WARNING - Time taken to find the best model is 104% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHtZnVcyaXeW"
      },
      "source": [
        "''' retrieve best config and best learner'''\n",
        "print('Best ML leaner:', automl.best_estimator)\n",
        "print('Best hyperparmeter config:', automl.best_config)\n",
        "print('Best accuracy on validation data: {0:.4g}'.format(1-automl.best_loss))\n",
        "print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "retrieve best config and best learner\n",
        "\n",
        "Best ML leaner: lgbm\n",
        "Best hyperparmeter config: {'n_estimators': 701, 'num_leaves': 12, 'min_child_samples': 62, 'learning_rate': 0.11190988982157068, 'subsample': 1.0, 'log_max_bin': 8, 'colsample_bytree': 0.7733707792852584, 'reg_alpha': 0.001291764523034099, 'reg_lambda': 0.5058442385321611}\n",
        "Best accuracy on validation data: 0.8516\n",
        "Training duration of best run: 14.68 s\n",
        "\n",
        "Best ML leaner: lrl1\n",
        "Best hyperparmeter config: {'C': 1.0}\n",
        "Best accuracy on validation data: 0.8546\n",
        "Training duration of best run: 13.9 s\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWOE9p38aexX"
      },
      "source": [
        "automl.model.estimator\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "LogisticRegression(n_jobs=-1, penalty='l1', solver='saga')\n",
        "\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNwqiSY6aet5"
      },
      "source": [
        "''' compute predictions of testing dataset ''' \n",
        "y_pred = automl.predict(X_test)\n",
        "print('Predicted labels', y_pred)\n",
        "print('True labels', y_test)\n",
        "y_pred_proba = automl.predict_proba(X_test)[:,1]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEJJbsjwai8j"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "mcc_y_test_predict = matthews_corrcoef(y_test, y_pred)\n",
        "mcc_y_test_predict\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "0.7274299237200138\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7VwVVzD2FKL"
      },
      "source": [
        "## **SGD Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAsWEqJc2eLQ"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ah8NP4T5LA4"
      },
      "source": [
        "clf= SGDClassifier()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7hQW524119b"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: on 20210918 at 12:59 (IMDB)\n",
        "\n",
        "clf= SGDClassifier(loss='hinge', \n",
        "                   penalty='l2', \n",
        "                   alpha=0.0001, \n",
        "                   l1_ratio=0.15, \n",
        "                   fit_intercept=True,\n",
        "                   max_iter=1000, \n",
        "                   tol=0.001, \n",
        "                   shuffle=True, \n",
        "                   verbose=0, \n",
        "                   epsilon=0.1, \n",
        "                   n_jobs=None, \n",
        "                   random_state=42, \n",
        "                   learning_rate='optimal', \n",
        "                   eta0=0.0, \n",
        "                   power_t=0.5, \n",
        "                   early_stopping=False, \n",
        "                   validation_fraction=0.1, \n",
        "                   n_iter_no_change=5, \n",
        "                   class_weight=None, \n",
        "                   warm_start=False, \n",
        "                   average=False)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW1lcIET2Imt"
      },
      "source": [
        "get_metrics(clf, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2SG87Tc2Ihx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJp6zHHl2Ie5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LhbbhiU2Ib3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttbyxoARZpbW"
      },
      "source": [
        "pos_tweets = [('I love this car', 'positive'),\n",
        "    ('This view is amazing', 'positive'),\n",
        "    ('I feel great this morning', 'positive'),\n",
        "    ('I am so excited about the concert', 'positive'),\n",
        "    ('He is my best friend', 'positive')]\n",
        "\n",
        "neg_tweets = [('I do not like this car', 'negative'),\n",
        "    ('This view is horrible', 'negative'),\n",
        "    ('I feel tired this morning', 'negative'),\n",
        "    ('I am not looking forward to the concert', 'negative'),\n",
        "    ('He is my enemy', 'negative')]\n",
        "\n",
        "test_tweets = [\n",
        "    ('feel happy this morning', 'positive'),\n",
        "    ('larry is my friend', 'positive'),\n",
        "    ('I do not like that man', 'negative'),\n",
        "    ('house is not great', 'negative'),\n",
        "    ('your song is annoying', 'negative')]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpAqSj9EZpbX"
      },
      "source": [
        "dat = []\n",
        "for i in pos_tweets+neg_tweets+test_tweets:\n",
        "    dat.append(i)\n",
        "    \n",
        "X = np.array(dat).T[0]\n",
        "y = np.array(dat).T[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y70WWuE-ZpbY"
      },
      "source": [
        "# TfidfVectorizer?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egx5jku-Zpba"
      },
      "source": [
        "vec = TfidfVectorizer(stop_words='english', ngram_range = (1, 1), lowercase = True)\n",
        "X_vec = vec.fit_transform(X)\n",
        "Xtrain = X_vec[:10]\n",
        "Xtest = X_vec[10:]\n",
        "ytrain = y[:10]\n",
        "ytest= y[10:] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV4bat5XZpbb"
      },
      "source": [
        "pd.DataFrame(X_vec.toarray(), columns=vec.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d0L7nWVZpbe"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB # 1. choose model class\n",
        "\n",
        "model = GaussianNB()                       # 2. instantiate model\n",
        "model.fit(Xtrain.toarray(), ytrain)                  # 3. fit model to data\n",
        "y_model = model.predict(Xtest.toarray())              # 4. predict on new data\n",
        "y_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sley7t3qZpbg"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(ytest, y_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5NamgBzZpbg"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svc=SVC(kernel='rbf', gamma=1) # 超级参数 \n",
        "svc.fit(Xtrain.toarray(), ytrain)                  # 3. fit model to data\n",
        "y_model=svc.predict(Xtest.toarray())\n",
        "accuracy_score(ytest, y_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XgIJnOZZpbh"
      },
      "source": [
        "y_model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kaiz6M66Zpbh"
      },
      "source": [
        "y_model=svc.predict(Xtest.toarray())\n",
        "y_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nac5yddVZpbh"
      },
      "source": [
        "# Don’t be too positive, let’s try another example:\n",
        "\n",
        "vocabulary = vec.get_feature_names()\n",
        "\n",
        "def classify_sentiment(str_list, model, vocabulary):\n",
        "    # str_list = ['a str']\n",
        "    vec_pred = TfidfVectorizer(stop_words='english', ngram_range = (1, 1), lowercase = True, vocabulary = vocabulary)\n",
        "    return model.predict(vec_pred.fit_transform(str_list).toarray())\n",
        "\n",
        "classify_sentiment(['Your song is annoying','larry is horrible'], model, vocabulary)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awBhytADZpbi"
      },
      "source": [
        "classify_sentiment(['I do not like larry', 'larry is my friend'], svc, vocabulary)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVX1twkgZpbj"
      },
      "source": [
        "作业\n",
        "\n",
        "- 使用另外一种sklearn的分类器来对tweet_negative2进行情感分析\n",
        "\n",
        "- 使用https://github.com/victorneo/Twitter-Sentimental-Analysis 所提供的推特数据进行情感分析，可以使用其代码 https://github.com/victorneo/Twitter-Sentimental-Analysis/blob/master/classification.py\n",
        "\n",
        "- Sentiment Analysis of IMDb movie review Dataset Using Sklearn https://nbviewer.jupyter.org/github/rasbt/python-machine-learning-book/blob/master/code/ch08/ch08.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQJo_LNlZpbj"
      },
      "source": [
        "## PaddlePaddle\n",
        " \n",
        "<div><img src=\"https://github.com/chengjun/mybook/blob/main/images/paddlepaddle.png?raw=1\" align=\"right\"></div>  飞桨（PaddlePaddle）以百度多年的深度学习技术研究和业务应用为基础，集深度学习核心框架、基础模型库、端到端开发套件、工具组件和服务平台于一体，2016 年正式开源，是全面开源开放、技术领先、功能完备的产业级深度学习平台。 \n",
        "\n",
        "http://paddlepaddle.org\n",
        "\n",
        "https://github.com/PaddlePaddle/book/tree/develop/06.understand_sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2LjrYD8Zpbk"
      },
      "source": [
        "## Turicreate\n",
        "\n",
        "https://github.com/apple/turicreate\n",
        "\n",
        "<div><img src=\"https://github.com/chengjun/mybook/blob/main/images/turicreate.png?raw=1\" align = \"right\"></div>\n",
        "Turi Create simplifies the development of custom machine learning models. You don't have to be a machine learning expert to add recommendations, object detection, image classification, image similarity or activity classification to your app.\n",
        "\n",
        "\n",
        "https://apple.github.io/turicreate/docs/userguide/text_classifier/\n",
        "\n",
        "https://www.kaggle.com/prakharrathi25/updated-turicreate-sentiment-analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAASEzkxZpbk"
      },
      "source": [
        "![image.png](https://github.com/chengjun/mybook/blob/main/images/end.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFnwy3l1Zpbl"
      },
      "source": [
        "## Creating Sentiment Classifier with Turicreate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPdJ7itSZpbl"
      },
      "source": [
        "In this notebook, I will explain how to develop sentiment analysis classifiers that are based on a bag-of-words model. \n",
        "Then, I will demonstrate how these classifiers can be utilized to solve Kaggle's \"When Bag of Words Meets Bags of Popcorn\" challenge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fLXfmOLZpbm"
      },
      "source": [
        "Using <del>GraphLab</del> Turicreate it is very easy and straight foward to create a sentiment classifier based on bag-of-words model. Given a dataset stored as a CSV file, you can construct your sentiment classifier using the following code: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwFSCSiQZpbn"
      },
      "source": [
        "# toy code, do not run it\n",
        "\n",
        "import turicreate as tc\n",
        "\n",
        "train_data = tc.SFrame.read_csv(traindata_path,header=True, \n",
        "                                delimiter='\\t',quote_char='\"', \n",
        "                                column_type_hints = {'id':str, \n",
        "                                                     'sentiment' : int, \n",
        "                                                     'review':str } )\n",
        "train_data['1grams features'] = tc.text_analytics.count_ngrams(\n",
        "    train_data['review'],1)\n",
        "train_data['2grams features'] = tc.text_analytics.count_ngrams(\n",
        "    train_data['review'],2)\n",
        "\n",
        "cls = tc.classifier.create(train_data, target='sentiment', \n",
        "                           features=['1grams features',\n",
        "                                     '2grams features'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEjY-KebZpbo"
      },
      "source": [
        "In the rest of this notebook, we will explain this code recipe in details, by demonstrating how this recipe can used to create IMDB movie reviews sentiment classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KScHX-VAZpbo"
      },
      "source": [
        "Before we begin constructing the classifiers, we need to import some Python libraries: turicreate (tc), and IPython display utilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRM1S-jVZpbp"
      },
      "source": [
        "import turicreate as tc\n",
        "from IPython.display import display\n",
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etE1oVtJZpbp"
      },
      "source": [
        "### IMDB movies reviews Dataset \n",
        "\n",
        "> Bag of Words Meets Bags of Popcorn\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnm6Cv8cZpbp"
      },
      "source": [
        "Throughout this notebook, I will use Kaggle's IMDB movies reviews datasets that is available to download from the following link: https://www.kaggle.com/c/word2vec-nlp-tutorial/data. I downloaded labeledTrainData.tsv and testData.tsv files, and unzipped them to the following local files.\n",
        "\n",
        "###  DeepLearningMovies\n",
        "\n",
        "Kaggle's competition for using Google's word2vec package for sentiment analysis\n",
        "\n",
        "https://github.com/wendykan/DeepLearningMovies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AxcQHr3Zpbq"
      },
      "source": [
        "traindata_path = \"/Users/datalab/bigdata/cjc/kaggle_popcorn_data/labeledTrainData.tsv\"\n",
        "testdata_path = \"/Users/datalab/bigdata/cjc/kaggle_popcorn_data/testData.tsv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKq3m8OhZpbq"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjKpbuZkZpbq"
      },
      "source": [
        "We will load the data with IMDB movie reviews to an SFrame using SFrame.read_csv function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIizJqPDZpbr"
      },
      "source": [
        "movies_reviews_data = tc.SFrame.read_csv(traindata_path,header=True, \n",
        "                                         delimiter='\\t',quote_char='\"', \n",
        "                                         column_type_hints = {'id':str, \n",
        "                                                              'sentiment' : str, \n",
        "                                                              'review':str } )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeuB1ujzZpbr"
      },
      "source": [
        "By using the SFrame show function, we can visualize the data and notice that the train dataset consists of 12,500 positive and 12,500 negative, and overall 24,932 unique reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTgWmQeeZpbs"
      },
      "source": [
        "movies_reviews_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vabwS6juZpbs"
      },
      "source": [
        "### Constructing Bag-of-Words Classifier "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyrnnW4LZpbs"
      },
      "source": [
        "One of the common techniques to perform document classification (and reviews classification) is using Bag-of-Words model, in which the frequency of each word in the document is used as a feature for training a classifier. GraphLab's text analytics toolkit makes it easy to calculate the frequency of each word in each review. Namely, by using the count_ngrams function with n=1, we can calculate the frequency of each word in each review. By running the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00CE9JfvZpbs"
      },
      "source": [
        "movies_reviews_data['1grams features'] = tc.text_analytics.count_ngrams(movies_reviews_data ['review'],1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoVvLb3iZpbt"
      },
      "source": [
        "By running the last command, we created a new column in movies_reviews_data SFrame object. In this column each value is a dictionary object, where each dictionary's keys are the different words which appear in the corresponding review, and the dictionary's values are the frequency of each word.\n",
        "We can view the values of this new column using the following command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2oIGXLvZpbt"
      },
      "source": [
        "movies_reviews_data#[['review','1grams features']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKV1ApggZpbt"
      },
      "source": [
        "We are now ready to construct and evaluate the movie reviews sentiment classifier using the calculated above features. But first, to be able to perform a quick evaluation of the constructed classifier, we need to create labeled train and test datasets. We will create train and test datasets by randomly splitting the train dataset into two parts. The first part will contain 80% of the labeled train dataset and will be used as the training dataset, while the second part will contain 20% of the labeled train dataset and will be used as the testing dataset. We will create these two dataset by using the following command:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMQ9mNR-SnK6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwlvvCo0Zpbt"
      },
      "source": [
        "train_set, test_set = movies_reviews_data.random_split(0.8, seed=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD77i3x7Zpbu"
      },
      "source": [
        "We are now ready to create a classifier using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjZ2DS_GZpbu"
      },
      "source": [
        "model_1 = tc.classifier.create(train_set, target='sentiment', \\\n",
        "                               features=['1grams features'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKy4-ZtKZpbu"
      },
      "source": [
        "We can evaluate the performence of the classifier by evaluating it on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqTOtgdTZpbv"
      },
      "source": [
        "result1 = model_1.evaluate(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcsN_sCoZpbv"
      },
      "source": [
        "In order to get an easy view of the classifier's prediction result, we define and use the following function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HCEMHDuZpbv"
      },
      "source": [
        "def print_statistics(result):\n",
        "    print( \"*\" * 30)\n",
        "    print( \"Accuracy        : \", result[\"accuracy\"])\n",
        "    print( \"Confusion Matrix: \\n\", result[\"confusion_matrix\"])\n",
        "print_statistics(result1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3v5nXW5Zpbv"
      },
      "source": [
        "As can be seen in the results above, in just a few relatively straight foward lines of code, we have developed a sentiment classifier that has accuracy of about ~0.88. Next, we demonstrate how we can improve the classifier accuracy even more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c749tKCSZpbw"
      },
      "source": [
        "### Improving The Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyOJaT_wZpbw"
      },
      "source": [
        "One way to improve the movie reviews sentiment classifier is to extract more meaningful features from the reviews. One method to add additional features, which might be meaningful, is to calculate the frequency of every two consecutive words in each review. To calculate the frequency of each two consecutive words in each review, as before, we will use turicreate's count_ngrams function only this time we will set n to be equal 2 (n=2) to create new column named '2grams features'.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_2TL_T3Zpbw"
      },
      "source": [
        "movies_reviews_data['2grams features'] = tc.text_analytics.count_ngrams(movies_reviews_data['review'],2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOQ8XmVJZpbx"
      },
      "source": [
        "movies_reviews_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3huiGpZZpbx"
      },
      "source": [
        "As before, we will construct and evaluate a movie reviews sentiment classifier. However, this time we will use both the '1grams features' and the '2grams features' features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dhh3-9DTZpby"
      },
      "source": [
        "train_set, test_set = movies_reviews_data.random_split(0.8, seed=5)\n",
        "model_2 = tc.classifier.create(train_set, target='sentiment', features=['1grams features','2grams features'])\n",
        "result2 = model_2.evaluate(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja7iEEbuZpby"
      },
      "source": [
        "print_statistics(result2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emZ_6b_iZpbz"
      },
      "source": [
        "Indeed, the new constructed classifier seems to be more accurate with an accuracy of about ~0.9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqIY2d5jZpbz"
      },
      "source": [
        "### Unlabeled Test File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt5E78hXZpbz"
      },
      "source": [
        "To test how well the presented method works, we will use all the 25,000 labeled IMDB movie reviews in the train dataset to construct a classifier. Afterwards, we will utilize the constructed classifier to predict sentiment for each review in the unlabeled dataset. Lastly, we will create a submission file according to Kaggle's guidelines and submit it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N1Nl0OZZpbz"
      },
      "source": [
        "traindata_path = \"/Users/datalab/bigdata/cjc/kaggle_popcorn_data/labeledTrainData.tsv\"\n",
        "testdata_path = \"/Users/datalab/bigdata/cjc/kaggle_popcorn_data/testData.tsv\"\n",
        "#creating classifier using all 25,000 reviews\n",
        "train_data = tc.SFrame.read_csv(traindata_path,header=True, delimiter='\\t',quote_char='\"', \n",
        "                                column_type_hints = {'id':str, 'sentiment' : int, 'review':str } )\n",
        "train_data['1grams features'] = tc.text_analytics.count_ngrams(train_data['review'],1)\n",
        "train_data['2grams features'] = tc.text_analytics.count_ngrams(train_data['review'],2)\n",
        "\n",
        "cls = tc.classifier.create(train_data, target='sentiment', features=['1grams features','2grams features'])\n",
        "#creating the test dataset\n",
        "test_data = tc.SFrame.read_csv(testdata_path,header=True, delimiter='\\t',quote_char='\"', \n",
        "                               column_type_hints = {'id':str, 'review':str } )\n",
        "test_data['1grams features'] = tc.text_analytics.count_ngrams(test_data['review'],1)\n",
        "test_data['2grams features'] = tc.text_analytics.count_ngrams(test_data['review'],2)\n",
        "\n",
        "#predicting the sentiment of each review in the test dataset\n",
        "test_data['sentiment'] = cls.classify(test_data)['class'].astype(int)\n",
        "\n",
        "#saving the prediction to a CSV for submission\n",
        "test_data[['id','sentiment']].save(\"/Users/datalab/bigdata/cjc/kaggle_popcorn_data/predictions.csv\", format=\"csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsfBFn4AZpb0"
      },
      "source": [
        "We then submitted the predictions.csv file to the Kaggle challange website and scored AUC of about 0.88."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyEr081mZpb1"
      },
      "source": [
        "### Further Readings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8KXMCWUZpb1"
      },
      "source": [
        "Further reading materials can be found in the following links:\n",
        "\n",
        "http://en.wikipedia.org/wiki/Bag-of-words_model\n",
        "\n",
        "https://dato.com/products/create/docs/generated/graphlab.SFrame.html\n",
        "\n",
        "https://dato.com/products/create/docs/graphlab.toolkits.classifier.html\n",
        "\n",
        "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
        "\n",
        "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). \"Learning Word Vectors for Sentiment Analysis.\" The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).\n"
      ]
    }
  ]
}